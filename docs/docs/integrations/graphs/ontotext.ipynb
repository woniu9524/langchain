{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1271ba5c-1700-4872-b193-f7c162944521",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-27T18:44:53.493675Z",
          "iopub.status.busy": "2024-03-27T18:44:53.493473Z",
          "iopub.status.idle": "2024-03-27T18:44:53.499541Z",
          "shell.execute_reply": "2024-03-27T18:44:53.498940Z",
          "shell.execute_reply.started": "2024-03-27T18:44:53.493660Z"
        }
      },
      "source": [
        "# Ontotext GraphDB\n\n>[Ontotext GraphDB](https://graphdb.ontotext.com/) 是一个符合 [RDF](https://www.w3.org/RDF/) 和 [SPARQL](https://www.w3.org/TR/sparql11-query/) 标准的图数据库和知识发现工具。\n\n>本笔记本演示了如何使用 LLMs 为 `Ontotext GraphDB` 提供自然语言查询（NLQ to SPARQL，也称为 `text2sparql`）。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922a7a98-7d73-4a1a-8860-76a33451d1be",
      "metadata": {
        "id": "922a7a98-7d73-4a1a-8860-76a33451d1be"
      },
      "source": [
        "## GraphDB LLM 功能\n\n`GraphDB` 支持一些 LLM 集成功能，具体描述如下：\n\n[gpt-queries](https://graphdb.ontotext.com/documentation/10.5/gpt-queries.html)\n\n* 使用魔法谓词（magic predicates）通过知识图谱（KG）中的数据，向 LLM 获取文本、列表或表格。\n* 查询解释\n* 结果解释、摘要、改写、翻译\n\n[retrieval-graphdb-connector](https://graphdb.ontotext.com/documentation/10.5/retrieval-graphdb-connector.html)\n\n* 在向量数据库中索引 KG 实体。\n* 支持任何文本嵌入算法和向量数据库。\n* 使用 GraphDB 用于 Elastic、Solr、Lucene 的相同强大连接器（索引）语言。\n* 自动同步 RDF 数据更改到 KG 实体索引。\n* 支持嵌套对象（GraphDB 10.5 版本不支持 UI）。\n* 将 KG 实体序列化为文本，例如（以 Wines 数据集为例）：\n\n```\nFranvino:\n- is a RedWine.\n- made from grape Merlo.\n- made from grape Cabernet Franc.\n- has sugar dry.\n- has year 2012.\n```\n\n[talk-to-graph](https://graphdb.ontotext.com/documentation/10.5/talk-to-graph.html)\n\n* 使用定义的 KG 实体索引的简单聊天机器人。\n\n在本教程中，我们将不使用 GraphDB LLM 集成，而是从自然语言查询（NLQ）生成 SPARQL。我们将使用 `Star Wars API` (`SWAPI`) 的本体和数据集，您可以在[此处](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo/blob/main/starwars-data.trig)进行了解。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b464ff-8556-403f-a3d6-14ffcd703313",
      "metadata": {},
      "source": [
        "## 设置\n\n您需要一个正在运行的 GraphDB 实例。本教程将展示如何使用 [GraphDB Docker 镜像](https://hub.docker.com/r/ontotext/graphdb) 在本地运行数据库。它提供了一个 docker compose 设置，其中包含 Star Wars 数据集。包括此笔记本在内的所有必需文件都可以从 [GitHub 仓库 langchain-graphdb-qa-chain-demo](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo) 下载。\n\n* 安装 [Docker](https://docs.docker.com/get-docker/)。本教程使用的是 Docker 版本 `24.0.7`，其中捆绑了 [Docker Compose](https://docs.docker.com/compose/)。对于早期版本的 Docker，您可能需要单独安装 Docker Compose。\n* 将 [GitHub 仓库 langchain-graphdb-qa-chain-demo](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo) 克隆到您机器上的本地文件夹中。\n* 从同一文件夹执行以下脚本启动 GraphDB\n\n```\ndocker build --tag graphdb .\ndocker compose up -d graphdb\n```\n\n您需要等待几秒钟，数据库才能在 `http://localhost:7200/` 上启动。Star Wars 数据集 `starwars-data.trig` 会自动加载到 `langchain` 存储库中。您可以使用本地 SPARQL 端点 `http://localhost:7200/repositories/langchain` 来运行查询。您也可以在您喜欢的网页浏览器中打开 GraphDB Workbench `http://localhost:7200/sparql`，在那里您可以交互式地进行查询。\n* 设置工作环境\n\n如果您使用 `conda`，请创建一个新的 conda 环境并激活它，例如：\n\n```\nconda create -n graph_ontotext_graphdb_qa python=3.12\nconda activate graph_ontotext_graphdb_qa\n```\n\n安装以下库：\n\n```\npip install jupyter==1.1.1\npip install rdflib==7.1.1\npip install langchain-community==0.3.4\npip install langchain-openai==0.2.4\n```\n\n使用以下命令运行 Jupyter：\n```\njupyter notebook\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e51b397c-2fdc-4b99-9fed-1ab2b6ef7547",
      "metadata": {
        "id": "e51b397c-2fdc-4b99-9fed-1ab2b6ef7547"
      },
      "source": [
        "## 指定本体\n\n为了让 LLM 能够生成 SPARQL，它需要了解知识图谱的模式（本体）。可以通过 `OntotextGraphDBGraph` 类上的以下两个参数之一来提供：\n\n* `query_ontology`: 一个在 SPARQL 端点上执行的 `CONSTRUCT` 查询，用于返回 KG schema 语句。我们建议将本体存储在自己的命名图中，这样可以更容易地获取相关的语句（如以下示例）。不支持 `DESCRIBE` 查询，因为 `DESCRIBE` 返回的是对称简洁有界描述（SCBD），即也包括传入的类链接。对于拥有百万个实例的大型图谱，这效率不高。请查看 https://github.com/eclipse-rdf4j/rdf4j/issues/4857\n* `local_file`: 一个本地 RDF 本体文件。支持的 RDF 格式有 `Turtle`、`RDF/XML`、`JSON-LD`、`N-Triples`、`Notation-3`、`Trig`、`Trix`、`N-Quads`。\n\n无论哪种情况，本体转储都应：\n\n* 包含关于类、属性、属性与类之间的关联（使用 `rdfs:domain`、`schema:domainIncludes` 或 OWL 限制）以及分类体系（重要的个体）的足够信息。\n* 不包含过于冗长且与 SPARQL 构建无关的定义和示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dc8792e0-acfb-4310-b5fa-8f649e448870",
      "metadata": {
        "id": "dc8792e0-acfb-4310-b5fa-8f649e448870"
      },
      "outputs": [],
      "source": [
        "from langchain_community.graphs import OntotextGraphDBGraph\n",
        "\n",
        "# feeding the schema using a user construct query\n",
        "\n",
        "graph = OntotextGraphDBGraph(\n",
        "    query_endpoint=\"http://localhost:7200/repositories/langchain\",\n",
        "    query_ontology=\"CONSTRUCT {?s ?p ?o} FROM <https://swapi.co/ontology/> WHERE {?s ?p ?o}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a08b8d8c-af01-4401-8069-5f2cd022a6df",
      "metadata": {
        "id": "a08b8d8c-af01-4401-8069-5f2cd022a6df"
      },
      "outputs": [],
      "source": [
        "# feeding the schema using a local RDF file\n",
        "\n",
        "graph = OntotextGraphDBGraph(\n",
        "    query_endpoint=\"http://localhost:7200/repositories/langchain\",\n",
        "    local_file=\"/path/to/langchain_graphdb_tutorial/starwars-ontology.nt\",  # change the path here\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583b26ce-fb0d-4e9c-b5cd-9ec0e3be8922",
      "metadata": {
        "id": "583b26ce-fb0d-4e9c-b5cd-9ec0e3be8922"
      },
      "source": [
        "无论哪种情况，本体（schema）都以 `Turtle` 格式输入到 LLM 中，因为带有适当前缀的 `Turtle` 最紧凑，并且对 LLM 来说最容易记住。\n\n《星球大战》本体有点不寻常，因为它包含了许多关于类的特定三元组，例如 `:Aleena` 物种生活在 `<planet/38>` 上，它们是 `:Reptile` 的子类，具有某些典型特征（平均身高、平均寿命、肤色），并且特定个体（角色）是该类的代表：\n\n```\n@prefix : <https://swapi.co/vocabulary/> .\n@prefix owl: <http://www.w3.org/2002/07/owl#> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n\n:Aleena a owl:Class, :Species ;\n    rdfs:label \"Aleena\" ;\n    rdfs:isDefinedBy <https://swapi.co/ontology/> ;\n    rdfs:subClassOf :Reptile, :Sentient ;\n    :averageHeight 80.0 ;\n    :averageLifespan \"79\" ;\n    :character <https://swapi.co/resource/aleena/47> ;\n    :film <https://swapi.co/resource/film/4> ;\n    :language \"Aleena\" ;\n    :planet <https://swapi.co/resource/planet/38> ;\n    :skinColor \"blue\", \"gray\" .\n\n    ...\n\n ```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6277d911-b0f6-4aeb-9aa5-96416b668468",
      "metadata": {
        "id": "6277d911-b0f6-4aeb-9aa5-96416b668468"
      },
      "source": [
        "为了保持本教程的简洁性，我们使用了未加密的 GraphDB。如果 GraphDB 已加密，您应在 `OntotextGraphDBGraph` 初始化之前设置环境变量 'GRAPHDB_USERNAME' 和 'GRAPHDB_PASSWORD'。\n\n```python\nos.environ[\"GRAPHDB_USERNAME\"] = \"graphdb-user\"\nos.environ[\"GRAPHDB_PASSWORD\"] = \"graphdb-password\"\n\ngraph = OntotextGraphDBGraph(\n    query_endpoint=...,\n    query_ontology=...\n)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "446d8a00-c98f-43b8-9e84-77b244f7bb24",
      "metadata": {
        "id": "446d8a00-c98f-43b8-9e84-77b244f7bb24"
      },
      "source": [
        "## 针对 StarWars 数据集的问答\n\n我们现在可以使用 `OntotextGraphDBQAChain` 来提问了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fab63d88-511d-4049-9bf0-ca8748f1fbff",
      "metadata": {
        "id": "fab63d88-511d-4049-9bf0-ca8748f1fbff"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from langchain.chains import OntotextGraphDBQAChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# We'll be using an OpenAI model which requires an OpenAI API Key.\n",
        "# However, other models are available as well:\n",
        "# https://python.langchain.com/docs/integrations/chat/\n",
        "\n",
        "# Set the environment variable `OPENAI_API_KEY` to your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-***\"\n",
        "\n",
        "# Any available OpenAI model can be used here.\n",
        "# We use 'gpt-4-1106-preview' because of the bigger context window.\n",
        "# The 'gpt-4-1106-preview' model_name will deprecate in the future and will change to 'gpt-4-turbo' or similar,\n",
        "# so be sure to consult with the OpenAI API https://platform.openai.com/docs/models for the correct naming.\n",
        "\n",
        "chain = OntotextGraphDBQAChain.from_llm(\n",
        "    ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\"),\n",
        "    graph=graph,\n",
        "    verbose=True,\n",
        "    allow_dangerous_requests=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64de8463-35b1-4c65-91e4-387daf4dd7d4",
      "metadata": {},
      "source": [
        "让我们问一个简单的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f1dc4bea-b0f1-48f7-99a6-351a31acac7b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new OntotextGraphDBQAChain chain...\u001b[0m\n",
            "Generated SPARQL:\n",
            "\u001b[32;1m\u001b[1;3mPREFIX : <https://swapi.co/vocabulary/>\n",
            "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
            "\n",
            "SELECT ?climate\n",
            "WHERE {\n",
            "  ?planet rdfs:label \"Tatooine\" ;\n",
            "          :climate ?climate .\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The climate on Tatooine is arid.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({chain.input_key: \"What is the climate on Tatooine?\"})[chain.output_key]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d3a37f4-5c56-4b3e-b6ae-3eb030ffcc8f",
      "metadata": {},
      "source": [
        "还有稍微复杂一点的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4dde8b18-4329-4a86-abfb-26d3e77034b7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new OntotextGraphDBQAChain chain...\u001b[0m\n",
            "Generated SPARQL:\n",
            "\u001b[32;1m\u001b[1;3mPREFIX : <https://swapi.co/vocabulary/>\n",
            "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
            "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
            "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
            "\n",
            "SELECT ?climate\n",
            "WHERE {\n",
            "  ?character rdfs:label \"Luke Skywalker\" .\n",
            "  ?character :homeworld ?planet .\n",
            "  ?planet :climate ?climate .\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"The climate on Luke Skywalker's home planet is arid.\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({chain.input_key: \"What is the climate on Luke Skywalker's home planet?\"})[\n",
        "    chain.output_key\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d3ce3e-9528-4a65-8f3e-2281de08cbf1",
      "metadata": {},
      "source": [
        "我们还可以提出更复杂的问题，例如"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ab6f55f1-a3e0-4615-abd2-3cb26619c8d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new OntotextGraphDBQAChain chain...\u001b[0m\n",
            "Generated SPARQL:\n",
            "\u001b[32;1m\u001b[1;3mPREFIX : <https://swapi.co/vocabulary/>\n",
            "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
            "\n",
            "SELECT (AVG(?boxOffice) AS ?averageBoxOfficeRevenue)\n",
            "WHERE {\n",
            "  ?film a :Film .\n",
            "  ?film :boxOffice ?boxOfficeValue .\n",
            "  BIND(xsd:decimal(?boxOfficeValue) AS ?boxOffice)\n",
            "}\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The average box office revenue for all the Star Wars movies is approximately 754.1 million dollars.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\n",
        "    {\n",
        "        chain.input_key: \"What is the average box office revenue for all the Star Wars movies?\"\n",
        "    }\n",
        ")[chain.output_key]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11511345-8436-4634-92c6-36f2c0dd44db",
      "metadata": {
        "id": "11511345-8436-4634-92c6-36f2c0dd44db"
      },
      "source": [
        "## Chain modifiers\n\nOntotext GraphDB QA chain 允许您通过提示来优化查询，以进一步改进 QA chain 并提升应用的整体用户体验。\n\n### \"SPARQL Generation\" prompt\n\n此 prompt 用于根据用户的查询和 KG schema 来生成 SPARQL 查询。\n\n- `sparql_generation_prompt`\n\n    默认值：\n  ````python\n    GRAPHDB_SPARQL_GENERATION_TEMPLATE = \"\"\"\n    Write a SPARQL SELECT query for querying a graph database.\n    The ontology schema delimited by triple backticks in Turtle format is:\n    ```\n    {schema}\n    ```\n    Use only the classes and properties provided in the schema to construct the SPARQL query.\n    Do not use any classes or properties that are not explicitly provided in the SPARQL query.\n    Include all necessary prefixes.\n    Do not include any explanations or apologies in your responses.\n    Do not wrap the query in backticks.\n    Do not include any text except the SPARQL query generated.\n    The question delimited by triple backticks is:\n    ```\n    {prompt}\n    ```\n    \"\"\"\n    GRAPHDB_SPARQL_GENERATION_PROMPT = PromptTemplate(\n        input_variables=[\"schema\", \"prompt\"],\n        template=GRAPHDB_SPARQL_GENERATION_TEMPLATE,\n    )\n  ````\n\n### \"SPARQL Fix\" prompt\n\n有时，LLM 可能会生成带有语法错误或缺少前缀的 SPARQL 查询。该链会尝试通过提示 LLM 在一定次数内进行更正来修复此问题。\n\n- `sparql_fix_prompt`\n\n    默认值：\n  ````python\n    GRAPHDB_SPARQL_FIX_TEMPLATE = \"\"\"\n    This following SPARQL query delimited by triple backticks\n    ```\n    {generated_sparql}\n    ```\n    is not valid.\n    The error delimited by triple backticks is\n    ```\n    {error_message}\n    ```\n    Give me a correct version of the SPARQL query.\n    Do not change the logic of the query.\n    Do not include any explanations or apologies in your responses.\n    Do not wrap the query in backticks.\n    Do not include any text except the SPARQL query generated.\n    The ontology schema delimited by triple backticks in Turtle format is:\n    ```\n    {schema}\n    ```\n    \"\"\"\n\n    GRAPHDB_SPARQL_FIX_PROMPT = PromptTemplate(\n        input_variables=[\"error_message\", \"generated_sparql\", \"schema\"],\n        template=GRAPHDB_SPARQL_FIX_TEMPLATE,\n    )\n  ````\n\n- `max_fix_retries`\n\n    默认值： `5`\n\n### \"Answering\" prompt\n\n该 prompt 用于根据从数据库返回的结果和用户的初始问题来回答问题。默认情况下，LLM 会被指示只使用返回结果中的信息。如果结果集为空，LLM 应告知它无法回答该问题。\n\n- `qa_prompt`\n\n  默认值：\n  ````python\n    GRAPHDB_QA_TEMPLATE = \"\"\"Task: Generate a natural language response from the results of a SPARQL query.\n    You are an assistant that creates well-written and human understandable answers.\n    The information part contains the information provided, which you can use to construct an answer.\n    The information provided is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n    Make your response sound like the information is coming from an AI assistant, but don't add any information.\n    Don't use internal knowledge to answer the question, just say you don't know if no information is available.\n    Information:\n    {context}\n\n    Question: {prompt}\n    Helpful Answer:\"\"\"\n    GRAPHDB_QA_PROMPT = PromptTemplate(\n        input_variables=[\"context\", \"prompt\"], template=GRAPHDB_QA_TEMPLATE\n    )\n  ````"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ef8c073-003d-44ab-8a7b-cf45c50f6370",
      "metadata": {
        "id": "2ef8c073-003d-44ab-8a7b-cf45c50f6370"
      },
      "source": [
        "当您完成 GraphDB 的 QA 测试后，您可以通过运行以下命令来关闭 Docker 环境：\n\n```\ndocker compose down -v --remove-orphans\n```\n\n请在包含 Docker compose 文件的目录中执行此命令。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}