# Ollama

>[Ollama](https://ollama.com/) 可让您在本地运行开源大语言模型，例如 [Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/)。
>
>`Ollama` 将模型权重、配置和数据打包成一个由 Modelfile 定义的单一文件。它会优化设置和配置细节，包括 GPU 使用。
>如需支持的模型和模型变体的完整列表，请参阅 [Ollama 模型库](https://ollama.ai/library)。

有关如何将 `Ollama` 与 LangChain 结合使用的更多详细信息，请参阅[本指南](/docs/how_to/local_llms)。

## 安装和设置
### Ollama 安装
请遵循[这些说明](https://github.com/ollama/ollama?tab=readme-ov-file#ollama)来设置和运行本地 Ollama 实例。

Ollama 将自动作为后台服务启动，如果禁用，请运行：

```bash
# export OLLAMA_HOST=127.0.0.1 # 设置 ollama 主机的环境变量
# export OLLAMA_PORT=11434 # 设置 ollama 端口的环境变量
ollama serve
```

启动 ollama 后，运行 `ollama pull <模型名称>` 从[Ollama 模型库](https://ollama.ai/library)下载模型：

```bash
ollama pull llama3.1
```

- 这将下载模型的默认标记版本。通常，默认值指向最新、最小参数尺寸的模型。
- 要查看所有已拉取的（下载的）模型，请使用 `ollama list`

现在我们已准备好安装 `langchain-ollama` 伙伴包并运行模型。

### Ollama LangChain 伙伴包安装
使用以下命令安装集成包：
```bash
pip install langchain-ollama
```
## LLM

```python
from langchain_ollama.llms import OllamaLLM
```

请在此处查看笔记本示例：[here](/docs/integrations/llms/ollama)。

## Chat Models

### Chat Ollama

```python
from langchain_ollama.chat_models import ChatOllama
```

请在此处查看笔记本示例：[here](/docs/integrations/chat/ollama)。

### Ollama 工具调用
[Ollama 工具调用](https://ollama.com/blog/tool-support) 使用符合 OpenAI 的 Web 服务器规范，可以与默认的 `BaseChatModel.bind_tools()` 方法一起使用，如[此处](/docs/how_to/tool_calling/)所述。请确保选择支持[工具调用](https://ollama.com/search?&c=tools)的 ollama 模型。

## Embedding models

```python
from langchain_community.embeddings import OllamaEmbeddings
```

请在此处查看笔记本示例：[here](/docs/integrations/text_embedding/ollama)。