# Xorbits 推理 (Xinference)

此页面演示了如何使用 [Xinference](https://github.com/xorbitsai/inference)
与 LangChain 结合。

`Xinference` 是一个强大而通用的库，旨在部署大型语言模型、语音识别模型和多模态模型，甚至可以在你的笔记本电脑上运行。
通过 Xorbits Inference，你只需一个命令即可轻松部署和提供你自己的或者最先进的内置模型。

## 安装与设置

可以通过 pip 从 PyPI 安装 Xinference：

```bash
pip install "xinference[all]"
```

## LLM

Xinference 支持与 GGML 兼容的各种模型，包括 chatglm、baichuan、whisper、
vicuna 和 orca。要查看内置模型，请运行命令：

```bash
xinference list --all
```


### Xinference 的 Wrapper

你可以通过运行以下命令来启动一个本地 Xinference 实例：

```bash
xinference
```

你也可以在分布式集群中部署 Xinference。为此，首先在你想要运行它的服务器上启动一个 Xinference supervisor：

```bash
xinference-supervisor -H "${supervisor_host}"
```


然后，在你想要运行它们的其他服务器上启动 Xinference workers：

```bash
xinference-worker -e "http://${supervisor_host}:9997"
```

你也可以通过运行以下命令来启动一个本地 Xinference 实例：

```bash
xinference
```

一旦 Xinference 运行起来，就可以通过 CLI 或 Xinference 客户端访问一个端点来管理模型。

对于本地部署，端点将是 http://localhost:9997.


对于集群部署，端点将是 http://${supervisor_host}:9997。


然后，你需要启动一个模型。你可以指定模型名称和其他属性，
包括 model_size_in_billions 和 quantization。你可以使用命令行界面 (CLI) 来
这样做。例如：

```bash
xinference launch -n orca -s 3 -q q4_0
```

将返回一个模型 uid。

示例用法：

```python
from langchain_community.llms import Xinference

llm = Xinference(
    server_url="http://0.0.0.0:9997",
    model_uid = {model_uid} # 将 model_uid 替换为启动模型时返回的模型 UID
)

llm(
    prompt="Q: where can we visit in the capital of France? A:",
    generate_config={"max_tokens": 1024, "stream": True},
)

```

### 用法

有关更多信息和详细示例，请参阅
[xinference LLM 的示例](/docs/integrations/llms/xinference)

### Embeddings

Xinference 还支持对查询和文档进行嵌入。请参阅
[xinference 嵌入的示例](/docs/integrations/text_embedding/xinference)
以获取更详细的演示。


### Xinference LangChain 合作伙伴包安装
使用以下命令安装集成包：
```bash
pip install langchain-xinference
```
## Chat Models

```python
from langchain_xinference.chat_models import ChatXinference
```

## LLM

```python
from langchain_xinference.llms import Xinference
```