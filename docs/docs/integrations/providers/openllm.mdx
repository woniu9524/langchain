---
keywords: [openllm]
---

# OpenLLM

OpenLLM 允许开发者只需一条命令，即可将任何**开源 LLM** 作为**兼容 OpenAI 的 API** 端点来运行。

- 🔬 为快速和生产使用而构建
- 🚂 支持 llama3、qwen2、gemma 等，以及许多**量化**版本 [完整列表](https://github.com/bentoml/openllm-models)
- ⛓️ 兼容 OpenAI 的 API
- 💬 内置类似 ChatGPT 的 UI
- 🔥 使用最先进的推理后端加速 LLM 解码
- 🌥️ 准备好进行企业级云部署（Kubernetes、Docker 和 BentoCloud）

## 安装和设置

通过 PyPI 安装 OpenLLM 包：

```bash
pip install openllm
```

## LLM

OpenLLM 支持广泛的开源 LLM，同时也支持为用户提供自定义的微调 LLM。使用 `openllm model` 命令可查看所有预先为 OpenLLM 优化的可用模型。

## Wrappers

有一个 OpenLLM Wrapper 支持与运行中的 OpenLLM 服务器进行交互：

```python
from langchain_community.llms import OpenLLM
```

### OpenLLM 服务器的 Wrapper

此 Wrapper 支持与 OpenLLM 的兼容 OpenAI 的端点进行交互。

要运行模型，请执行：

```bash
openllm hello
```

Wrapper 用法：

```python
from langchain_community.llms import OpenLLM

llm = OpenLLM(base_url="http://localhost:3000/v1", api_key="na")

llm("What is the difference between a duck and a goose? And why there are so many Goose in Canada?")
```

### 用法

有关 OpenLLM Wrapper 的更详细教程，请参阅
[示例笔记本](/docs/integrations/llms/openllm)