# Langfuse 🪢

> **什么是 Langfuse？** [Langfuse](https://langfuse.com) 是一个开源的 LLM 工程平台，可帮助团队跟踪 API 调用、监控性能以及调试其 AI 应用程序中的问题。

## 跟踪 LangChain

[Langfuse Tracing](https://langfuse.com/docs/tracing) 使用 Langchain 回调机制 ([Python](https://python.langchain.com/docs/how_to/#callbacks), [JS](https://js.langchain.com/docs/how_to/#callbacks)) 与 Langchain 集成。因此，Langfuse SDK 会为您的 Langchain 应用程序的每次运行自动创建一个嵌套跟踪。这允许您记录、分析和调试您的 LangChain 应用程序。

您可以通过 (1) 构造函数参数或 (2) 环境变量配置集成。通过在 [cloud.langfuse.com](https://cloud.langfuse.com) 注册帐户或 [自托管 Langfuse](https://langfuse.com/self-hosting) 来获取您的 Langfuse 凭证。

### 构造函数参数

```python
pip install langfuse
```

```python
from langfuse import Langfuse, get_client
from langfuse.langchain import CallbackHandler
from langchain_openai import ChatOpenAI  # 示例 LLM
from langchain_core.prompts import ChatPromptTemplate
 
# 使用构造函数参数初始化 Langfuse 客户端
Langfuse(
    public_key="your-public-key",
    secret_key="your-secret-key",
    host="https://cloud.langfuse.com"  # 可选：默认为 https://cloud.langfuse.com
)
 
# 获取已配置的客户端实例
langfuse = get_client()
 
# 初始化 Langfuse 处理程序
langfuse_handler = CallbackHandler()
 
# 创建您的 LangChain 组件
llm = ChatOpenAI(model_name="gpt-4o")
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
chain = prompt | llm
 
# 使用 Langfuse 跟踪运行您的链
response = chain.invoke({"topic": "cats"}, config={"callbacks": [langfuse_handler]})
print(response.content)
 
# 在短期应用程序中将事件刷新到 Langfuse
langfuse.flush()
```

### 环境变量

```bash filename=".env"
LANGFUSE_SECRET_KEY="sk-lf-..."
LANGFUSE_PUBLIC_KEY="pk-lf-..."
# 🇪🇺 欧洲地区数据
LANGFUSE_HOST="https://cloud.langfuse.com"
# 🇺🇸 美国地区数据
# LANGFUSE_HOST="https://us.cloud.langfuse.com"
```

```python
# 初始化 Langfuse 处理程序
from langfuse.langchain import CallbackHandler
langfuse_handler = CallbackHandler()
 
# 您的 Langchain 代码
 
# 将 Langfuse 处理程序添加为回调（经典和 LCEL）
chain.invoke({"input": "<user_input>"}, config={"callbacks": [langfuse_handler]})
```

要了解如何将此集成与其他 Langfuse 功能结合使用，请查看[此端到端示例](https://langfuse.com/docs/integrations/langchain/example-python)。

## 跟踪 LangGraph

本部分展示了 [Langfuse](https://langfuse.com/docs) 如何使用 [LangChain 集成](https://langfuse.com/docs/integrations/langchain/tracing) 来调试、分析和迭代您的 LangGraph 应用程序。

### 初始化 Langfuse

**注意：** 您需要运行至少 Python 3.11 ([GitHub Issue](https://github.com/langfuse/langfuse/issues/1926))。

使用 Langfuse UI 中项目的 [API 密钥](https://langfuse.com/faq/all/where-are-langfuse-api-keys) 初始化 Langfuse 客户端，并将它们添加到您的环境变量中。


```python
%pip install langfuse
%pip install langchain langgraph langchain_openai langchain_community
```


```python
import os

# 从 https://cloud.langfuse.com 获取您项目的密钥
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-***"
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-***"
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com" # 欧洲数据区域
# os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # 美国数据区域

# 您的 openai 密钥
os.environ["OPENAI_API_KEY"] = "***"
```

### 使用 LangGraph 的简单聊天应用

**本节我们将做什么：**

*   构建一个支持聊天机器人，能够回答常见问题
*   使用 Langfuse 跟踪聊天机器人的输入和输出

我们将从一个基础聊天机器人开始，然后在下一节中构建一个更高级的多智能体设置，逐步介绍关键的 LangGraph 概念。

#### 创建 Agent

首先创建一个 `StateGraph`。`StateGraph` 对象将我们的聊天机器人的结构定义为状态机。我们将添加节点来表示 LLM 和聊天机器人可以调用的函数，并添加边来指定机器人如何在这些函数之间转换。


```python
from typing import Annotated

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from typing_extensions import TypedDict

from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages

class State(TypedDict):
    # Messages 的类型为“list”。注释中的 `add_messages` 函数定义了此状态键应如何更新
    # （在此示例中，它会将消息附加到列表中，而不是覆盖它们）
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

llm = ChatOpenAI(model = "gpt-4o", temperature = 0.2)

# chatbot 节点函数接收当前状态作为输入并返回更新后的消息列表。这是所有 LangGraph 节点函数的模式。
def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}

# 添加一个“chatbot”节点。节点代表工作单元。它们通常是常规的 Python 函数。
graph_builder.add_node("chatbot", chatbot)

# 添加一个入口点。这告诉我们的图每次运行图时从哪里开始工作。
graph_builder.set_entry_point("chatbot")

# 设置一个完成点。这指示图“任何时候运行此节点，您都可以退出。”
graph_builder.set_finish_point("chatbot")

# 为了能够运行我们的图，请调用图构建器上的“compile()”。这将创建一个我们可以用于在状态上调用 `invoke` 的“CompiledGraph”。
graph = graph_builder.compile()
```

#### 将 Langfuse 添加为调用的回调

现在，我们将添加 [LangChain 的 Langfuse 回调处理程序](https://langfuse.com/docs/integrations/langchain/tracing) 来跟踪我们应用程序的步骤：`config={"callbacks": [langfuse_handler]}`


```python
from langfuse.langchain import CallbackHandler

# 初始化 Langchain 的 Langfuse CallbackHandler (用于跟踪)
langfuse_handler = CallbackHandler()

for s in graph.stream({"messages": [HumanMessage(content = "What is Langfuse?")]},
                      config={"callbacks": [langfuse_handler]}):
    print(s)
```

```
{'chatbot': {'messages': [AIMessage(content='Langfuse is a tool designed to help developers monitor and observe the performance of their Large Language Model (LLM) applications. It provides detailed insights into how these applications are functioning, allowing for better debugging, optimization, and overall management. Langfuse offers features such as tracking key metrics, visualizing data, and identifying potential issues in real-time, making it easier for developers to maintain and improve their LLM-based solutions.', response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 13, 'total_tokens': 99}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_400f27fa1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-9a0c97cb-ccfe-463e-902c-5a5900b796b4-0', usage_metadata={'input_tokens': 13, 'output_tokens': 86, 'total_tokens': 99})]}}
```


#### 在 Langfuse 中查看跟踪

Langfuse 中的示例跟踪：https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/d109e148-d188-4d6e-823f-aac0864afbab

![Langfuse 中聊天应用的跟踪视图](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_chatapp_trace.png)

- 查看[完整的 Notebook](https://langfuse.com/docs/integrations/langchain/example-python-langgraph) 以查看更多示例。
- 要了解如何评估您的 LangGraph 应用程序的性能，请查看[LangGraph 评估指南](https://langfuse.com/docs/integrations/langchain/example-langgraph-agents)。