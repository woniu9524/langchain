# Intel

>[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#optimum-intel) 是 🤗 Transformers 和 Diffusers 库与 Intel 提供的各种工具和库之间的接口，可在 Intel 架构上加速端到端流水线。

>[Intel® Extension for Transformers](https://github.com/intel/intel-extension-for-transformers?tab=readme-ov-file#intel-extension-for-transformers) (ITREX) 是一个创新的工具包，旨在通过 Transformer 模型在各种 Intel 平台（包括 Intel Gaudi2、Intel CPU 和 Intel GPU）上的最佳性能来加速各地的 GenAI/LLM。

本文档介绍如何将 optimum-intel 和 ITREX 与 LangChain 结合使用。

## Optimum-intel

与 [optimum-intel](https://github.com/huggingface/optimum-intel.git) 和 [IPEX](https://github.com/intel/intel-extension-for-pytorch) 相关的所有功能。

### 安装

使用以下命令安装 optimum-intel 和 ipex：

```bash
pip install optimum[neural-compressor]
pip install intel_extension_for_pytorch
```

请按照如下安装说明进行操作：

* 按照 [此处](https://github.com/huggingface/optimum-intel) 的说明安装 optimum-intel。
* 按照 [此处](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.2.0%2Bcpu) 的说明安装 IPEX。

### Embedding Models

请参阅 [使用示例](/docs/integrations/text_embedding/optimum_intel)。
我们还在 cookbook 目录中提供了一个完整的教程笔记本 "rag_with_quantized_embeddings.ipynb"，用于在 RAG 流水线中使用 embedder。

```python
from langchain_community.embeddings import QuantizedBiEncoderEmbeddings
```

## Intel® Extension for Transformers (ITREX)
(ITREX) 是一个创新的工具包，用于在 Intel 平台上加速 Transformer 模型，尤其在第四代 Intel Xeon 可扩展处理器 Sapphire Rapids（代号 Sapphire Rapids）上效果显著。

量化是一个过程，通过使用较少数量的比特来表示这些权重，从而降低它们的精度。仅权重量化专门侧重于量化神经网络的权重，同时将激活等其他组件保持其原始精度。

随着大型语言模型（LLMs）的日益普及，对新的和改进的量化方法的需求不断增长，这些方法可以满足这些现代架构的计算需求，同时保持准确性。与 W8A8 等[常规量化](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/quantization.md)相比，仅权重量化可能是平衡性能和准确性的更好权衡，因为我们将在下面看到部署 LLM 的瓶颈是内存带宽，而仅权重量化通常可以带来更好的准确性。

在此，我们将介绍针对 ITREX 的 Transformer 大型语言模型的 Embedding Models 和仅权重量化。仅权重量化是一种用于深度学习的技术，用于减少神经网络的内存和计算需求。在深度神经网络的上下文中，模型参数，也称为权重，通常使用浮点数表示，这会消耗大量的内存并需要密集的计算资源。

与 [optimum-intel](https://github.com/huggingface/optimum-intel.git) 和 [IPEX](https://github.com/intel/intel-extension-for-pytorch) 相关的所有功能。

### 安装

安装 intel-extension-for-transformers。有关系统要求和其他安装技巧，请参阅[安装指南](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/installation.md)

```bash
pip install intel-extension-for-transformers
```
安装其他必需的包。

```bash
pip install -U torch onnx accelerate datasets
```

### Embedding Models

请参阅 [使用示例](/docs/integrations/text_embedding/itrex)。

```python
from langchain_community.embeddings import QuantizedBgeEmbeddings
```

### Weight-Only Quantization with ITREX

请参阅 [使用示例](/docs/integrations/llms/weight_only_quantization)。

## 配置参数详解

以下是 `WeightOnlyQuantConfig` 类的详细信息。

#### weight_dtype (string): 权重数据类型，默认为 "nf4"。
我们支持将权重量化为以下数据类型进行存储（WeightOnlyQuantConfig 中的 weight_dtype）：
* **int8**: 使用 8 位数据类型。
* **int4_fullrange**: 相对于常规的 int4 范围 [-7,7]，使用 int4 的 -8 值。
* **int4_clip**: 将值裁剪并保留在 int4 范围内，将其他值设置为零。
* **nf4**: 使用归一化 4 位浮点数据类型。
* **fp4_e2m1**: 使用常规 4 位浮点数据类型。"e2" 表示指数使用 2 位，"m1" 表示尾数使用 1 位。

#### compute_dtype (string): 计算数据类型，默认为 "fp32"。
虽然这些技术将权重存储为 4 位或 8 位，但计算仍然以 float32、bfloat16 或 int8（WeightOnlyQuantConfig 中的 compute_dtype）进行：
* **fp32**: 使用 float32 数据类型进行计算。
* **bf16**: 使用 bfloat16 数据类型进行计算。
* **int8**: 使用 8 位数据类型进行计算。

#### llm_int8_skip_modules (list of module's name): 跳过量化的模块，默认为 None。
这是一个要跳过量化的模块列表。

#### scale_dtype (string): Scale 数据类型，默认为 "fp32"。
目前仅支持 "fp32"（float32）。

#### mse_range (boolean): 是否搜索最佳裁剪范围 [0.805, 1.0, 0.005]，默认为 False。
#### use_double_quant (boolean): 是否量化 Scale，默认为 False。
暂不支持。
#### double_quant_dtype (string): 预留给双量化。
#### double_quant_scale_dtype (string): 预留给双量化。
#### group_size (int): 量化时的组大小。
#### scheme (string): 权重将被量化成的格式。默认为 "sym"。
* **sym**: 对称量化。
* **asym**: 非对称量化。
#### algorithm (string): 用于提高准确性的算法。默认为 "RTN"。
* **RTN**: 四舍五入到最近值 (RTN) 是一种我们可以直观理解的量化方法。
* **AWQ**: 只保护 1% 的显著权重就可以大大减少量化误差。显著权重通道是通过观察每个通道的激活和权重的分布来选择的。显著权重在量化前乘以一个大的缩放因子进行保护后再进行量化。
* **TEQ**: 在仅权重量化中保留 FP32 精度的可训练等效变换。