# Outlines

>[Outlines](https://github.com/dottxt-ai/outlines) 是一个用于约束语言生成的 Python 库。它提供了一个统一的接口来处理各种语言模型，并允许使用正则表达式匹配、类型约束、JSON Schema 和上下文无关文法等技术进行结构化生成。

Outlines 支持多种后端，包括：
- Hugging Face Transformers
- llama.cpp
- vLLM
- MLX

这种集成允许您将 Outlines 模型与 LangChain 一起使用，提供 LLM 和聊天模型接口。

## 安装和设置

要将 Outlines 与 LangChain 一起使用，您需要安装 Outlines 库：

```bash
pip install outlines
```

根据您选择的后端，您可能需要安装额外的依赖项：

- 对于 Transformers：`pip install transformers torch datasets`
- 对于 llama.cpp：`pip install llama-cpp-python`
- 对于 vLLM：`pip install vllm`
- 对于 MLX：`pip install mlx`

## LLM

要将 Outlines 用作 LangChain 中的 LLM，您可以使用 `Outlines` 类：

```python
from langchain_community.llms import Outlines
```

## 聊天模型

要将 Outlines 用作 LangChain 中的聊天模型，您可以使用 `ChatOutlines` 类：

```python
from langchain_community.chat_models import ChatOutlines
```

## 模型配置

`Outlines` 和 `ChatOutlines` 类共享相似的配置选项：

```python
model = Outlines(
    model="meta-llama/Llama-2-7b-chat-hf",  # 模型标识符
    backend="transformers",  # 要使用的后端 (transformers, llamacpp, vllm, or mlxlm)
    max_tokens=256,  # 要生成的最大 token 数
    stop=["\n"],  # 可选的停止字符串列表
    streaming=True,  # 是否流式输出
    # 结构化生成的附加参数：
    regex=None,
    type_constraints=None,
    json_schema=None,
    grammar=None,
    # 其他模型参数：
    model_kwargs={"temperature": 0.7}
)
```

### 模型标识符

`model` 参数可以是：
- Hugging Face 模型名称（例如 "meta-llama/Llama-2-7b-chat-hf"）
- 模型的本地路径
- 对于 GGUF 模型，格式为 "repo_id/file_name" （例如 "TheBloke/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf"）

### 后端选项

`backend` 参数指定要使用的后端：
- `"transformers"`：用于 Hugging Face Transformers 模型（默认）
- `"llamacpp"`：用于使用 llama.cpp 的 GGUF 模型
- `"transformers_vision"`：用于视觉-语言模型（例如 LLaVA）
- `"vllm"`：用于使用 vLLM 库的模型
- `"mlxlm"`：用于使用 MLX 框架的模型

### 结构化生成

Outlines 提供了多种结构化生成方法：

1. **正则表达式匹配**：
   ```python
   model = Outlines(
       model="meta-llama/Llama-2-7b-chat-hf",
       regex=r"((25[0-5]|2[0-4]\d|[01]?\d\d?)\.){3}(25[0-5]|2[0-4]\d|[01]?\d\d?)"
   )
   ```
   这将确保生成的文本匹配指定的正则表达式模式（在此示例中为有效的 IP 地址）。

2. **类型约束**：
   ```python
   model = Outlines(
       model="meta-llama/Llama-2-7b-chat-hf",
       type_constraints=int
   )
   ```
   这将输出限制为有效的 Python 类型（int、float、bool、datetime.date、datetime.time、datetime.datetime）。

3. **JSON Schema**：
   ```python
   from pydantic import BaseModel

   class Person(BaseModel):
       name: str
       age: int

   model = Outlines(
       model="meta-llama/Llama-2-7b-chat-hf",
       json_schema=Person
   )
   ```
   这将确保生成的输出符合指定的 JSON Schema 或 Pydantic 模型。

4. **上下文无关文法**：
   ```python
   model = Outlines(
       model="meta-llama/Llama-2-7b-chat-hf",
       grammar="""
           ?start: expression
           ?expression: term (("+" | "-") term)*
           ?term: factor (("*" | "/") factor)*
           ?factor: NUMBER | "-" factor | "(" expression ")"
           %import common.NUMBER
       """
   )
   ```
   这将生成符合 EBNF 格式指定上下文无关文法的文本。

## 使用示例

### LLM 示例

```python
from langchain_community.llms import Outlines

llm = Outlines(model="meta-llama/Llama-2-7b-chat-hf", max_tokens=100)
result = llm.invoke("Tell me a short story about a robot.")
print(result)
```

### 聊天模型示例

```python
from langchain_community.chat_models import ChatOutlines
from langchain_core.messages import HumanMessage, SystemMessage

chat = ChatOutlines(model="meta-llama/Llama-2-7b-chat-hf", max_tokens=100)
messages = [
    SystemMessage(content="You are a helpful AI assistant."),
    HumanMessage(content="What's the capital of France?")
]
result = chat.invoke(messages)
print(result.content)
```

### 流式传输示例

```python
from langchain_community.chat_models import ChatOutlines
from langchain_core.messages import HumanMessage

chat = ChatOutlines(model="meta-llama/Llama-2-7b-chat-hf", streaming=True)
for chunk in chat.stream("Tell me a joke about programming."):
    print(chunk.content, end="", flush=True)
print()
```

### 结构化输出示例

```python
from langchain_community.llms import Outlines
from pydantic import BaseModel

class MovieReview(BaseModel):
    title: str
    rating: int
    summary: str

llm = Outlines(
    model="meta-llama/Llama-2-7b-chat-hf",
    json_schema=MovieReview
)
result = llm.invoke("Write a short review for the movie 'Inception'.")
print(result)
```

## 附加功能

### Tokenizer 访问

您可以访问模型的底层分词器：

```python
tokenizer = llm.tokenizer
encoded = tokenizer.encode("Hello, world!")
decoded = tokenizer.decode(encoded)
```