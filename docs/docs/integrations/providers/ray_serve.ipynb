{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ray Serve\n\n[Ray Serve](https://docs.ray.io/en/latest/serve/index.html) 是一个可扩展的模型服务库，用于构建在线推理 API。Serve 特别适合系统组合，使您能够用 Python 代码构建由多个链接和业务逻辑组成的复杂推理服务。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 本 Notebook 的目标\n本 Notebook 展示了一个将 OpenAI chain 部署到生产环境的简单示例。您可以将其扩展，部署自己的自托管模型，轻松定义在生产环境中高效运行模型所需的硬件资源量（GPU 和 CPU）。有关自动扩缩容等可用选项的更多信息，请参阅 Ray Serve [文档](https://docs.ray.io/en/latest/serve/getting_started.html)。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 设置 Ray Serve\n使用 `pip install ray[serve]` 安装 Ray。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 通用骨架"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "部署服务的通用骨架如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0: Import ray serve and request from starlette\n",
        "from ray import serve\n",
        "from starlette.requests import Request\n",
        "\n",
        "\n",
        "# 1: Define a Ray Serve deployment.\n",
        "@serve.deployment\n",
        "class LLMServe:\n",
        "    def __init__(self) -> None:\n",
        "        # All the initialization code goes here\n",
        "        pass\n",
        "\n",
        "    async def __call__(self, request: Request) -> str:\n",
        "        # You can parse the request here\n",
        "        # and return a response\n",
        "        return \"Hello World\"\n",
        "\n",
        "\n",
        "# 2: Bind the model to deployment\n",
        "deployment = LLMServe.bind()\n",
        "\n",
        "# 3: Run the deployment\n",
        "serve.api.run(deployment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shutdown the deployment\n",
        "serve.api.shutdown()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 部署 OpenAI 链的示例及自定义提示词"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "从[这里](https://platform.openai.com/account/api-keys)获取 OpenAI API 密钥。运行以下代码，系统将要求您提供 API 密钥。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@serve.deployment\n",
        "class DeployLLM:\n",
        "    def __init__(self):\n",
        "        # We initialize the LLM, template and the chain here\n",
        "        llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
        "        template = \"Question: {question}\\n\\nAnswer: Let's think step by step.\"\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "        self.chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "    def _run_chain(self, text: str):\n",
        "        return self.chain(text)\n",
        "\n",
        "    async def __call__(self, request: Request):\n",
        "        # 1. Parse the request\n",
        "        text = request.query_params[\"text\"]\n",
        "        # 2. Run the chain\n",
        "        resp = self._run_chain(text)\n",
        "        # 3. Return the response\n",
        "        return resp[\"text\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在我们可以绑定部署了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bind the model to deployment\n",
        "deployment = DeployLLM.bind()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们可以在运行部署时分配端口号和主机。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example port number\n",
        "PORT_NUMBER = 8282\n",
        "# Run the deployment\n",
        "serve.api.run(deployment, port=PORT_NUMBER)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在服务已经部署在 `localhost:8282` 端口，我们可以发送一个 post 请求来获取结果。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "text = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
        "response = requests.post(f\"http://localhost:{PORT_NUMBER}/?text={text}\")\n",
        "print(response.content.decode())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ray",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}