{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDDVue_1cq6d"
      },
      "source": [
        "# NVIDIA NIMs\n\n`langchain-nvidia-ai-endpoints` 包包含 LangChain 集成，用于构建使用 NVIDIA NIM 推理微服务上模型的应用程序。NIM 支持来自社区以及 NVIDIA 的聊天、嵌入和重新排序模型等领域模型。这些模型由 NVIDIA 优化，可在 NVIDIA 加速基础设施上实现最佳性能，并作为 NIM 部署。NIM 是易于使用的预构建容器，通过 NVIDIA 加速基础设施上的单一命令即可在任何地方进行部署。\n\nNVIDIA 托管的 NIM 部署可在 [NVIDIA API catalog](https://build.nvidia.com/) 上进行测试。测试后，可以使用 NVIDIA AI Enterprise 许可证从 NVIDIA 的 API 目录导出 NIM，并在本地或云中运行，从而使企业能够拥有并完全控制其 IP 和 AI 应用程序。\n\nNIM 按模型打包为容器镜像，并通过 NVIDIA NGC Catalog 作为 NGC 容器镜像分发。本质上，NIM 提供简单、一致且熟悉的 API 来运行 AI 模型推理。\n\n此示例介绍了如何使用 LangChain 通过 `NVIDIAEmbeddings` 类与支持的 [NVIDIA Retrieval QA Embedding Model](https://build.nvidia.com/nvidia/embed-qa-4) 进行交互，以实现[检索增强生成](https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/)。\n\n有关通过此 API 访问聊天模型的更多信息，请参阅 [ChatNVIDIA](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) 文档。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 安装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain-nvidia-ai-endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKcxQMFTcwWi"
      },
      "source": [
        "## 设置\n\n**开始之前：**\n\n1. 在 [NVIDIA](https://build.nvidia.com/) 创建一个免费账户，NVIDIA 在此托管 NVIDIA AI Foundation 模型。\n\n2. 选择 `Retrieval` 选项卡，然后选择您选择的模型。\n\n3. 在 `Input` 下选择 `Python` 选项卡，然后点击 `Get API Key`。然后点击 `Generate Key`。\n\n4. 复制并保存生成的密钥作为 `NVIDIA_API_KEY`。之后，您应该就可以访问端点。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoF41-tNczS3",
        "outputId": "7f2833dc-191c-4d73-b823-7b2745a93a2f"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
        "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
        "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
        "else:\n",
        "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
        "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l185et2kc8pS"
      },
      "source": [
        "我们应该能在列表中看到一个嵌入模型，该模型可以与 LLM 结合使用，以实现有效的 RAG 解决方案。我们可以通过 `NVIDIAEmbeddings` 类来与该模型以及 NIM 支持的其他嵌入模型进行交互。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 在 NVIDIA API Catalog 中使用 NIM\n\n在初始化嵌入模型时，你可以通过传递模型名称来选择模型，例如下面示例中的 `NV-Embed-QA`，或者不传递任何参数来使用默认模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbXmJssPdIPX"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "\n",
        "embedder = NVIDIAEmbeddings(model=\"NV-Embed-QA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvQijbCwdLXB"
      },
      "source": [
        "该模型是一个微调后的 E5-large 模型，支持预期的 `Embeddings` 方法，包括：\n\n- `embed_query`：为查询样本生成查询 embedding。\n\n- `embed_documents`：为要搜索的文档列表生成 passage embedding。\n\n- `aembed_query`/`aembed_documents`：上述方法的异步版本。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 使用自托管 NVIDIA NIM\n\n准备部署时，您可以使用 NVIDIA NIM 自托管模型——NVIDIA AI Enterprise 软件许可已包含此项，并可在任何地方运行它们，让您拥有自定义内容的版权，并完全控制您的知识产权 (IP) 和 AI 应用。\n\n[了解更多关于 NIM 的信息](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "\n",
        "# connect to an embedding NIM running at localhost:8080\n",
        "embedder = NVIDIAEmbeddings(base_url=\"http://localhost:8080/v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcDu3v4CbmWk"
      },
      "source": [
        "### **相似度**\n\n以下是对这些数据点相似度的快速测试：\n\n**查询：**\n\n- 堪察加的天气怎么样？\n\n- 意大利以哪些食物闻名？\n\n- 我叫什么名字？我敢打赌你不记得了……\n\n- 生命的意义到底是什么？\n\n- 生命的意义在于玩乐 :D\n\n**文档：**\n\n- 堪察加的天气寒冷，冬季漫长而严酷。\n\n- 意大利以意大利面、披萨、冰淇淋和浓缩咖啡闻名。\n\n- 我无法回忆个人姓名，只能提供信息。\n\n- 人生的意义因人而异，通常被视为个人成就感。\n\n- 享受生活的美好时光确实是一种很棒的方式。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrmtRzgXdhMF"
      },
      "source": [
        "### 嵌入运行时"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUQM6OoObM_C",
        "outputId": "afbb1ea0-4f14-46b0-da42-25c5ae8eab2e"
      },
      "outputs": [],
      "source": [
        "print(\"\\nSequential Embedding: \")\n",
        "q_embeddings = [\n",
        "    embedder.embed_query(\"What's the weather like in Komchatka?\"),\n",
        "    embedder.embed_query(\"What kinds of food is Italy known for?\"),\n",
        "    embedder.embed_query(\"What's my name? I bet you don't remember...\"),\n",
        "    embedder.embed_query(\"What's the point of life anyways?\"),\n",
        "    embedder.embed_query(\"The point of life is to have fun :D\"),\n",
        "]\n",
        "print(\"Shape:\", (len(q_embeddings), len(q_embeddings[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfX00xRdbKDw"
      },
      "source": [
        "### 文档嵌入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1vKyTx-O_vZ",
        "outputId": "a8d864a8-01e8-4431-ee8a-b466d8348bef"
      },
      "outputs": [],
      "source": [
        "print(\"\\nBatch Document Embedding: \")\n",
        "d_embeddings = embedder.embed_documents(\n",
        "    [\n",
        "        \"Komchatka's weather is cold, with long, severe winters.\",\n",
        "        \"Italy is famous for pasta, pizza, gelato, and espresso.\",\n",
        "        \"I can't recall personal names, only provide information.\",\n",
        "        \"Life's purpose varies, often seen as personal fulfillment.\",\n",
        "        \"Enjoying life's moments is indeed a wonderful approach.\",\n",
        "    ]\n",
        ")\n",
        "print(\"Shape:\", (len(q_embeddings), len(q_embeddings[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6AilXxjdm1I"
      },
      "source": [
        "现在我们已经生成了嵌入，我们可以对结果进行简单的相似度检查，看看在检索任务中哪些文档会触发为合理的答案："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "7szaiBBYCHQ-",
        "outputId": "86b6d2c4-6bee-4324-f7b1-3fcf2b940763"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compute the similarity matrix between q_embeddings and d_embeddings\n",
        "cross_similarity_matrix = cosine_similarity(\n",
        "    np.array(q_embeddings),\n",
        "    np.array(d_embeddings),\n",
        ")\n",
        "\n",
        "# Plotting the cross-similarity matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(cross_similarity_matrix, cmap=\"Greens\", interpolation=\"nearest\")\n",
        "plt.colorbar()\n",
        "plt.title(\"Cross-Similarity Matrix\")\n",
        "plt.xlabel(\"Query Embeddings\")\n",
        "plt.ylabel(\"Document Embeddings\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5sLkHWZcRF2"
      },
      "source": [
        "温馨提示，发送到我们系统的查询和文档如下：\n\n**查询：**\n\n- 堪察加的天气怎么样？\n\n- 意大利以哪些食物闻名？\n\n- 我叫什么名字？我打赌你不记得了……\n\n- 人生的意义到底是什么？\n\n- 人生的意义在于玩得开心 :D\n\n**文档：**\n\n- 堪察加的天气寒冷，冬季漫长而严酷。\n\n- 意大利以意大利面、披萨、冰淇淋和浓缩咖啡闻名。\n\n- 我不记得个人名字，只能提供信息。\n\n- 人生的意义因人而异，通常被视为个人实现。\n\n- 享受生活中的美好时光确实是一种很棒的方式。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 截断\n\n嵌入式模型通常有一个固定的上下文窗口，它决定了可以嵌入的最大输入令牌数。这个限制可能是硬性限制，等于模型最大输入令牌长度，也可能是有效限制，超过这个限制后嵌入的准确性会下降。\n\n由于模型基于令牌运行，而应用程序通常处理文本，因此应用程序要确保其输入保持在模型的令牌限制之内可能具有挑战性。默认情况下，如果输入过大，会抛出异常。\n\n为了解决这个问题，NVIDIA 的 NIM（API Catalog 或本地）提供了一个 `truncate` 参数，该参数可以在服务器端截断过大的输入。\n\n`truncate` 参数有三个选项：\n- \"NONE\"：默认选项。如果输入过大，则会抛出异常。\n- \"START\"：服务器从开头（左侧）截断输入，必要时丢弃令牌。\n- \"END\"：服务器从末尾（右侧）截断输入，必要时丢弃令牌。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "long_text = \"AI is amazing, amazing is \" * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strict_embedder = NVIDIAEmbeddings()\n",
        "try:\n",
        "    strict_embedder.embed_query(long_text)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "truncating_embedder = NVIDIAEmbeddings(truncate=\"END\")\n",
        "truncating_embedder.embed_query(long_text)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNIeY4N96v3B"
      },
      "source": [
        "## RAG 检索：\n\n以下是改编自 [LangChain 表达式语言检索食谱条目](\nhttps://python.langchain.com/docs/expression_language/cookbook/retrieval) 的初始示例，使用了 AI Foundation Models 的 [Mixtral 8x7B Instruct](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/mixtral-8x7b) 和 [NVIDIA Retrieval QA Embedding](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nvolve-40k) 模型，这些模型在其 Playground 环境中可用。食谱后续的示例也能如期运行，我们鼓励您使用这些选项进行探索。\n\n**提示：** 我们建议将 Mixtral 用于内部推理（例如，数据提取、工具选择等的指令遵循），并将 Llama-Chat 用于一个简单的最终“基于历史和上下文为用户生成响应”的回复。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn_zeRGP64DJ"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain faiss-cpu tiktoken langchain_community\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zIXyr9Vd7CED",
        "outputId": "a8d36812-c3e0-4fd4-804a-4b5ba43948e5"
      },
      "outputs": [],
      "source": [
        "vectorstore = FAISS.from_texts(\n",
        "    [\"harrison worked at kensho\"],\n",
        "    embedding=NVIDIAEmbeddings(model=\"NV-Embed-QA\"),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\",\n",
        "        ),\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatNVIDIA(model=\"ai-mixtral-8x7b-instruct\")\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke(\"where did harrison work?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OuY62kJ28oNK",
        "outputId": "672ff6df-64d8-442b-9143-f69dbc09f763"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer using information solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\"\n",
        "            \"\\nSpeak only in the following language: {language}\",\n",
        "        ),\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"language\": itemgetter(\"language\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}