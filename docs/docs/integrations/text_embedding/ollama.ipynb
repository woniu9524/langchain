{
  "cells": [
    {
      "cell_type": "raw",
      "id": "afaf8039",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_label: Ollama\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a3d6f34",
      "metadata": {},
      "source": [
        "# OllamaEmbeddings\n\n这可以帮助您入门使用 LangChain 的 Ollama embedding 模型。有关 `OllamaEmbeddings` 功能和配置选项的详细文档，请参阅 [API 参考](https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html)。\n\n## 概述\n### 集成详情\n\nimport { ItemTable } from \"@theme/FeatureTables\";\n\n<ItemTable category=\"text_embedding\" item=\"Ollama\" />\n\n## 设置\n\n首先，请按照[这些说明](https://github.com/ollama/ollama?tab=readme-ov-file#ollama)来设置和运行本地 Ollama 实例：\n\n* [下载](https://ollama.ai/download)并安装 Ollama 到可用的支持平台（包括 Windows Subsystem for Linux，即 WSL、macOS 和 Linux）\n    * macOS 用户可以通过 Homebrew 安装，命令为 `brew install ollama`，启动命令为 `brew services start ollama`\n* 通过 `ollama pull <模型名称>` 获取可用的 LLM 模型\n    * 通过[模型库](https://ollama.ai/library)查看可用模型列表\n    * 例如：`ollama pull llama3`\n* 这将下载模型的默认标记版本。通常，默认版本指向最新、最小参数量的模型。\n\n> 在 Mac 上，模型将下载到 `~/.ollama/models`\n>\n> 在 Linux（或 WSL）上，模型将存储在 `/usr/share/ollama/.ollama/models`\n\n* 如此指定您感兴趣的模型的确切版本 `ollama pull vicuna:13b-v1.5-16k-q4_0`（在此实例中查看[“Vicuna”模型各种标签](https://ollama.ai/library/vicuna/tags)）\n* 要查看所有已拉取的模型，请使用 `ollama list`\n* 要直接从命令行与模型进行对话，请使用 `ollama run <模型名称>`\n* 查看[Ollama 文档](https://github.com/ollama/ollama/tree/main/docs)以获取更多命令。您可以在终端中运行 `ollama help` 来查看可用命令。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c84fb993",
      "metadata": {},
      "source": [
        "要启用模型调用的自动跟踪，请设置您的 [LangSmith](https://docs.smith.langchain.com/) API 密钥："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "39a4953b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9664366",
      "metadata": {},
      "source": [
        "### 安装\n\nLangChain Ollama 集成位于 `langchain-ollama` 包中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "64853226",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45dd1724",
      "metadata": {},
      "source": [
        "## 实例化\n\n现在我们可以实例化我们的模型对象并生成嵌入："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9ea7a09b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "embeddings = OllamaEmbeddings(\n",
        "    model=\"llama3\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d271b6",
      "metadata": {},
      "source": [
        "## 索引和检索\n\n嵌入模型常用于检索增强生成（RAG）流程中，既作为索引数据的一部分，也用于后续检索数据。更详细的说明，请参阅我们的 [RAG 教程](/docs/tutorials/rag/)。\n\n下面，请看如何使用我们之前初始化的 `embeddings` 对象来索引和检索数据。在本例中，我们将索引和检索 `InMemoryVectorStore` 中的示例文档。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d817716b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain is the framework for building context-aware reasoning applications\n"
          ]
        }
      ],
      "source": [
        "# Create a vector store with a sample text\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
        "\n",
        "vectorstore = InMemoryVectorStore.from_texts(\n",
        "    [text],\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "# Use the vectorstore as a retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Retrieve the most similar text\n",
        "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
        "\n",
        "# Show the retrieved document's content\n",
        "print(retrieved_documents[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e02b9855",
      "metadata": {},
      "source": [
        "## 直接使用\n\n在底层，vectorstore 和 retriever 实现会分别调用 `embeddings.embed_documents(...)` 和 `embeddings.embed_query(...)` 来为 `from_texts` 和检索 `invoke` 操作中使用的文本创建 embedding。\n\n您可以直接调用这些方法来为自己的用例获取 embedding。\n\n### 嵌入单个文本\n\n您可以使用 `embed_query` 来嵌入单个文本或文档："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0d2befcd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.0039849705, 0.023019705, -0.001768838, -0.0058736936, 0.00040999008, 0.017861595, -0.011274585, \n"
          ]
        }
      ],
      "source": [
        "single_vector = embeddings.embed_query(text)\n",
        "print(str(single_vector)[:100])  # Show the first 100 characters of the vector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b5a7d03",
      "metadata": {},
      "source": [
        "### 嵌入多个文本\n\n您可以使用 `embed_documents` 嵌入多个文本："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2f4d6e97",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.0039849705, 0.023019705, -0.001768838, -0.0058736936, 0.00040999008, 0.017861595, -0.011274585, \n",
            "[-0.0066985516, 0.009878328, 0.008019467, -0.009384944, -0.029560851, 0.025744654, 0.004872892, -0.0\n"
          ]
        }
      ],
      "source": [
        "text2 = (\n",
        "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
        ")\n",
        "two_vectors = embeddings.embed_documents([text, text2])\n",
        "for vector in two_vectors:\n",
        "    print(str(vector)[:100])  # Show the first 100 characters of the vector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98785c12",
      "metadata": {},
      "source": [
        "## API 参考\n\n有关 `OllamaEmbeddings` 功能和配置选项的详细文档，请参阅[API 参考](https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html)。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}