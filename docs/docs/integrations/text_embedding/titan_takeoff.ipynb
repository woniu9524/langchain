{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Titan Takeoff\n\n`TitanML` 通过我们的训练、压缩和推理优化平台，帮助企业构建和部署更好、更小、更便宜、更快的 NLP 模型。\n\n我们的推理服务器 [Titan Takeoff](https://docs.titanml.co/docs/intro) 能够通过单个命令在您的本地硬件上部署 LLM。大多数嵌入模型都支持开箱即用，如果您在使用特定模型时遇到问题，请通过 hello@titanml.co 告诉我们。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 示例用法\n以下是一些帮助您开始使用 Titan Takeoff Server 的示例。在运行这些命令之前，您需要确保 Takeoff Server 已经在后台启动。有关更多信息，请参阅 [启动 Takeoff 的文档页面](https://docs.titanml.co/docs/Docs/launching/)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "from langchain_community.embeddings import TitanTakeoffEmbed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例 1\n基本用法，假设 Takeoff 正在您的机器上运行并使用其默认端口（例如 localhost:3000）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed = TitanTakeoffEmbed()\n",
        "output = embed.embed_query(\n",
        "    \"What is the weather in London in August?\", consumer_group=\"embed\"\n",
        ")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例 2\n使用 TitanTakeoffEmbed Python Wrapper 启动 readers。如果您在首次启动 Takeoff 时未创建任何 readers，或者想添加其他 readers，可以在初始化 TitanTakeoffEmbed 对象时进行。只需将您想启动的模型列表作为 `models` 参数传递即可。\n\n您可以使用 `embed.query_documents` 来一次性嵌入多个文档。预期的输入是字符串列表，而不是 `embed_query` 方法所期望的单个字符串。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model config for the embedding model, where you can specify the following parameters:\n",
        "#   model_name (str): The name of the model to use\n",
        "#   device: (str): The device to use for inference, cuda or cpu\n",
        "#   consumer_group (str): The consumer group to place the reader into\n",
        "embedding_model = {\n",
        "    \"model_name\": \"BAAI/bge-large-en-v1.5\",\n",
        "    \"device\": \"cpu\",\n",
        "    \"consumer_group\": \"embed\",\n",
        "}\n",
        "embed = TitanTakeoffEmbed(models=[embedding_model])\n",
        "\n",
        "# The model needs time to spin up, length of time need will depend on the size of model and your network connection speed\n",
        "time.sleep(60)\n",
        "\n",
        "prompt = \"What is the capital of France?\"\n",
        "# We specified \"embed\" consumer group so need to send request to the same consumer group so it hits our embedding model and not others\n",
        "output = embed.embed_query(prompt, consumer_group=\"embed\")\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}