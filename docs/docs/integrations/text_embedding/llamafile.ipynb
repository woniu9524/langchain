{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "278b6c63",
      "metadata": {},
      "source": [
        "# llamafile\n",
        "\n",
        "让我们加载 [llamafile](https://github.com/Mozilla-Ocho/llamafile) Embeddings 类。\n",
        "\n",
        "## 设置\n",
        "\n",
        "首先，有 3 个设置步骤：\n",
        "\n",
        "1. 下载一个 llamafile。在此 notebook 中，我们使用 `TinyLlama-1.1B-Chat-v1.0.Q5_K_M`，但 [HuggingFace](https://huggingface.co/models?other=llamafile) 上还有许多其他选项。\n",
        "2. 使 llamafile 可执行。\n",
        "3. 以服务器模式启动 llamafile。\n",
        "\n",
        "您可以运行以下 bash 脚本来完成所有这些操作："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43ef6dfa-9cc4-4552-8a53-5df523afae7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# llamafile setup\n",
        "\n",
        "# Step 1: Download a llamafile. The download may take several minutes.\n",
        "wget -nv -nc https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n",
        "\n",
        "# Step 2: Make the llamafile executable. Note: if you're on Windows, just append '.exe' to the filename.\n",
        "chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n",
        "\n",
        "# Step 3: Start llamafile server in background. All the server logs will be written to 'tinyllama.log'.\n",
        "# Alternatively, you can just open a separate terminal outside this notebook and run: \n",
        "#   ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding\n",
        "./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding > tinyllama.log 2>&1 &\n",
        "pid=$!\n",
        "echo \"${pid}\" > .llamafile_pid  # write the process pid to a file so we can terminate the server later"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3188b22f-879f-47b3-9a27-24412f6fad5f",
      "metadata": {},
      "source": [
        "## 使用 LlamafileEmbeddings 进行文本嵌入\n",
        "\n",
        "现在，我们可以使用 `LlamafileEmbeddings` 类来与 llamafile 服务器进行交互，该服务器目前正在 http://localhost:8080 托管我们的 TinyLlama 模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be1af71",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import LlamafileEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c66e5da",
      "metadata": {},
      "outputs": [],
      "source": [
        "embedder = LlamafileEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01370375",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"This is a test document.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a42e4035",
      "metadata": {},
      "source": [
        "要生成嵌入，您可以查询单个文本，也可以查询文本列表。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91bc875d-829b-4c3d-8e6f-fc2dda30a3bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_result = embedder.embed_query(text)\n",
        "query_result[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4b0d49e-0c73-44b6-aed5-5b426564e085",
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_result = embedder.embed_documents([text])\n",
        "doc_result[0][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ccc78fc-03ae-411d-ae73-74a4ee91c725",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# cleanup: kill the llamafile server process\n",
        "kill $(cat .llamafile_pid)\n",
        "rm .llamafile_pid"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e971737741ff4ec9aff7dc6155a1060a59a8a6d52c757dbbe66bf8ee389494b1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
