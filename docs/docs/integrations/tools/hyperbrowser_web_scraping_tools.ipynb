{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\nsidebar_label: 超级浏览器网页抓取工具\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperbrowser 网页抓取工具\n\n[Hyperbrowser](https://hyperbrowser.ai) 是一个用于运行和扩展无头浏览器的平台。它允许您大规模启动和管理浏览器会话，并为任何网页抓取需求提供易于使用的解决方案，例如抓取单个页面或爬取整个网站。\n\n主要特点：\n\n- 即时可扩展性 - 无需基础设施方面的烦恼，即可在几秒钟内启动数百个浏览器会话\n- 简单集成 - 可与 Puppeteer 和 Playwright 等流行工具无缝协作\n- 强大的 API - 易于使用的 API，可用于抓取/爬取任何网站，以及更多功能\n- 绕过反机器人措施 - 内置隐身模式、广告拦截、自动验证码解决和代理轮换\n\n本笔记提供了使用 Hyperbrowser 网页工具入门的快速概述。\n\n有关 Hyperbrowser 的更多信息，请访问 [Hyperbrowser 网站](https://hyperbrowser.ai)，或者如果您想查看文档，可以访问 [Hyperbrowser 文档](https://docs.hyperbrowser.ai)。\n\n## 主要功能\n\n### 抓取 (Scrape)\n\nHyperbrowser 提供强大的抓取功能，可让您从任何网页中提取数据。抓取工具可以将网页内容转换为 Markdown 或 HTML 等结构化格式，从而便于处理和分析数据。\n\n### 爬取 (Crawl)\n\n爬取功能使您能够自动浏览网站的多个页面。您可以设置页面限制等参数来控制爬虫对网站的探索程度，并从其访问的每个页面收集数据。\n\n### 提取 (Extract)\n\nHyperbrowser 的提取功能利用人工智能根据您定义的模式从网页中提取特定信息。这使您可以将非结构化的网页内容转换为符合您确切要求的数据。\n\n## 概述\n\n### 集成详情\n\n| 工具 (Tool)    | 包 (Package)           | 本地 (Local) | 可序列化 (Serializable) | JS 支持 (JS support) |\n| :------------- | :--------------------- | :----------: | :---------------------: | :------------------: |\n| 爬取工具 (Crawl Tool) | langchain-hyperbrowser |      ❌       |           ❌            |          ❌          |\n| 抓取工具 (Scrape Tool) | langchain-hyperbrowser |      ❌       |           ❌            |          ❌          |\n| 提取工具 (Extract Tool) | langchain-hyperbrowser |      ❌       |           ❌            |          ❌          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 设置\n\n要访问 Hyperbrowser Web 工具，您需要安装 `langchain-hyperbrowser` 集成包，并创建一个 Hyperbrowser 账户并获取 API 密钥。\n\n### 凭证\n\n前往 [Hyperbrowser](https://app.hyperbrowser.ai/) 注册并生成 API 密钥。完成此操作后，设置 HYPERBROWSER_API_KEY 环境变量：\n\n```bash\nexport HYPERBROWSER_API_KEY=<your-api-key>\n```\n\n### 安装\n\n安装 **langchain-hyperbrowser**。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU langchain-hyperbrowser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 实例化\n\n### 爬虫工具\n\n`HyperbrowserCrawlTool` 是一款强大的工具，可以从给定的 URL 开始爬取整个网站。它支持可配置的页面限制和抓取选项。\n\n```python\nfrom langchain_hyperbrowser import HyperbrowserCrawlTool\ntool = HyperbrowserCrawlTool()\n```\n\n### 抓取工具\n\n`HyperbrowserScrapeTool` 是一款可以抓取网页内容的工具。它支持 Markdown 和 HTML 输出格式，并能提取元数据。\n\n```python\nfrom langchain_hyperbrowser import HyperbrowserScrapeTool\ntool = HyperbrowserScrapeTool()\n```\n\n### 提取工具\n\n`HyperbrowserExtractTool` 是一款强大的工具，它利用 AI 从网页中提取结构化数据。它可以根据预定义的模式提取信息。\n\n```python\nfrom langchain_hyperbrowser import HyperbrowserExtractTool\ntool = HyperbrowserExtractTool()\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 调用\n\n### 基本用法\n\n#### 爬虫工具"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': [CrawledPage(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\\n\\n# Example Domain\\n\\nThis domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.\\n\\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None, url='https://example.com', status='completed', error=None)], 'error': None}\n"
          ]
        }
      ],
      "source": [
        "from langchain_hyperbrowser import HyperbrowserCrawlTool\n",
        "\n",
        "result = HyperbrowserCrawlTool().invoke(\n",
        "    {\n",
        "        \"url\": \"https://example.com\",\n",
        "        \"max_pages\": 2,\n",
        "        \"scrape_options\": {\"formats\": [\"markdown\"]},\n",
        "    }\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scrape Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': ScrapeJobData(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\\n\\n# Example Domain\\n\\nThis domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.\\n\\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None), 'error': None}\n"
          ]
        }
      ],
      "source": [
        "from langchain_hyperbrowser import HyperbrowserScrapeTool\n",
        "\n",
        "result = HyperbrowserScrapeTool().invoke(\n",
        "    {\"url\": \"https://example.com\", \"scrape_options\": {\"formats\": [\"markdown\"]}}\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 提取工具"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': {'title': 'Example Domain'}, 'error': None}\n"
          ]
        }
      ],
      "source": [
        "from langchain_hyperbrowser import HyperbrowserExtractTool\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class SimpleExtractionModel(BaseModel):\n",
        "    title: str\n",
        "\n",
        "\n",
        "result = HyperbrowserExtractTool().invoke(\n",
        "    {\n",
        "        \"url\": \"https://example.com\",\n",
        "        \"schema\": SimpleExtractionModel,\n",
        "    }\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 自定义选项\n\n#### 爬虫工具（附带自定义选项）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': [CrawledPage(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\\n\\n# Example Domain\\n\\nThis domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.\\n\\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None, url='https://example.com', status='completed', error=None)], 'error': None}\n"
          ]
        }
      ],
      "source": [
        "result = HyperbrowserCrawlTool().run(\n",
        "    {\n",
        "        \"url\": \"https://example.com\",\n",
        "        \"max_pages\": 2,\n",
        "        \"scrape_options\": {\n",
        "            \"formats\": [\"markdown\", \"html\"],\n",
        "        },\n",
        "        \"session_options\": {\"use_proxy\": True, \"solve_captchas\": True},\n",
        "    }\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 刮刀工具（Scrape Tool）及自定义选项"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': ScrapeJobData(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html='<html><head>\\n    <title>Example Domain</title>\\n\\n    <meta charset=\"utf-8\">\\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n        \\n</head>\\n\\n<body>\\n<div>\\n    <h1>Example Domain</h1>\\n    <p>This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.</p>\\n    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\\n</div>\\n\\n\\n</body></html>', markdown='Example Domain\\n\\n# Example Domain\\n\\nThis domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.\\n\\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None), 'error': None}\n"
          ]
        }
      ],
      "source": [
        "result = HyperbrowserScrapeTool().run(\n",
        "    {\n",
        "        \"url\": \"https://example.com\",\n",
        "        \"scrape_options\": {\n",
        "            \"formats\": [\"markdown\", \"html\"],\n",
        "        },\n",
        "        \"session_options\": {\"use_proxy\": True, \"solve_captchas\": True},\n",
        "    }\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 提取工具与自定义模式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': {'products': [{'price': 9.99, 'title': 'Essence Mascara Lash Princess'}, {'price': 19.99, 'title': 'Eyeshadow Palette with Mirror'}, {'price': 14.99, 'title': 'Powder Canister'}, {'price': 12.99, 'title': 'Red Lipstick'}, {'price': 8.99, 'title': 'Red Nail Polish'}, {'price': 49.99, 'title': 'Calvin Klein CK One'}, {'price': 129.99, 'title': 'Chanel Coco Noir Eau De'}, {'price': 89.99, 'title': \"Dior J'adore\"}, {'price': 69.99, 'title': 'Dolce Shine Eau de'}, {'price': 79.99, 'title': 'Gucci Bloom Eau de'}]}, 'error': None}\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class ProductSchema(BaseModel):\n",
        "    title: str\n",
        "    price: float\n",
        "\n",
        "\n",
        "class ProductsSchema(BaseModel):\n",
        "    products: List[ProductSchema]\n",
        "\n",
        "\n",
        "result = HyperbrowserExtractTool().run(\n",
        "    {\n",
        "        \"url\": \"https://dummyjson.com/products?limit=10\",\n",
        "        \"schema\": ProductsSchema,\n",
        "        \"session_options\": {\"session_options\": {\"use_proxy\": True}},\n",
        "    }\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 异步用法\n\n所有工具都支持异步用法："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'List' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_hyperbrowser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     HyperbrowserCrawlTool,\n\u001b[32m      3\u001b[39m     HyperbrowserExtractTool,\n\u001b[32m      4\u001b[39m     HyperbrowserScrapeTool,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mExtractionSchema\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpopular_library_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mweb_operations\u001b[39m():\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Crawl\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mExtractionSchema\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mExtractionSchema\u001b[39;00m(BaseModel):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     popular_library_name: \u001b[43mList\u001b[49m[\u001b[38;5;28mstr\u001b[39m]\n",
            "\u001b[31mNameError\u001b[39m: name 'List' is not defined"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_hyperbrowser import (\n",
        "    HyperbrowserCrawlTool,\n",
        "    HyperbrowserExtractTool,\n",
        "    HyperbrowserScrapeTool,\n",
        ")\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class ExtractionSchema(BaseModel):\n",
        "    popular_library_name: List[str]\n",
        "\n",
        "\n",
        "async def web_operations():\n",
        "    # Crawl\n",
        "    crawl_tool = HyperbrowserCrawlTool()\n",
        "    crawl_result = await crawl_tool.arun(\n",
        "        {\n",
        "            \"url\": \"https://example.com\",\n",
        "            \"max_pages\": 5,\n",
        "            \"scrape_options\": {\"formats\": [\"markdown\"]},\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Scrape\n",
        "    scrape_tool = HyperbrowserScrapeTool()\n",
        "    scrape_result = await scrape_tool.arun(\n",
        "        {\"url\": \"https://example.com\", \"scrape_options\": {\"formats\": [\"markdown\"]}}\n",
        "    )\n",
        "\n",
        "    # Extract\n",
        "    extract_tool = HyperbrowserExtractTool()\n",
        "    extract_result = await extract_tool.arun(\n",
        "        {\n",
        "            \"url\": \"https://npmjs.com\",\n",
        "            \"schema\": ExtractionSchema,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return crawl_result, scrape_result, extract_result\n",
        "\n",
        "\n",
        "results = await web_operations()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 在代理中使用\n\n以下是如何在代理中使用任何 Web 工具："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Crawl https://example.com and get content from up to 5 pages\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  hyperbrowser_crawl_data (call_G2ofdHOqjdnJUZu4hhbuga58)\n",
            " Call ID: call_G2ofdHOqjdnJUZu4hhbuga58\n",
            "  Args:\n",
            "    url: https://example.com\n",
            "    max_pages: 5\n",
            "    scrape_options: {'formats': ['markdown']}\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: hyperbrowser_crawl_data\n",
            "\n",
            "{'data': [CrawledPage(metadata={'url': 'https://www.example.com/', 'title': 'Example Domain', 'viewport': 'width=device-width, initial-scale=1', 'sourceURL': 'https://example.com'}, html=None, markdown='Example Domain\\n\\n# Example Domain\\n\\nThis domain is for use in illustrative examples in documents. You may use this\\ndomain in literature without prior coordination or asking for permission.\\n\\n[More information...](https://www.iana.org/domains/example)', links=None, screenshot=None, url='https://example.com', status='completed', error=None)], 'error': None}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I have crawled the website [https://example.com](https://example.com) and retrieved content from the first page. Here is the content in markdown format:\n",
            "\n",
            "```\n",
            "Example Domain\n",
            "\n",
            "# Example Domain\n",
            "\n",
            "This domain is for use in illustrative examples in documents. You may use this\n",
            "domain in literature without prior coordination or asking for permission.\n",
            "\n",
            "[More information...](https://www.iana.org/domains/example)\n",
            "```\n",
            "\n",
            "If you would like to crawl more pages or need additional information, please let me know!\n"
          ]
        }
      ],
      "source": [
        "from langchain_hyperbrowser import HyperbrowserCrawlTool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Initialize the crawl tool\n",
        "crawl_tool = HyperbrowserCrawlTool()\n",
        "\n",
        "# Create the agent with the crawl tool\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "agent = create_react_agent(llm, [crawl_tool])\n",
        "user_input = \"Crawl https://example.com and get content from up to 5 pages\"\n",
        "for step in agent.stream(\n",
        "    {\"messages\": user_input},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 配置选项\n\n### 通用选项\n\n所有工具都支持这些基本配置选项：\n\n- `url`: 要处理的 URL\n- `session_options`: 浏览器会话配置\n  - `use_proxy`: 是否使用代理\n  - `solve_captchas`: 是否自动解决 CAPTCHA\n  - `accept_cookies`: 是否接受 cookie\n\n### 工具特定选项\n\n#### Crawl 工具\n\n- `max_pages`: 要爬取的最大页面数\n- `scrape_options`: 抓取每个页面的选项\n  - `formats`: 输出格式列表 (markdown, html)\n\n#### Scrape 工具\n\n- `scrape_options`: 抓取页面的选项\n  - `formats`: 输出格式列表 (markdown, html)\n\n#### Extract 工具\n\n- `schema`: 定义要提取的结构的 Pydantic 模型\n- `extraction_prompt`: 用于提取的自然语言提示\n\n更多详情，请参阅相应的 API 参考：\n\n- [Crawl API Reference](https://docs.hyperbrowser.ai/reference/api-reference/crawl)\n- [Scrape API Reference](https://docs.hyperbrowser.ai/reference/api-reference/scrape)\n- [Extract API Reference](https://docs.hyperbrowser.ai/reference/api-reference/extract)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API 参考\n\n- [GitHub](https://github.com/hyperbrowserai/langchain-hyperbrowser/)\n- [PyPi](https://pypi.org/project/langchain-hyperbrowser/)\n- [Hyperbrowser Docs](https://docs.hyperbrowser.ai/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-hyperbrowser-AlekOQAq-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}