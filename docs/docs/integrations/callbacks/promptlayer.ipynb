{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PromptLayer\n\n>[PromptLayer](https://docs.promptlayer.com/introduction) æ˜¯ä¸€ä¸ªç”¨äº Prompt Engineering çš„å¹³å°ã€‚å®ƒè¿˜æä¾› LLM å¯è§‚æµ‹æ€§ï¼Œç”¨äºå¯è§†åŒ–è¯·æ±‚ã€ç‰ˆæœ¬åŒ– Prompt ä»¥åŠè·Ÿè¸ªä½¿ç”¨æƒ…å†µã€‚\n\nè™½ç„¶ `PromptLayer` ç¡®å®æœ‰ä¸ LangChain ç›´æ¥é›†æˆçš„ LLMï¼ˆä¾‹å¦‚ [`PromptLayerOpenAI`](/docs/integrations/llms/promptlayer_openai)ï¼‰ï¼Œä½†ä½¿ç”¨å›è°ƒæ˜¯é›†æˆ `PromptLayer` ä¸ LangChain çš„æ¨èæ–¹å¼ã€‚\n\nåœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•è®¾ç½® `PromptLayerCallbackHandler`ã€‚\n\næ›´å¤šä¿¡æ¯è¯·å‚é˜… [PromptLayer æ–‡æ¡£](https://docs.promptlayer.com/languages/langchain)ã€‚"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## å®‰è£…ä¸è®¾ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain-community promptlayer --upgrade"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### è·å– API å‡­è¯\n\nå¦‚æœæ‚¨è¿˜æ²¡æœ‰ PromptLayer è´¦æˆ·ï¼Œè¯·å…ˆåœ¨ [promptlayer.com](https://www.promptlayer.com) ä¸Šåˆ›å»ºä¸€ä¸ªã€‚ç„¶åï¼Œç‚¹å‡»å¯¼èˆªæ ä¸­çš„è®¾ç½®é½¿è½®å›¾æ ‡æ¥è·å– API å¯†é’¥ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºåä¸º `PROMPTLAYER_API_KEY` çš„ç¯å¢ƒå˜é‡ã€‚"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç”¨æ³•\n\nå¼€å§‹ä½¿ç”¨ `PromptLayerCallbackHandler` éå¸¸ç®€å•ï¼Œå®ƒæ¥å—ä¸¤ä¸ªå¯é€‰å‚æ•°ï¼š\n1. `pl_tags` - ä¸€ä¸ªå¯é€‰çš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå°†åœ¨ PromptLayer ä¸Šä½œä¸ºæ ‡ç­¾è¿›è¡Œè·Ÿè¸ªã€‚\n2. `pl_id_callback` - ä¸€ä¸ªå¯é€‰çš„å‡½æ•°ï¼Œå®ƒå°†ä»¥ `promptlayer_request_id` ä½œä¸ºå‚æ•°ã€‚æ­¤ ID å¯ä¸ PromptLayer çš„æ‰€æœ‰è·Ÿè¸ªåŠŸèƒ½ä¸€èµ·ç”¨äºè·Ÿè¸ªã€å…ƒæ•°æ®ã€åˆ†æ•°å’Œæç¤ºä½¿ç”¨æƒ…å†µã€‚"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç®€å•çš„ OpenAI ç¤ºä¾‹\n\nåœ¨è¿™ä¸ªç®€å•çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ `ChatOpenAI` å’Œ `PromptLayerCallbackHandler`ã€‚æˆ‘ä»¬æ·»åŠ ä¸€ä¸ªåä¸º `chatopenai` çš„ PromptLayer æ ‡ç­¾ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-06T19:10:56.673622Z",
          "iopub.status.busy": "2024-03-06T19:10:56.673421Z",
          "iopub.status.idle": "2024-03-06T19:10:56.887519Z",
          "shell.execute_reply": "2024-03-06T19:10:56.886895Z",
          "shell.execute_reply.started": "2024-03-06T19:10:56.673608Z"
        }
      },
      "outputs": [],
      "source": [
        "import promptlayer  # Don't forget this ğŸ°\n",
        "from langchain_community.callbacks.promptlayer_callback import (\n",
        "    PromptLayerCallbackHandler,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    callbacks=[PromptLayerCallbackHandler(pl_tags=[\"chatopenai\"])],\n",
        ")\n",
        "llm_results = chat_llm.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"What comes after 1,2,3 ?\"),\n",
        "        HumanMessage(content=\"Tell me another joke?\"),\n",
        "    ]\n",
        ")\n",
        "print(llm_results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPT4All ç¤ºä¾‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms import GPT4All\n",
        "\n",
        "model = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8)\n",
        "callbacks = [PromptLayerCallbackHandler(pl_tags=[\"langchain\", \"gpt4all\"])]\n",
        "\n",
        "response = model.invoke(\n",
        "    \"Once upon a time, \",\n",
        "    config={\"callbacks\": callbacks},\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## åŠŸèƒ½é½å…¨çš„ç¤ºä¾‹\n\nåœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†è§£é” `PromptLayer` çš„æ›´å¤šå¼ºå¤§åŠŸèƒ½ã€‚\n\nPromptLayer å…è®¸æ‚¨ä»¥å¯è§†åŒ–çš„æ–¹å¼åˆ›å»ºã€ç‰ˆæœ¬åŒ–å’Œè·Ÿè¸ªæç¤ºæ¨¡æ¿ã€‚ä½¿ç”¨ [Prompt Registry](https://docs.promptlayer.com/features/prompt-registry)ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥ç¼–ç¨‹æ–¹å¼è·å–åä¸º `example` çš„æç¤ºæ¨¡æ¿ã€‚\n\næˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ª `pl_id_callback` å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥æ”¶ `promptlayer_request_id` å¹¶è®°å½•åˆ†æ•°ã€å…ƒæ•°æ®ä»¥åŠé“¾æ¥æ‰€ä½¿ç”¨çš„æç¤ºæ¨¡æ¿ã€‚åœ¨[æˆ‘ä»¬çš„æ–‡æ¡£](https://docs.promptlayer.com/features/prompt-history/request-id)ä¸­é˜…è¯»æœ‰å…³è·Ÿè¸ªçš„æ›´å¤šä¿¡æ¯ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "\n",
        "def pl_id_callback(promptlayer_request_id):\n",
        "    print(\"prompt layer id \", promptlayer_request_id)\n",
        "    promptlayer.track.score(\n",
        "        request_id=promptlayer_request_id, score=100\n",
        "    )  # score is an integer 0-100\n",
        "    promptlayer.track.metadata(\n",
        "        request_id=promptlayer_request_id, metadata={\"foo\": \"bar\"}\n",
        "    )  # metadata is a dictionary of key value pairs that is tracked on PromptLayer\n",
        "    promptlayer.track.prompt(\n",
        "        request_id=promptlayer_request_id,\n",
        "        prompt_name=\"example\",\n",
        "        prompt_input_variables={\"product\": \"toasters\"},\n",
        "        version=1,\n",
        "    )  # link the request to a prompt template\n",
        "\n",
        "\n",
        "openai_llm = OpenAI(\n",
        "    model_name=\"gpt-3.5-turbo-instruct\",\n",
        "    callbacks=[PromptLayerCallbackHandler(pl_id_callback=pl_id_callback)],\n",
        ")\n",
        "\n",
        "example_prompt = promptlayer.prompts.get(\"example\", version=1, langchain=True)\n",
        "openai_llm.invoke(example_prompt.format(product=\"toasters\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "å°±æ˜¯è¿™æ ·ï¼è®¾ç½®å®Œæˆåï¼Œæ‚¨çš„æ‰€æœ‰è¯·æ±‚éƒ½ä¼šæ˜¾ç¤ºåœ¨ PromptLayer ä»ªè¡¨æ¿ä¸Šã€‚\næ­¤å›è°ƒä¹Ÿé€‚ç”¨äº LangChain ä¸Šå®ç°çš„ä»»ä½• LLMã€‚"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "c4fe2cd85a8d9e8baaec5340ce66faff1c77581a9f43e6c45e85e09b6fced008"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}