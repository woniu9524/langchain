{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure ML\n\n[Azure ML](https://azure.microsoft.com/en-us/products/machine-learning/) 是一个用于构建、训练和部署机器学习模型的平台。用户可以在模型目录中探索要部署的模型类型，该目录提供了来自不同提供商的基础模型和通用模型。\n\n本笔记本将介绍如何使用托管在 `Azure ML Online Endpoint` 上的 LLM。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##Installing the langchain packages needed to use the integration\n",
        "%pip install -qU langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms.azureml_endpoint import AzureMLOnlineEndpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 设置\n\n您必须[在 Azure ML 上部署模型](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-foundation-models?view=azureml-api-2#deploying-foundation-models-to-endpoints-for-inferencing)或[部署到 Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-open)，并获取以下参数：\n\n* `endpoint_url`: 端点提供的 REST 端点 URL。\n* `endpoint_api_type`: 将模型部署到 **专用端点**（托管的托管基础结构）时，请使用 `endpoint_type='dedicated'`。使用 **即用即付**（模型即服务）产品部署模型时，请使用 `endpoint_type='serverless'`。\n* `endpoint_api_key`: 端点提供的 API 密钥。\n* `deployment_name`: (可选) 使用端点的模型的部署名称。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 内容格式化程序\n\n`content_formatter` 参数是一个处理程序类，用于转换 AzureML 终结点的请求和响应，以匹配所需架构。由于模型目录中存在各种各样的模型，每个模型可能以不同于其他模型的方式处理数据，因此提供了一个 `ContentFormatterBase` 类，允许用户根据自己的喜好转换数据。提供以下内容格式化程序：\n\n* `GPT2ContentFormatter`: 为 GPT2 格式化请求和响应数据\n* `DollyContentFormatter`: 为 Dolly-v2 格式化请求和响应数据\n* `HFContentFormatter`: 为文本生成 Hugging Face 模型格式化请求和响应数据\n* `CustomOpenAIContentFormatter`: 为 LLaMa2 等遵循 OpenAI API 兼容方案的模型格式化请求和响应数据。\n\n*注意：`OSSContentFormatter` 正在被弃用，并被 `GPT2ContentFormatter` 取代。逻辑相同，但 `GPT2ContentFormatter` 是一个更合适的名称。您仍然可以继续使用 `OSSContentFormatter`，因为这些更改是向后兼容的。*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 示例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例：使用实时终结点进行 LlaMa 2 补全"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms.azureml_endpoint import (\n",
        "    AzureMLEndpointApiType,\n",
        "    CustomOpenAIContentFormatter,\n",
        ")\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "llm = AzureMLOnlineEndpoint(\n",
        "    endpoint_url=\"https://<your-endpoint>.<your_region>.inference.ml.azure.com/score\",\n",
        "    endpoint_api_type=AzureMLEndpointApiType.dedicated,\n",
        "    endpoint_api_key=\"my-api-key\",\n",
        "    content_formatter=CustomOpenAIContentFormatter(),\n",
        "    model_kwargs={\"temperature\": 0.8, \"max_new_tokens\": 400},\n",
        ")\n",
        "response = llm.invoke(\"Write me a song about sparkling water:\")\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "模型参数也可以在调用时指定："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = llm.invoke(\"Write me a song about sparkling water:\", temperature=0.5)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例：按量付费部署的聊天补全（模型即服务）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms.azureml_endpoint import (\n",
        "    AzureMLEndpointApiType,\n",
        "    CustomOpenAIContentFormatter,\n",
        ")\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "llm = AzureMLOnlineEndpoint(\n",
        "    endpoint_url=\"https://<your-endpoint>.<your_region>.inference.ml.azure.com/v1/completions\",\n",
        "    endpoint_api_type=AzureMLEndpointApiType.serverless,\n",
        "    endpoint_api_key=\"my-api-key\",\n",
        "    content_formatter=CustomOpenAIContentFormatter(),\n",
        "    model_kwargs={\"temperature\": 0.8, \"max_new_tokens\": 400},\n",
        ")\n",
        "response = llm.invoke(\"Write me a song about sparkling water:\")\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例：自定义内容格式化程序\n\n下面是使用 Hugging Face 摘要模型的一个示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from typing import Dict\n",
        "\n",
        "from langchain_community.llms.azureml_endpoint import (\n",
        "    AzureMLOnlineEndpoint,\n",
        "    ContentFormatterBase,\n",
        ")\n",
        "\n",
        "\n",
        "class CustomFormatter(ContentFormatterBase):\n",
        "    content_type = \"application/json\"\n",
        "    accepts = \"application/json\"\n",
        "\n",
        "    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
        "        input_str = json.dumps(\n",
        "            {\n",
        "                \"inputs\": [prompt],\n",
        "                \"parameters\": model_kwargs,\n",
        "                \"options\": {\"use_cache\": False, \"wait_for_model\": True},\n",
        "            }\n",
        "        )\n",
        "        return str.encode(input_str)\n",
        "\n",
        "    def format_response_payload(self, output: bytes) -> str:\n",
        "        response_json = json.loads(output)\n",
        "        return response_json[0][\"summary_text\"]\n",
        "\n",
        "\n",
        "content_formatter = CustomFormatter()\n",
        "\n",
        "llm = AzureMLOnlineEndpoint(\n",
        "    endpoint_api_type=\"dedicated\",\n",
        "    endpoint_api_key=os.getenv(\"BART_ENDPOINT_API_KEY\"),\n",
        "    endpoint_url=os.getenv(\"BART_ENDPOINT_URL\"),\n",
        "    model_kwargs={\"temperature\": 0.8, \"max_new_tokens\": 400},\n",
        "    content_formatter=content_formatter,\n",
        ")\n",
        "large_text = \"\"\"On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona's \n",
        "next album because of mental health concerns. She was said to be diagnosed with \"intermittent anxiety symptoms\" and would be \n",
        "taking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along \n",
        "with the title track \"So What\".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other \n",
        "songs on the album, including \"365\". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at \n",
        "number 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with \"So What\" on Mnet's \n",
        "M Countdown.[42]\n",
        "\n",
        "On October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single \n",
        "\"Why Not?\". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] \n",
        "The EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released \n",
        "the music video for \"Star\", another song on [12:00].[46] Peaking at number 40, \"Star\" is Loona's first entry on the Billboard \n",
        "Mainstream Top 40, making them the second K-pop girl group to enter the chart.[47]\n",
        "\n",
        "On June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).\n",
        "[48] The following day, on June 2, a teaser was posted to Loona's official social media accounts showing twelve sets of eyes, \n",
        "confirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, \n",
        "Choerry, and Go Won released the song \"Yum-Yum\" as a collaboration with Cocomong.[50] On September 8, they released another \n",
        "collaboration song named \"Yummy-Yummy\".[51] On June 27, 2021, Loona announced at the end of their special clip that they are \n",
        "making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced \n",
        "that Loona will release the double A-side single, \"Hula Hoop / Star Seed\" on September 15, with a physical CD release on October \n",
        "20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]\n",
        "\"\"\"\n",
        "summarized_text = llm.invoke(large_text)\n",
        "print(summarized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例：Dolly 与 LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_community.llms.azureml_endpoint import DollyContentFormatter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "formatter_template = \"Write a {word_count} word essay about {topic}.\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"word_count\", \"topic\"], template=formatter_template\n",
        ")\n",
        "\n",
        "content_formatter = DollyContentFormatter()\n",
        "\n",
        "llm = AzureMLOnlineEndpoint(\n",
        "    endpoint_api_key=os.getenv(\"DOLLY_ENDPOINT_API_KEY\"),\n",
        "    endpoint_url=os.getenv(\"DOLLY_ENDPOINT_URL\"),\n",
        "    model_kwargs={\"temperature\": 0.8, \"max_tokens\": 300},\n",
        "    content_formatter=content_formatter,\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "print(chain.invoke({\"word_count\": 100, \"topic\": \"how to make friends\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 序列化 LLM\n您也可以保存和加载 LLM 配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms.loading import load_llm\n",
        "\n",
        "save_llm = AzureMLOnlineEndpoint(\n",
        "    deployment_name=\"databricks-dolly-v2-12b-4\",\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0.2,\n",
        "        \"max_tokens\": 150,\n",
        "        \"top_p\": 0.8,\n",
        "        \"frequency_penalty\": 0.32,\n",
        "        \"presence_penalty\": 72e-3,\n",
        "    },\n",
        ")\n",
        "save_llm.save(\"azureml.json\")\n",
        "loaded_llm = load_llm(\"azureml.json\")\n",
        "\n",
        "print(loaded_llm)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "langchain"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}