{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RunPod LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "开始使用 RunPod LLMs。\n\n## 概述\n\n本指南介绍如何使用 LangChain `RunPod` LLM 类与托管在 [RunPod Serverless](https://www.runpod.io/serverless-gpu) 上的文本生成模型进行交互。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 设置\n\n1. **安装包：**\n   ```bash\n   pip install -qU langchain-runpod\n   ```\n2. **部署 LLM 端点：** 按照 [RunPod Provider Guide](/docs/integrations/providers/runpod#setup) 中的设置步骤，在 RunPod Serverless 上部署兼容的文本生成端点并获取其 Endpoint ID。\n3. **设置环境变量：** 确保 `RUNPOD_API_KEY` 和 `RUNPOD_ENDPOINT_ID` 已经设置。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Make sure environment variables are set (or pass them directly to RunPod)\n",
        "if \"RUNPOD_API_KEY\" not in os.environ:\n",
        "    os.environ[\"RUNPOD_API_KEY\"] = getpass.getpass(\"Enter your RunPod API Key: \")\n",
        "if \"RUNPOD_ENDPOINT_ID\" not in os.environ:\n",
        "    os.environ[\"RUNPOD_ENDPOINT_ID\"] = input(\"Enter your RunPod Endpoint ID: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 实例化\n\n初始化 `RunPod` 类。您可以通过 `model_kwargs` 传递模型特定的参数并配置轮询行为。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from langchain_runpod import RunPod\n",
        "\n",
        "llm = RunPod(\n",
        "    # runpod_endpoint_id can be passed here if not set in env\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"temperature\": 0.6,\n",
        "        \"top_k\": 50,\n",
        "        # Add other parameters supported by your endpoint handler\n",
        "    },\n",
        "    # Optional: Adjust polling\n",
        "    # poll_interval=0.3,\n",
        "    # max_polling_attempts=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 调用\n\n请使用 LangChain 标准的 `.invoke()` 和 `.ainvoke()` 方法来调用模型。您也可以通过 `.stream()` 和 `.astream()` 来启用流式传输（通过轮询 RunPod `/stream` 端点模拟）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a tagline for an ice cream shop on the moon.\"\n",
        "\n",
        "# Invoke (Sync)\n",
        "try:\n",
        "    response = llm.invoke(prompt)\n",
        "    print(\"--- Sync Invoke Response ---\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    print(\n",
        "        f\"Error invoking LLM: {e}. Ensure endpoint ID/API key are correct and endpoint is active/compatible.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Stream (Sync, simulated via polling /stream)\n",
        "print(\"\\n--- Sync Stream Response ---\")\n",
        "try:\n",
        "    for chunk in llm.stream(prompt):\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "    print()  # Newline\n",
        "except Exception as e:\n",
        "    print(\n",
        "        f\"\\nError streaming LLM: {e}. Ensure endpoint handler supports streaming output format.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 异步用法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# AInvoke (Async)\n",
        "try:\n",
        "    async_response = await llm.ainvoke(prompt)\n",
        "    print(\"--- Async Invoke Response ---\")\n",
        "    print(async_response)\n",
        "except Exception as e:\n",
        "    print(f\"Error invoking LLM asynchronously: {e}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# AStream (Async)\n",
        "print(\"\\n--- Async Stream Response ---\")\n",
        "try:\n",
        "    async for chunk in llm.astream(prompt):\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "    print()  # Newline\n",
        "except Exception as e:\n",
        "    print(\n",
        "        f\"\\nError streaming LLM asynchronously: {e}. Ensure endpoint handler supports streaming output format.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 链式调用\n\nLLM 无缝集成到 [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/) 链中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Assumes 'llm' variable is instantiated from the 'Instantiation' cell\n",
        "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = prompt_template | llm | parser\n",
        "\n",
        "try:\n",
        "    chain_response = chain.invoke({\"topic\": \"bears\"})\n",
        "    print(\"--- Chain Response ---\")\n",
        "    print(chain_response)\n",
        "except Exception as e:\n",
        "    print(f\"Error running chain: {e}\")\n",
        "\n",
        "# Async chain\n",
        "try:\n",
        "    async_chain_response = await chain.ainvoke({\"topic\": \"robots\"})\n",
        "    print(\"--- Async Chain Response ---\")\n",
        "    print(async_chain_response)\n",
        "except Exception as e:\n",
        "    print(f\"Error running async chain: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 端点注意事项\n\n- **输入：** 端点处理程序应在 `{\"input\": {\"prompt\": \"...\", ...}}` 中预料到 prompt 字符串。\n- **输出：** 处理程序应在最终状态响应的 `\"output\"` 键内返回生成的文本（例如 `{\"output\": \"生成的文本...\"}` 或 `{\"output\": {\"text\": \"...\"}}`）。\n- **流式传输：** 对于通过 `/stream` 端点进行的模拟流式传输，处理程序必须在状态响应中用一系列字典填充 `\"stream\"` 键，例如 `[{\"output\": \"token1\"}, {\"output\": \"token2\"}]`。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API 参考\n\n有关 `RunPod` LLM 类、参数和方法的详细文档，请参阅源代码或生成的 API 参考（如果可用）。\n\n指向源代码的链接：[https://github.com/runpod/langchain-runpod/blob/main/langchain_runpod/llms.py](https://github.com/runpod/langchain-runpod/blob/main/langchain_runpod/llms.py)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}