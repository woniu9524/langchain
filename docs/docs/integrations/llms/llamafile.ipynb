{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Llamafile\n",
        "\n",
        "[Llamafile](https://github.com/Mozilla-Ocho/llamafile) 允许你通过单个文件分发和运行大语言模型。\n",
        "\n",
        "Llamafile 通过将 [llama.cpp](https://github.com/ggerganov/llama.cpp) 与 [Cosmopolitan Libc](https://github.com/jart/cosmopolitan) 结合到一个框架中，消除了大语言模型的所有复杂性，将其压缩成一个单一文件的可执行文件（称为“llamafile”），该文件无需安装即可在大多数计算机上本地运行。\n",
        "\n",
        "## 设置\n",
        "\n",
        "1. 下载你想要使用的模型的 llamafile。你可以在 [HuggingFace](https://huggingface.co/models?other=llamafile) 上找到许多 llamafile 格式的模型。在本指南中，我们将下载一个小的模型，`TinyLlama-1.1B-Chat-v1.0.Q5_K_M`。注意：如果你没有 `wget`，你可以通过这个 [链接](https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile?download=true) 下载模型。\n",
        "\n",
        "```bash\n",
        "wget https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n",
        "```\n",
        "\n",
        "2. 使 llamafile 可执行。首先，如果你还没有这样做，请打开一个终端。**如果你使用的是 MacOS、Linux 或 BSD，** 你需要使用 `chmod` 授予你的计算机执行此新文件的权限（见下文）。**如果你在 Windows 上，** 请通过在文件名后添加 \".exe\" 来重命名文件（模型文件名应为 `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile.exe`）。\n",
        "\n",
        "\n",
        "```bash\n",
        "chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile  # 如果你在 MacOS、Linux 或 BSD 上则运行此命令\n",
        "```\n",
        "\n",
        "3. 以“服务器模式”运行 llamafile：\n",
        "\n",
        "```bash\n",
        "./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser\n",
        "```\n",
        "\n",
        "现在你可以调用 llamafile 的 REST API。默认情况下，llamafile 服务器监听在 http://localhost:8080 你可以在 [这里](https://github.com/Mozilla-Ocho/llamafile/blob/main/llama.cpp/server/README.md#api-endpoints) 找到完整的服务器文档。你可以直接通过 REST API 与 llamafile 交互，但在这里我们将展示如何使用 LangChain 与它交互。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 用法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'? \\nI\\'ve got a thing for pink, but you know that.\\n\"Can we not talk about work anymore?\" - What did she say?\\nI don\\'t want to be a burden on you.\\nIt\\'s hard to keep a good thing going.\\nYou can\\'t tell me what I want, I have a life too!'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.llms.llamafile import Llamafile\n",
        "\n",
        "llm = Llamafile()\n",
        "\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "要流式传输 token，请使用 `.stream(...)` 方法："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".\n",
            "- She said, \"I’m tired of my life. What should I do?\"\n",
            "- The man replied, \"I hear you. But don’t worry. Life is just like a joke. It has its funny parts too.\"\n",
            "- The woman looked at him, amazed and happy to hear his wise words. - \"Thank you for your wisdom,\" she said, smiling. - He replied, \"Any time. But it doesn't come easy. You have to laugh and keep moving forward in life.\"\n",
            "- She nodded, thanking him again. - The man smiled wryly. \"Life can be tough. Sometimes it seems like you’re never going to get out of your situation.\"\n",
            "- He said, \"I know that. But the key is not giving up. Life has many ups and downs, but in the end, it will turn out okay.\"\n",
            "- The woman's eyes softened. \"Thank you for your advice. It's so important to keep moving forward in life,\" she said. - He nodded once again. \"You’re welcome. I hope your journey is filled with laughter and joy.\"\n",
            "- They both smiled and left the bar, ready to embark on their respective adventures.\n"
          ]
        }
      ],
      "source": [
        "query = \"Tell me a joke\"\n",
        "\n",
        "for chunks in llm.stream(query):\n",
        "    print(chunks, end=\"\")\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "要了解有关 LangChain Expressive Language 以及 LLM 上可用方法的更多信息，请参阅 [LCEL 接口](/docs/concepts/runnables)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
