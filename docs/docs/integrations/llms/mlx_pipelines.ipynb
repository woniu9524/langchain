{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "959300d4",
      "metadata": {},
      "source": [
        "# MLX 本地管道\n\nMLX 模型可以通过 `MLXPipeline` 类在本地运行。\n\n[MLX 社区](https://huggingface.co/mlx-community)托管了超过 150 个模型，所有模型都是开源的，并且可以在 Hugging Face Model Hub 上公开获取。Hugging Face Model Hub 是一个在线平台，人们可以在那里轻松协作并共同构建机器学习项目。\n\n它们可以通过这个本地管道包装器从 LangChain 中调用，或者通过 `MLXPipeline` 类调用它们托管的推理端点。有关 mlx 的更多信息，请参阅 [examples repo](https://github.com/ml-explore/mlx-examples/tree/main/llms) notebook。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c1b8450-5eaf-4d34-8341-2d785448a1ff",
      "metadata": {
        "tags": []
      },
      "source": [
        "要使用，您应该安装 ``mlx-lm`` Python [程序包](https://pypi.org/project/mlx-lm/)，以及 [transformers](https://pypi.org/project/transformers/)。您也可以安装 `huggingface_hub`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d772b637-de00-4663-bd77-9bc96d798db2",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  mlx-lm transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ad075f-71d5-4bc8-ab91-cc0ad5ef16bb",
      "metadata": {},
      "source": [
        "### 模型加载\n\n模型可以通过使用 `from_model_id` 方法指定模型参数来加载。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "165ae236-962a-4763-8052-c4836d78a5d2",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
        "\n",
        "pipe = MLXPipeline.from_model_id(\n",
        "    \"mlx-community/quantized-gemma-2b-it\",\n",
        "    pipeline_kwargs={\"max_tokens\": 10, \"temp\": 0.1},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00104b27-0c15-4a97-b198-4512337ee211",
      "metadata": {},
      "source": [
        "它们也可以通过直接传递现有的 `transformers` pipeline 来加载"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f426a4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlx_lm import load\n",
        "\n",
        "model, tokenizer = load(\"mlx-community/quantized-gemma-2b-it\")\n",
        "pipe = MLXPipeline(model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60e7ba8d",
      "metadata": {},
      "source": [
        "### 创建链\n\n在模型已加载到内存后，您可以将其与提示组合以形成链。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3acf0069",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | pipe\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question}))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}