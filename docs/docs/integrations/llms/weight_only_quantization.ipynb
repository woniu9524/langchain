{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "959300d4",
      "metadata": {},
      "source": [
        "# Intel 仅权重量化\n## 使用 Intel 扩展进行 Transformer 流水线任务的 Huggingface 模型仅权重量化\n\nHugging Face 模型可以通过 `WeightOnlyQuantPipeline` 类在本地运行，并进行仅权重量化。\n\n[Hugging Face Model Hub](https://huggingface.co/models) 主持了超过 120k 个模型、20k 个数据集和 50k 个演示应用 (Spaces)，所有这些都是开源且公开可用的，在一个在线平台上，人们可以轻松协作和共同构建机器学习。\n\n这些可以通过此本地流水线包装器类从 LangChain 调用。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c1b8450-5eaf-4d34-8341-2d785448a1ff",
      "metadata": {
        "tags": []
      },
      "source": [
        "要使用本功能，您需要安装 ``transformers`` Python [软件包](https://pypi.org/project/transformers/)，以及 [pytorch](https://pytorch.org/get-started/locally/) 和 [intel-extension-for-transformers](https://github.com/intel/intel-extension-for-transformers)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d772b637-de00-4663-bd77-9bc96d798db2",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%pip install transformers --quiet\n",
        "%pip install intel-extension-for-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ad075f-71d5-4bc8-ab91-cc0ad5ef16bb",
      "metadata": {},
      "source": [
        "### 模型加载\n\n可以通过 `from_model_id` 方法指定模型参数来加载模型。模型参数包括 `intel_extension_for_transformers` 中的 `WeightOnlyQuantConfig` 类。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "165ae236-962a-4763-8052-c4836d78a5d2",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig\n",
        "from langchain_community.llms.weight_only_quantization import WeightOnlyQuantPipeline\n",
        "\n",
        "conf = WeightOnlyQuantConfig(weight_dtype=\"nf4\")\n",
        "hf = WeightOnlyQuantPipeline.from_model_id(\n",
        "    model_id=\"google/flan-t5-large\",\n",
        "    task=\"text2text-generation\",\n",
        "    quantization_config=conf,\n",
        "    pipeline_kwargs={\"max_new_tokens\": 10},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00104b27-0c15-4a97-b198-4512337ee211",
      "metadata": {},
      "source": [
        "它们也可以通过直接传入一个现有的 `transformers` pipeline 来加载"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f426a4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from intel_extension_for_transformers.transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "model_id = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
        ")\n",
        "hf = WeightOnlyQuantPipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4418c20-8fbb-475e-b389-9b27428b8fe1",
      "metadata": {},
      "source": [
        "### 创建链\n\n模型加载到内存后，您可以将其与提示组合以形成一个链。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c8e151-2999-4d52-9c9c-db99df4f4321",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | hf\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbbc3a37",
      "metadata": {},
      "source": [
        "### CPU 推理\n\n目前 intel-extension-for-transformers 仅支持 CPU 设备推理。稍后将支持 Intel GPU。当在带有 CPU 的机器上运行时，您可以指定 `device=\"cpu\"` 或 `device=-1` 参数将模型置于 CPU 设备上。\nCPU 推理的默认值为 `-1`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "703c91c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "conf = WeightOnlyQuantConfig(weight_dtype=\"nf4\")\n",
        "llm = WeightOnlyQuantPipeline.from_model_id(\n",
        "    model_id=\"google/flan-t5-large\",\n",
        "    task=\"text2text-generation\",\n",
        "    quantization_config=conf,\n",
        "    pipeline_kwargs={\"max_new_tokens\": 10},\n",
        ")\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59276016",
      "metadata": {},
      "source": [
        "### Batch CPU 推理\n\n你也可以在 CPU 上以批处理模式运行推理。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097ba62f",
      "metadata": {},
      "outputs": [],
      "source": [
        "conf = WeightOnlyQuantConfig(weight_dtype=\"nf4\")\n",
        "llm = WeightOnlyQuantPipeline.from_model_id(\n",
        "    model_id=\"google/flan-t5-large\",\n",
        "    task=\"text2text-generation\",\n",
        "    quantization_config=conf,\n",
        "    pipeline_kwargs={\"max_new_tokens\": 10},\n",
        ")\n",
        "\n",
        "chain = prompt | llm.bind(stop=[\"\\n\\n\"])\n",
        "\n",
        "questions = []\n",
        "for i in range(4):\n",
        "    questions.append({\"question\": f\"What is the number {i} in french?\"})\n",
        "\n",
        "answers = chain.batch(questions)\n",
        "for answer in answers:\n",
        "    print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc4c225-53d7-4003-a0a6-eefb1a7ededc",
      "metadata": {},
      "source": [
        "### Intel-extension-for-transformers 支持的数据类型\n\n我们支持将权重量化为以下数据类型进行存储（`weight_dtype` in `WeightOnlyQuantConfig`）：\n\n*   **int8**：使用 8 位数据类型。\n*   **int4_fullrange**：使用 int4 范围的 -8 值，与正常的 int4 范围 [-7,7] 相比。\n*   **int4_clip**：将值裁剪并保留在 int4 范围内，其余设置为零。\n*   **nf4**：使用归一化浮点 4 位数据类型。\n*   **fp4_e2m1**：使用常规浮点 4 位数据类型。“e2”表示使用 2 位表示指数，“m1”表示使用 1 位表示尾数。\n\n虽然这些技术将权重存储为 4 位或 8 位，但计算仍然以 float32、bfloat16 或 int8 进行（`compute_dtype` in `WeightOnlyQuantConfig`）：\n*   **fp32**：使用 float32 数据类型进行计算。\n*   **bf16**：使用 bfloat16 数据类型进行计算。\n*   **int8**：使用 8 位数据类型进行计算。\n\n### 支持的算法矩阵\n\nIntel-extension-for-transformers 中支持的量化算法（`algorithm` in `WeightOnlyQuantConfig`）：\n\n| 算法   | PyTorch | LLM Runtime |\n| :----- | :------ | :---------- |\n| RTN    | &#10004; | &#10004;    |\n| AWQ    | &#10004; | 敬请期待    |\n| TEQ    | &#10004; | 敬请期待    |\n> **RTN:** 一种我们可以直观理解的量化方法。它不需要额外的训练数据集，是一种非常快速的量化方法。一般来说，RTN 会将权重转换为均匀分布的整数数据类型，但有些算法（例如 Qlora）提出了非均匀 NF4 数据类型，并证明了其理论上的最优性。\n\n> **AWQ:** 证明了仅保护 1% 的显著权重就可以大大减少量化误差。显著权重通道是通过观察每个通道的激活和权重的分布来选择的。显著权重在量化前乘以一个较大的缩放因子以进行保留，然后进行量化。\n\n> **TEQ:** 一种可训练的等价变换，可在仅权重量化中保持 FP32 精度。它受到 AWQ 的启发，同时提供了一种新的解决方案来搜索激活和权重之间最优的每通道缩放因子。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}