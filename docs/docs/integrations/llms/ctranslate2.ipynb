{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CTranslate2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CTranslate2** 是一个用于高效推断 Transformer 模型的 C++ 和 Python 库。\n\n该项目实现了一个自定义运行时，应用了许多性能优化技术，如权重量化、层融合、批次重排序等，以加速和减少 CPU 和 GPU 上 Transformer 模型的内存使用。\n\n完整的功能列表和支持的模型包含在[项目仓库](https://opennmt.net/CTranslate2/guides/transformers.html)中。要开始，请查看官方的[快速上手指南](https://opennmt.net/CTranslate2/quickstart.html)。\n\n要使用，您应该安装 `ctranslate2` Python 包。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  ctranslate2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "要将 Hugging Face 模型与 CTranslate2 结合使用，必须首先使用 `ct2-transformers-converter` 命令将其转换为 CTranslate2 格式。该命令需要预训练模型名称和转换后的模型目录路径。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.81it/s]\n"
          ]
        }
      ],
      "source": [
        "# conversation can take several minutes\n",
        "!ct2-transformers-converter --model meta-llama/Llama-2-7b-hf --quantization bfloat16 --output_dir ./llama-2-7b-ct2 --force"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms import CTranslate2\n",
        "\n",
        "llm = CTranslate2(\n",
        "    # output_dir from above:\n",
        "    model_path=\"./llama-2-7b-ct2\",\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-hf\",\n",
        "    device=\"cuda\",\n",
        "    # device_index can be either single int or list or ints,\n",
        "    # indicating the ids of GPUs to use for inference:\n",
        "    device_index=[0, 1],\n",
        "    compute_type=\"bfloat16\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 单次调用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "He presented me with plausible evidence for the existence of unicorns: 1) they are mentioned in ancient texts; and, more importantly to him (and not so much as a matter that would convince most people), he had seen one.\n",
            "I was skeptical but I didn't want my friend upset by his belief being dismissed outright without any consideration or argument on its behalf whatsoever - which is why we were having this conversation at all! So instead asked if there might be some other explanation besides \"unicorning\"... maybe it could have been an ostrich? Or perhaps just another horse-like animal like zebras do exist afterall even though no humans alive today has ever witnesses them firsthand either due lacking accessibility/availability etc.. But then again those animals aren’ t exactly known around here anyway…” And thus began our discussion about whether these creatures actually existed anywhere else outside Earth itself where only few scientists ventured before us nowadays because technology allows exploration beyond borders once thought impossible centuries ago when travel meant walking everywhere yourself until reaching destination point A->B via footsteps alone unless someone helped guide along way through woods full darkness nighttime hours\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    llm.invoke(\n",
        "        \"He presented me with plausible evidence for the existence of unicorns: \",\n",
        "        max_length=256,\n",
        "        sampling_topk=50,\n",
        "        sampling_temperature=0.2,\n",
        "        repetition_penalty=2,\n",
        "        cache_static_prompt=False,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 多次调用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generations=[[Generation(text='The list of top romantic songs:\\n1. “I Will Always Love You” by Whitney Houston\\n2. “Can’t Help Falling in Love” by Elvis Presley\\n3. “Unchained Melody” by The Righteous Brothers\\n4. “I Will Always Love You” by Dolly Parton\\n5. “I Will Always Love You” by Whitney Houston\\n6. “I Will Always Love You” by Dolly Parton\\n7. “I Will Always Love You” by The Beatles\\n8. “I Will Always Love You” by The Rol', generation_info=None)], [Generation(text='The list of top rap songs:\\n1. “God’s Plan” by Drake\\n2. “Rockstar” by Post Malone\\n3. “Bad and Boujee” by Migos\\n4. “Humble” by Kendrick Lamar\\n5. “Bodak Yellow” by Cardi B\\n6. “I’m the One” by DJ Khaled\\n7. “Motorsport” by Migos\\n8. “No Limit” by G-Eazy\\n9. “Bounce Back” by Big Sean\\n10. “', generation_info=None)]] llm_output=None run=[RunInfo(run_id=UUID('628e0491-a310-4d12-81db-6f2c5309d5c2')), RunInfo(run_id=UUID('f88fdbcd-c1f6-4f13-b575-810b80ecbaaf'))]\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    llm.generate(\n",
        "        [\"The list of top romantic songs:\\n1.\", \"The list of top rap songs:\\n1.\"],\n",
        "        max_length=128,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 将模型集成到 LLMChain 中"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Who was the US president in the year the first Pokemon game was released?\n",
            "\n",
            "Let's think step by step. 1996 was the year the first Pokemon game was released.\n",
            "\n",
            "\\begin{blockquote}\n",
            "\n",
            "\\begin{itemize}\n",
            "  \\item 1996 was the year Bill Clinton was president.\n",
            "  \\item 1996 was the year the first Pokemon game was released.\n",
            "  \\item 1996 was the year the first Pokemon game was released.\n",
            "\n",
            "\\end{itemize}\n",
            "\\end{blockquote}\n",
            "\n",
            "I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n",
            "Comment: I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n",
            "Comment: @JoeZ. I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n",
            "Comment: @JoeZ. I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n",
            "Comment: @JoeZ. I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n",
            "Comment: @JoeZ. I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n",
            "Comment: @JoeZ. I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n",
            "Comment: @JoeZ. I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n",
            "Comment: @JoeZ. I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n",
            "Comment: @JoeZ. I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n",
            "Comment: @JoeZ. I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n",
            "Comment: @JoeZ. I'm not sure if this is a valid question, but I'm sure it's a fun one.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"{question}\n",
        "\n",
        "Let's think step by step. \"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "question = \"Who was the US president in the year the first Pokemon game was released?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.12 ('langchain_venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "d1d3a3c58a58885896c5459933a599607cdbb9917d7e1ad7516c8786c51f2dd2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}