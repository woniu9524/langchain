{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NVIDIA\n\n这将帮助您开始使用 NVIDIA [模型](/docs/concepts/text_llms)。有关所有 `NVIDIA` 功能和配置的详细文档，请访问 [API 参考](https://python.langchain.com/api_reference/nvidia_ai_endpoints/llms/langchain_nvidia_ai_endpoints.chat_models.NVIDIA.html)。\n\n## 概述\n`langchain-nvidia-ai-endpoints` 包包含了 LangChain 的集成，用于构建使用 NVIDIA NIM 推理微服务上的模型驱动的应用程序。这些模型由 NVIDIA 优化，可在 NVIDIA 加速基础设施上提供最佳性能，并部署为 NIM。NIM 是易于使用的预构建容器，可以使用 NVIDIA 加速基础设施上的单一命令在任何地方进行部署。\n\nNVIDIA 托管的 NIM 部署可在 [NVIDIA API 目录](https://build.nvidia.com/)上进行测试。测试后，可以使用 NVIDIA AI Enterprise 许可从 NVIDIA 的 API 目录导出 NIM，并在本地或云端运行，从而使企业能够拥有并完全控制其 IP 和 AI 应用程序。\n\nNIM 按模型打包为容器镜像，并通过 NVIDIA NGC Catalog 以 NGC 容器镜像的形式分发。NIM 的核心是提供简单、一致且熟悉的 API 来运行 AI 模型推理。\n\n此示例将介绍如何使用 LangChain 通过 `NVIDIA` 类与 NVIDIA 进行交互。\n\n有关通过此 API 访问 LLM 模型的信息，请查看 [NVIDIA](https://python.langchain.com/docs/integrations/llms/nvidia_ai_endpoints/) 文档。\n\n### 集成详情\n\n| 类名 | 包名 | 本地 | 可序列化 | JS 支持 | 包下载量 | 包最新版本 |\n| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n| [NVIDIA](https://python.langchain.com/api_reference/nvidia_ai_endpoints/llms/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html) | [langchain_nvidia_ai_endpoints](https://python.langchain.com/api_reference/nvidia_ai_endpoints/index.html) | ✅ | beta | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain_nvidia_ai_endpoints?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain_nvidia_ai_endpoints?style=flat-square&label=%20) |\n\n### 模型特性\n| JSON 模式 | [图像输入](/docs/how_to/multimodal_inputs/) | 音频输入 | 视频输入 | [Token 级流式传输](/docs/how_to/chat_streaming/) | 原生异步 | [Token 使用量追踪](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n| :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n| ❌ | ✅ | ❌ | ❌ | ✅ | ❌ | ❌ | ❌ |\n\n## 设置\n\n**开始之前：**\n\n1. 在 [NVIDIA](https://build.nvidia.com/) 创建一个免费账户，该平台托管 NVIDIA AI Foundation 模型。\n\n2. 点击您选择模型。\n\n3. 在 `Input` 下选择 `Python` 标签页，然后点击 `Get API Key`。接着点击 `Generate Key`。\n\n4. 复制生成的密钥并将其保存为 `NVIDIA_API_KEY`。之后，您应该就可以访问端点了。\n\n### 凭证"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"NVIDIA_API_KEY\"):\n",
        "    # Note: the API key should start with \"nvapi-\"\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 安装\n\n LangChain NVIDIA AI Endpoints 集成位于 `langchain_nvidia_ai_endpoints` 包中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet langchain-nvidia-ai-endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 实例化\n\n有关完整功能，请参阅 [LLM](/docs/how_to#llms)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = NVIDIA().bind(max_tokens=256)\n",
        "llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 调用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"# Function that does quicksort written in Rust without comments:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(llm.invoke(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 流式、批量和异步\n\n这些模型原生支持流式传输，并且与所有 LangChain LLM 一样，它们公开了 batch 方法来处理并发请求，以及用于 invoke、stream 和 batch 的异步方法。以下是一些示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for chunk in llm.stream(prompt):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm.batch([prompt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await llm.ainvoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async for chunk in llm.astream(prompt):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await llm.abatch([prompt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async for chunk in llm.astream_log(prompt):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = llm.invoke(\n",
        "    \"X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.1) #Train a logistic regression model, predict the labels on the test set and compute the accuracy score\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 支持的模型\n\n查询 `available_models` 仍会列出您的 API 凭证提供的所有其他模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NVIDIA.get_available_models()\n",
        "# llm.get_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 链式调用\n\n我们可以像这样将模型与提示模板进行[链式调用](/docs/how_to/sequence/)："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"English\",\n",
        "        \"output_language\": \"German\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API 参考\n\n有关所有 `NVIDIA` 功能和配置的详细文档，请访问 API 参考：https://python.langchain.com/api_reference/nvidia_ai_endpoints/llms/langchain_nvidia_ai_endpoints.llms.NVIDIA.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-nvidia-ai-endpoints-m0-Y4aGr-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}