{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenLM\n[OpenLM](https://github.com/r2d4/openlm) 是一个零依赖、兼容 OpenAI 的 LLM 提供商，可以通过 HTTP 直接调用不同的推理端点。\n\n它实现了 OpenAI Completion 类，因此可以用作 OpenAI API 的即插即用替代品。此变更集利用了 BaseOpenAI 以添加最少的代码。\n\n本示例将介绍如何使用 LangChain 与 OpenAI 和 HuggingFace 进行交互。您需要两者的 API 密钥。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 设置\n安装依赖项并设置 API 密钥。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install openlm and openai if you haven't already\n",
        "\n",
        "%pip install --upgrade --quiet  openlm\n",
        "%pip install --upgrade --quiet  langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Check if OPENAI_API_KEY environment variable is set\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    print(\"Enter your OpenAI API key:\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
        "\n",
        "# Check if HF_API_TOKEN environment variable is set\n",
        "if \"HF_API_TOKEN\" not in os.environ:\n",
        "    print(\"Enter your HuggingFace Hub API key:\")\n",
        "    os.environ[\"HF_API_TOKEN\"] = getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 将 LangChain 与 OpenLM 结合使用\n\n在这里，我们将在 LLMChain 中调用两个模型：来自 OpenAI 的 `text-davinci-003` 和 HuggingFace 上的 `gpt2`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_community.llms import OpenLM\n",
        "from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: text-davinci-003\n",
            "Result:  France is a country in Europe. The capital of France is Paris.\n",
            "Model: huggingface.co/gpt2\n",
            "Result: Question: What is the capital of France?\n",
            "\n",
            "Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far more\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the capital of France?\"\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "for model in [\"text-davinci-003\", \"huggingface.co/gpt2\"]:\n",
        "    llm = OpenLM(model=model)\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "    result = llm_chain.run(question)\n",
        "    print(\n",
        "        \"\"\"Model: {}\n",
        "Result: {}\"\"\".format(model, result)\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}