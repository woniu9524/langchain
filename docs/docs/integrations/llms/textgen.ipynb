{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TextGen\n\n[GitHub:oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) 一个用于运行 LLaMA、llama.cpp、GPT-J、Pythia、OPT 和 GALACTICA 等大型语言模型的 gradio web UI。\n\n本示例将介绍如何通过 `text-generation-webui` API 集成使用 LangChain 与 LLM 模型进行交互。\n\n请确保您已配置好 `text-generation-webui` 并安装了 LLM。推荐通过适用于您操作系统的 [一键安装程序](https://github.com/oobabooga/text-generation-webui#one-click-installers) 进行安装。\n\n安装并确认 `text-generation-webui` 可在 Web 界面正常工作后，请通过 Web 模型配置选项卡启用 `api` 选项，或在启动命令中添加运行时参数 `--api`。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 设置 model_url 并运行示例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_url = \"http://localhost:5000\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.globals import set_debug\n",
        "from langchain_community.llms import TextGen\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "set_debug(True)\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "llm = TextGen(model_url=model_url)\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
        "\n",
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 流式版本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "您应该安装 `websocket-client` 来使用此功能。\n`pip install websocket-client`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_url = \"ws://localhost:5005\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.globals import set_debug\n",
        "from langchain_community.llms import TextGen\n",
        "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "set_debug(True)\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "llm = TextGen(\n",
        "    model_url=model_url, streaming=True, callbacks=[StreamingStdOutCallbackHandler()]\n",
        ")\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
        "\n",
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = TextGen(model_url=model_url, streaming=True)\n",
        "for chunk in llm.stream(\"Ask 'Hi, how are you?' like a pirate:'\", stop=[\"'\", \"\\n\"]):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}