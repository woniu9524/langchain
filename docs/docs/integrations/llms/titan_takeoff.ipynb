{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Titan Takeoff\n\n`TitanML` 通过我们的训练、压缩和推理优化平台，帮助企业构建和部署更好、更小、更便宜、更快的 NLP 模型。\n\n我们的推理服务器 [Titan Takeoff](https://docs.titanml.co/docs/intro) 能够通过单个命令在您的本地硬件上部署 LLM。支持大多数生成模型架构，例如 Falcon、Llama 2、GPT2、T5 等。如果您在使用特定模型时遇到问题，请通过 hello@titanml.co 告知我们。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 示例用法\n以下是一些有助于您开始使用 Titan Takeoff Server 的示例。在运行这些命令之前，您需要确保 Takeoff Server 已在后台启动。有关更多信息，请参阅[启动 Takeoff 的文档页面](https://docs.titanml.co/docs/Docs/launching/)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Note importing TitanTakeoffPro instead of TitanTakeoff will work as well both use same object under the hood\n",
        "from langchain_community.llms import TitanTakeoff\n",
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例 1\n\n基本用法，假设 Takeoff 在您的机器上运行，并使用其默认端口（即 localhost:3000）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = TitanTakeoff()\n",
        "output = llm.invoke(\"What is the weather in London in August?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例 2\n\n指定端口和其他生成参数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = TitanTakeoff(port=3000)\n",
        "# A comprehensive list of parameters can be found at https://docs.titanml.co/docs/next/apis/Takeoff%20inference_REST_API/generate#request\n",
        "output = llm.invoke(\n",
        "    \"What is the largest rainforest in the world?\",\n",
        "    consumer_group=\"primary\",\n",
        "    min_new_tokens=128,\n",
        "    max_new_tokens=512,\n",
        "    no_repeat_ngram_size=2,\n",
        "    sampling_topk=1,\n",
        "    sampling_topp=1.0,\n",
        "    sampling_temperature=1.0,\n",
        "    repetition_penalty=1.0,\n",
        "    regex_string=\"\",\n",
        "    json_schema=None,\n",
        ")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例 3\n\n使用 `generate` 处理多个输入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = TitanTakeoff()\n",
        "rich_output = llm.generate([\"What is Deep Learning?\", \"What is Machine Learning?\"])\n",
        "print(rich_output.generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例 4\n\n流式输出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = TitanTakeoff(\n",
        "    streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
        ")\n",
        "prompt = \"What is the capital of France?\"\n",
        "output = llm.invoke(prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例 5\n\n使用 LCEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = TitanTakeoff()\n",
        "prompt = PromptTemplate.from_template(\"Tell me about {topic}\")\n",
        "chain = prompt | llm\n",
        "output = chain.invoke({\"topic\": \"the universe\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 示例 6\n\n使用 TitanTakeoff Python 包装器启动读取器。如果你在首次启动 Takeoff 时没有创建任何读取器，或者想添加另一个读取器，可以在初始化 TitanTakeoff 对象时进行操作。只需将你想启动的模型配置列表作为 `models` 参数传递即可。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model config for the llama model, where you can specify the following parameters:\n",
        "#   model_name (str): The name of the model to use\n",
        "#   device: (str): The device to use for inference, cuda or cpu\n",
        "#   consumer_group (str): The consumer group to place the reader into\n",
        "#   tensor_parallel (Optional[int]): The number of gpus you would like your model to be split across\n",
        "#   max_seq_length (int): The maximum sequence length to use for inference, defaults to 512\n",
        "#   max_batch_size (int_: The max batch size for continuous batching of requests\n",
        "llama_model = {\n",
        "    \"model_name\": \"TheBloke/Llama-2-7b-Chat-AWQ\",\n",
        "    \"device\": \"cuda\",\n",
        "    \"consumer_group\": \"llama\",\n",
        "}\n",
        "llm = TitanTakeoff(models=[llama_model])\n",
        "\n",
        "# The model needs time to spin up, length of time need will depend on the size of model and your network connection speed\n",
        "time.sleep(60)\n",
        "\n",
        "prompt = \"What is the capital of France?\"\n",
        "output = llm.invoke(prompt, consumer_group=\"llama\")\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}