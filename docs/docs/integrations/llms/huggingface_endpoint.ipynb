{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Huggingface 端点\n\n> [Hugging Face Hub](https://huggingface.co/docs/hub/index) 是一个拥有超过 12 万个模型、2 万个数据集和 5 万个演示应用（Spaces）的平台，所有这些都是开源的，并且可以在线公开访问，人们可以在其中轻松协作并共同构建 ML 应用。\n\n`Hugging Face Hub` 还提供各种用于构建机器学习应用的端点。\n本示例展示了如何连接到不同类型的端点。\n\n特别是，文本生成推理由 [Text Generation Inference](https://github.com/huggingface/text-generation-inference) 提供支持：这是一个专为极速文本生成推理构建的定制化 Rust、Python 和 gRPC 服务器。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 安装与设置"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "要使用，您应该安装 ``huggingface_hub`` 的 python [包](https://huggingface.co/docs/huggingface_hub/installation)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 准备示例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 示例\n\n以下是如何访问 [推理提供商](https://huggingface.co/docs/inference-providers) API 的无服务器 `HuggingFaceEndpoint` 集成的示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "repo_id = \"deepseek-ai/DeepSeek-R1-0528\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    max_length=128,\n",
        "    temperature=0.5,\n",
        "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
        "    provider=\"auto\",  # set your provider here hf.co/settings/inference-providers\n",
        "    # provider=\"hyperbolic\",\n",
        "    # provider=\"nebius\",\n",
        "    # provider=\"together\",\n",
        ")\n",
        "llm_chain = prompt | llm\n",
        "print(llm_chain.invoke({\"question\": question}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 专用端点\n\n免费的无服务器 API 让您可以快速实现解决方案并进行迭代，但对于重度使用场景可能会受到速率限制，因为其负载与其他请求共享。\n\n对于企业级工作负载，最佳选择是使用 [Inference Endpoints - Dedicated](https://huggingface.co/inference-endpoints/dedicated)。\n这可以访问完全托管的基础设施，提供更大的灵活性和速度。这些资源附带持续支持和正常运行时间保证，以及自动扩展等选项。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the url to your Inference Endpoint below\n",
        "your_endpoint_url = \"https://fayjubiy2xqn36z0.us-east-1.aws.endpoints.huggingface.cloud\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=f\"{your_endpoint_url}\",\n",
        "    max_new_tokens=512,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "llm(\"What did foo say about bar?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 流式传输"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=f\"{your_endpoint_url}\",\n",
        "    max_new_tokens=512,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        "    streaming=True,\n",
        ")\n",
        "llm(\"What did foo say about bar?\", callbacks=[StreamingStdOutCallbackHandler()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这个 `HuggingFaceEndpoint` 类也可以与托管 LLM 的本地 [HuggingFace TGI 实例](https://github.com/huggingface/text-generation-inference/blob/main/docs/source/index.md) 一起使用。查阅 TGI [仓库](https://github.com/huggingface/text-generation-inference/tree/main) 以了解各种硬件（GPU、TPU、Gaudi 等）支持的详细信息。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agents",
      "language": "python",
      "name": "agents"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}