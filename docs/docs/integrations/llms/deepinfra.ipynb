{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepInfra\n\n[DeepInfra](https://deepinfra.com/?utm_source=langchain) 是一个无服务器的推理即服务（inference as a service），提供对[多种大语言模型](https://deepinfra.com/models?utm_source=langchain)和[嵌入模型](https://deepinfra.com/models?type=embeddings&utm_source=langchain)的访问。本笔记将介绍如何将 LangChain 与 DeepInfra 结合使用来处理语言模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 设置环境变量 API 密钥\n请务必从 DeepInfra 获取您的 API 密钥。您需要先[登录](https://deepinfra.com/login?from=%2Fdash)并获取新的令牌。\n\n您将获得 1 小时的免费无服务器 GPU 计算资源来测试不同的模型。（参见[这里](https://github.com/deepinfra/deepctl#deepctl)）\n您可以使用 `deepctl auth token` 命令打印您的令牌。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ········\n"
          ]
        }
      ],
      "source": [
        "# get a new token: https://deepinfra.com/login?from=%2Fdash\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "DEEPINFRA_API_TOKEN = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DEEPINFRA_API_TOKEN\"] = DEEPINFRA_API_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 创建 DeepInfra 实例\n您也可以使用我们的开源 [deepctl 工具](https://github.com/deepinfra/deepctl#deepctl) 来管理您的模型部署。您可以在[此处](https://deepinfra.com/databricks/dolly-v2-12b#API)查看可用参数列表。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms import DeepInfra\n",
        "\n",
        "llm = DeepInfra(model_id=\"meta-llama/Llama-2-70b-chat-hf\")\n",
        "llm.model_kwargs = {\n",
        "    \"temperature\": 0.7,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"max_new_tokens\": 250,\n",
        "    \"top_p\": 0.9,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'This is a question that has puzzled many people'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run inferences directly via wrapper\n",
        "llm(\"Who let the dogs out?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              " Will\n",
              " Smith\n",
              "."
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run streaming inference\n",
        "for chunk in llm.stream(\"Who let the dogs out?\"):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 创建 Prompt 模板\n我们将为问答创建一个 Prompt 模板。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 初始化 LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 运行 LLMChain\n提供一个问题并运行 LLMChain。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Penguins are found in Antarctica and the surrounding islands, which are located at the southernmost tip of the planet. The North Pole is located at the northernmost tip of the planet, and it would be a long journey for penguins to get there. In fact, penguins don't have the ability to fly or migrate over such long distances. So, no, penguins cannot reach the North Pole. \""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"Can penguins reach the North pole?\"\n",
        "\n",
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "a0a0263b650d907a3bfe41c0f8d6a63a071b884df3cfdc1579f00cdc1aed6b03"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}