{
  "cells": [
    {
      "cell_type": "raw",
      "id": "00a924a0-57e2-43fa-95dc-3ea48a56d3a5",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_label: Arxiv\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1b8ddb-8b06-4e7e-b0bb-8786dea15e2b",
      "metadata": {},
      "source": [
        "# ArxivRetriever\n\n>[arXiv](https://arxiv.org/) 是一个开放获取的档案库，收录了物理学、数学、计算机科学、定量生物学、定量金融学、统计学、电气工程与系统科学以及经济学领域的 200 万篇学术文章。\n\n本笔记本展示了如何从 Arxiv.org 中检索科学文章并将其转换为下游使用的 [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) 格式。\n\n有关 `ArxivRetriever` 所有功能和配置的详细文档，请访问 [API 参考](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.arxiv.ArxivRetriever.html)。\n\n### 集成详情\n\nimport {ItemTable} from \"@theme/FeatureTables\";\n\n<ItemTable category=\"external_retrievers\" item=\"ArxivRetriever\" />\n\n## 设置\n\n如果您想从单个查询中获得自动跟踪，还可以通过取消注释下面的行来设置您的 [LangSmith](https://docs.smith.langchain.com/) API 密钥："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d179b4-abc3-48db-9f8b-1cdb46d3aa77",
      "metadata": {},
      "outputs": [],
      "source": [
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51489529-5dcd-4b86-bda6-de0a39d8ffd1",
      "metadata": {},
      "source": [
        "### 安装\n\n此检索器位于 `langchain-community` 包中。我们还需要 [arxiv](https://pypi.org/project/arxiv/) 依赖项："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a737220",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-community arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c15470b-a16b-4e0d-bc6a-6998bafbb5a4",
      "metadata": {},
      "source": [
        "## 实例化\n\n`ArxivRetriever` 参数包括：\n- 可选 `load_max_docs`: 默认为 100。用于限制下载文档的数量。下载全部 100 篇文档需要时间，因此请在实验中使用较小的数字。目前有一个硬性限制是 300。\n- 可选 `load_all_available_meta`: 默认为 False。默认情况下，只下载最重要的字段：`Published` (文档发布/最后更新日期)、`Title`、`Authors`、`Summary`。如果设置为 True，则也会下载其他字段。\n- `get_full_documents`: 布尔值，默认为 False。决定是否获取文档的全文。\n\n更多详情请参阅 [API 参考](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.arxiv.ArxivRetriever.html)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a13f9e92-24b3-4cea-8541-2584c1cdb2d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import ArxivRetriever\n",
        "\n",
        "retriever = ArxivRetriever(\n",
        "    load_max_docs=2,\n",
        "    get_ful_documents=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30c27047-16cf-46b5-bb29-754f1696f2bb",
      "metadata": {},
      "source": [
        "## 用法\n\n`ArxivRetriever` 支持通过文章标识符进行检索："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "20ae1a74",
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = retriever.invoke(\"1605.08386\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1d5a5088",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Entry ID': 'http://arxiv.org/abs/1605.08386v1',\n",
              " 'Published': datetime.date(2016, 5, 26),\n",
              " 'Title': 'Heat-bath random walks with Markov bases',\n",
              " 'Authors': 'Caprice Stanley, Tobias Windisch'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0].metadata  # meta-information of the Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c0ccd0c7-f6a6-43e7-b842-5f57afb94224",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a ge'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0].page_content[:400]  # a content of the Document"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c525c5c2-0961-4f4c-a208-dd6ceed76ea1",
      "metadata": {},
      "source": [
        "`ArxivRetriever` 还支持基于自然语言文本的检索："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4cd3d079-4496-4ab8-adff-b86e6418bc74",
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = retriever.invoke(\"What is the ImageBind model?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9318c790-d388-45da-8d5c-57256619e2a1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Entry ID': 'http://arxiv.org/abs/2305.05665v2',\n",
              " 'Published': datetime.date(2023, 5, 31),\n",
              " 'Title': 'ImageBind: One Embedding Space To Bind Them All',\n",
              " 'Authors': 'Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2670363b-3806-4c7e-b14d-90a4d5d2a200",
      "metadata": {},
      "source": [
        "## 在链中使用\n\n与其他检索器一样，`ArxivRetriever` 可以通过[链](/docs/how_to/sequence/)集成到 LLM 应用中。\n\n我们将需要一个 LLM 或聊天模型：\n\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\n\n<ChatModelTabs customVarName=\"llm\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bcbeeaf5-79d1-4e29-8589-11dfb26761af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# | output: false\n",
        "# | echo: false\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bb3601df-53ea-4826-bdbe-554387bc3ad4",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Answer the question based only on the context provided.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "62889c3c-8a49-4c76-9141-d777311af1f4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The ImageBind model is an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. It shows that only image-paired data is sufficient to bind the modalities together and can leverage large scale vision-language models for zero-shot capabilities and emergent applications such as cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation.'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\"What is the ImageBind model?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e419acb8-d7ac-42a1-916f-c796f23dce9b",
      "metadata": {},
      "source": [
        "## API 参考\n\n如需了解 `ArxivRetriever` 的所有功能和配置的详细文档，请前往 [API 参考](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.arxiv.ArxivRetriever.html)。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}