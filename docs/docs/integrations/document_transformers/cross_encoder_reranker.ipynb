{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fc0db1bc",
      "metadata": {},
      "source": [
        "# Cross Encoder 重排器\n\n本 Notebook 展示了如何在检索器中实现重排器，使用您自己的交叉编码器，这些交叉编码器来自 [Hugging Face 交叉编码器模型](https://huggingface.co/cross-encoder) 或实现了交叉编码器功能的 Hugging Face 模型 ([示例：BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base))。`SagemakerEndpointCrossEncoder` 使您能够使用加载到 Sagemaker 上的这些 HuggingFace 模型。\n\n这建立在 [ContextualCompressionRetriever](/docs/how_to/contextual_compression) 的思想之上。本文档的整体结构来自于 [Cohere Reranker 文档](/docs/integrations/retrievers/cohere-reranker)。\n\n有关为什么交叉编码器可以与嵌入结合使用作为重排机制以实现更好检索的更多信息，请参阅 [Hugging Face 交叉编码器文档](https://www.sbert.net/examples/applications/cross-encoder/README.html)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b37bd138-4f3c-4d2c-bc4b-be705ce27a09",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip install faiss sentence_transformers\n",
        "\n",
        "# OR  (depending on Python version)\n",
        "\n",
        "#!pip install faiss-cpu sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "28e8dc12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function for printing docs\n",
        "\n",
        "\n",
        "def pretty_print_docs(docs):\n",
        "    print(\n",
        "        f\"\\n{'-' * 100}\\n\".join(\n",
        "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa3d916",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 设置基础向量存储检索器\n让我们从初始化一个简单的向量存储检索器开始，并将 2023 年国情咨文（分块）存储其中。我们可以将检索器设置为检索较多的文档（20 个）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fbcc58f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "documents = TextLoader(\"../../how_to/state_of_the_union.txt\").load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "embeddingsModel = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/msmarco-distilbert-dot-v5\"\n",
        ")\n",
        "retriever = FAISS.from_documents(texts, embeddingsModel).as_retriever(\n",
        "    search_kwargs={\"k\": 20}\n",
        ")\n",
        "\n",
        "query = \"What is the plan for the economy?\"\n",
        "docs = retriever.invoke(query)\n",
        "pretty_print_docs(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7648612",
      "metadata": {},
      "source": [
        "## 使用 CrossEncoderReranker 进行重排\n现在，我们将基础检索器包装在 `ContextualCompressionRetriever` 中。`CrossEncoderReranker` 使用 `HuggingFaceCrossEncoder` 来重排返回的结果。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "9a658023",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1:\n",
            "\n",
            "More infrastructure and innovation in America. \n",
            "\n",
            "More goods moving faster and cheaper in America. \n",
            "\n",
            "More jobs where you can earn a good living in America. \n",
            "\n",
            "And instead of relying on foreign supply chains, let’s make it in America. \n",
            "\n",
            "Economists call it “increasing the productive capacity of our economy.” \n",
            "\n",
            "I call it building a better America. \n",
            "\n",
            "My plan to fight inflation will lower your costs and lower the deficit.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 2:\n",
            "\n",
            "Second – cut energy costs for families an average of $500 a year by combatting climate change.  \n",
            "\n",
            "Let’s provide investments and tax credits to weatherize your homes and businesses to be energy efficient and you get a tax credit; double America’s clean energy production in solar, wind, and so much more;  lower the price of electric vehicles, saving you another $80 a month because you’ll never have to pay at the gas pump again.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 3:\n",
            "\n",
            "Look at cars. \n",
            "\n",
            "Last year, there weren’t enough semiconductors to make all the cars that people wanted to buy. \n",
            "\n",
            "And guess what, prices of automobiles went up. \n",
            "\n",
            "So—we have a choice. \n",
            "\n",
            "One way to fight inflation is to drive down wages and make Americans poorer.  \n",
            "\n",
            "I have a better plan to fight inflation. \n",
            "\n",
            "Lower your costs, not your wages. \n",
            "\n",
            "Make more cars and semiconductors in America. \n",
            "\n",
            "More infrastructure and innovation in America. \n",
            "\n",
            "More goods moving faster and cheaper in America.\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "\n",
        "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
        "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.invoke(\"What is the plan for the economy?\")\n",
        "pretty_print_docs(compressed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "419a2bf3-de4b-4c4d-9a40-4336552f604c",
      "metadata": {},
      "source": [
        "## 将 Hugging Face 模型上传到 SageMaker endpoint\n\n以下是一个示例 `inference.py` 文件，用于创建与 `SagemakerEndpointCrossEncoder` 配合使用的 endpoint。有关更详细的分步指南，请参阅[本文](https://huggingface.co/blog/kchoe/deploy-any-huggingface-model-to-sagemaker)。\n\n它会动态下载 Hugging Face 模型，因此您无需将 `pytorch_model.bin` 等模型构件保存在 `model.tar.gz` 中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e579c743-40c3-432f-9483-0982e2808f9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from sagemaker_inference import encoder\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "PAIRS = \"pairs\"\n",
        "SCORES = \"scores\"\n",
        "\n",
        "\n",
        "class CrossEncoder:\n",
        "    def __init__(self) -> None:\n",
        "        self.device = (\n",
        "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        )\n",
        "        logging.info(f\"Using device: {self.device}\")\n",
        "        model_name = \"BAAI/bge-reranker-base\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def __call__(self, pairs: List[List[str]]) -> List[float]:\n",
        "        with torch.inference_mode():\n",
        "            inputs = self.tokenizer(\n",
        "                pairs,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=512,\n",
        "            )\n",
        "            inputs = inputs.to(self.device)\n",
        "            scores = (\n",
        "                self.model(**inputs, return_dict=True)\n",
        "                .logits.view(\n",
        "                    -1,\n",
        "                )\n",
        "                .float()\n",
        "            )\n",
        "\n",
        "        return scores.detach().cpu().tolist()\n",
        "\n",
        "\n",
        "def model_fn(model_dir: str) -> CrossEncoder:\n",
        "    try:\n",
        "        return CrossEncoder()\n",
        "    except Exception:\n",
        "        logging.exception(f\"Failed to load model from: {model_dir}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def transform_fn(\n",
        "    cross_encoder: CrossEncoder, input_data: bytes, content_type: str, accept: str\n",
        ") -> bytes:\n",
        "    payload = json.loads(input_data)\n",
        "    model_output = cross_encoder(**payload)\n",
        "    output = {SCORES: model_output}\n",
        "    return encoder.encode(output, accept)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}