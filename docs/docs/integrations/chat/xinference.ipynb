{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_label: Xinference\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01ca90da",
      "metadata": {},
      "source": [
        "# ChatXinference\n\n[Xinference](https://github.com/xorbitsai/inference) 是一个强大且通用的库，用于部署 LLM、语音识别模型和多模态模型，甚至可以在你的笔记本电脑上运行。它支持多种与 GGML [兼容的](/docs/integrations/document_loaders/ggml/) 模型，例如 chatglm、baichuan、whisper、vicuna、orca 以及许多其他模型。\n\n## 概览\n### 集成详情\n\n| 类 | 包 | 本地 | 可序列化 | [JS 支持] | 包下载 | 最新包 |\n| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n| ChatXinference| langchain-xinference | ✅ | ❌ | ✅ | ✅ | ✅ |\n\n### 模型特性\n| [Tool calling](/docs/how_to/tool_calling/) | [Structured output](/docs/how_to/structured_output/) | JSON 模式 | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | Native async | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n| :---: |:----------------------------------------------------:| :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n| ✅ |                          ✅                           | ✅ | ❌ | ❌ | ❌ | ✅ | ✅ | ❌ | ❌ |\n\n## 设置\n\n通过 PyPI 安装 `Xinference`："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  \"xinference[all]\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fb27b941602401d91542211134fc71a",
      "metadata": {},
      "source": [
        "### 在本地或分布式集群中部署 Xinference。\n\n如需本地部署，请运行 `xinference`。\n\n要在集群中部署 Xinference，首先使用 `xinference-supervisor` 启动一个 Xinference supervisor。您也可以使用 `-p` 参数指定端口，使用 `-H` 参数指定主机。默认端口为 8080，默认主机为 0.0.0.0。\n\n然后，在您想运行 Xinference worker 的每个服务器上使用 `xinference-worker` 启动 Xinference worker。\n\n您可以查阅 [Xinference](https://github.com/xorbitsai/inference) 的 README 文件以获取更多信息。\n### Wrapper\n\n要将 Xinference 与 LangChain 结合使用，您需要先启动一个模型。您可以使用命令行界面 (CLI) 来实现此目的："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "acae54e37e7d407bbb7b55eff062a284",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064\n"
          ]
        }
      ],
      "source": [
        "%xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a63283cbaf04dbcab1f6479b197f3a8",
      "metadata": {},
      "source": [
        "一个模型 UID 将会被返回供您使用。现在您可以使用 Xinference 和 LangChain 了："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dd0d8092fe74a7c96281538738b07e2",
      "metadata": {},
      "source": [
        "## 安装\n\nLangChain Xinference 集成位于 `langchain-xinference` 包中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72eea5119410473aa328ad9291626812",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU langchain-xinference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8edb47106e1a46a883d545849b8ab81b",
      "metadata": {},
      "source": [
        "为了获得结构化输出，请确保您使用的是最新版本的 Xinference。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55935996",
      "metadata": {},
      "source": [
        "## 实例化\n\n现在我们可以实例化我们的模型对象并生成聊天补全了："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "10185d26023b46108eb7d9f57d49d2b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_xinference.chat_models import ChatXinference\n",
        "\n",
        "llm = ChatXinference(\n",
        "    server_url=\"your_server_url\", model_uid=\"7167b2b0-2a04-11ee-83f0-d29396a3f064\"\n",
        ")\n",
        "\n",
        "llm.invoke(\n",
        "    \"Q: where can we visit in the capital of France?\",\n",
        "    config={\"max_tokens\": 1024},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8763a12b2bbd4a93a75aff182afb95dc",
      "metadata": {},
      "source": [
        "## 调用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "50f6c0e4",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_xinference.chat_models import ChatXinference\n",
        "\n",
        "llm = ChatXinference(\n",
        "    server_url=\"your_server_url\", model_uid=\"7167b2b0-2a04-11ee-83f0-d29396a3f064\"\n",
        ")\n",
        "\n",
        "system_message = \"You are a helpful assistant that translates English to French. Translate the user sentence.\"\n",
        "human_message = \"I love programming.\"\n",
        "\n",
        "llm.invoke([HumanMessage(content=human_message), SystemMessage(content=system_message)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7623eae2785240b9bd12b16a66d81610",
      "metadata": {},
      "source": [
        "## 链式调用\n\n我们可以像这样将我们的模型与提示模板进行[链式调用](/docs/how_to/sequence/)："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "317d05c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_xinference.chat_models import ChatXinference\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input=[\"country\"], template=\"Q: where can we visit in the capital of {country}? A:\"\n",
        ")\n",
        "\n",
        "llm = ChatXinference(\n",
        "    server_url=\"your_server_url\", model_uid=\"7167b2b0-2a04-11ee-83f0-d29396a3f064\"\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "chain.invoke(input={\"country\": \"France\"})\n",
        "chain.stream(input={\"country\": \"France\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f7fb26",
      "metadata": {},
      "source": [
        "## API 参考\n\n有关 ChatXinference 所有功能和配置的详细文档，请参阅 API 参考：https://github.com/TheSongg/langchain-xinference"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}