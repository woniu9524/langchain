{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Llama.cpp\n\n>[llama.cpp python](https://github.com/abetlen/llama-cpp-python) 库是 `@ggerganov`\n>[llama.cpp](https://github.com/ggerganov/llama.cpp) 的简单 Python 绑定。\n>\n>该包提供：\n>\n> - 通过 ctypes 接口对 C API 的低级访问。\n> - 用于文本补全的高级 Python API\n>   - 类似 `OpenAI` 的 API\n>   - `LangChain` 兼容性\n>   - `LlamaIndex` 兼容性\n> - 类似 OpenAI 的 Web 服务器\n>   - 本地 Copilot 替代品\n>   - 函数调用支持\n>   - 视觉 API 支持\n>   - 多模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 概述\n\n### 集成详情\n| 类 | 包 | 本地 | 可序列化 | JS 支持 |\n| :--- | :--- | :---: | :---: |  :---: |\n| [ChatLlamaCpp](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.llamacpp.ChatLlamaCpp.html) | [langchain-community](https://python.langchain.com/api_reference/community/index.html) | ✅ | ❌ | ❌ |\n\n### 模型特性\n| [工具调用](/docs/how_to/tool_calling) | [结构化输出](/docs/how_to/structured_output/) | JSON 模式 | 图像输入 | 音频输入 | 视频输入 | [Token级流式输出](/docs/how_to/chat_streaming/) | 原生异步 | [Token 用量跟踪](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n| ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ❌ | ❌ | ✅ | \n\n## 设置\n\n为了开始并使用下面展示的**所有**功能，我们推荐使用经过工具调用微调的模型。\n\n我们将使用 NousResearch 的 [Hermes-2-Pro-Llama-3-8B-GGUF](https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF)。\n\n> Hermes 2 Pro 是 Nous Hermes 2 的升级版本，它包含一个更新和清理过的 OpenHermes 2.5 数据集，以及一个内部开发的全新函数调用和 JSON 模式数据集。新版本的 Hermes 在保持其出色的通用任务和对话能力的同时，在函数调用方面也表现出色。\n\n请参阅我们关于本地模型的指南以了解更多详情：\n\n* [在本地运行 LLM](https://python.langchain.com/v0.1/docs/guides/development/local_llms/)\n* [将本地模型与 RAG 结合使用](https://python.langchain.com/v0.1/docs/use_cases/question_answering/local_retrieval_qa/)\n\n### 安装\n\nLangChain LlamaCpp 集成位于 `langchain-community` 和 `llama-cpp-python` 包中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU langchain-community llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 实例化\n\n现在我们可以实例化我们的模型对象并生成聊天补全："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to your model weights\n",
        "local_model = \"local/path/to/Hermes-2-Pro-Llama-3-8B-Q8_0.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "from langchain_community.chat_models import ChatLlamaCpp\n",
        "\n",
        "llm = ChatLlamaCpp(\n",
        "    temperature=0.5,\n",
        "    model_path=local_model,\n",
        "    n_ctx=10000,\n",
        "    n_gpu_layers=8,\n",
        "    n_batch=300,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    max_tokens=512,\n",
        "    n_threads=multiprocessing.cpu_count() - 1,\n",
        "    repeat_penalty=1.5,\n",
        "    top_p=0.5,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 调用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J'aime programmer. (In France, \"programming\" is often used in its original sense of scheduling or organizing events.) \n",
            "\n",
            "If you meant computer-programming: \n",
            "Je suis amoureux de la programmation informatique.\n",
            "\n",
            "(You might also say simply 'programmation', which would be understood as both meanings - depending on context).\n"
          ]
        }
      ],
      "source": [
        "print(ai_msg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 链式调用\n\n我们可以像这样将我们的模型与提示模板进行[链式调用](/docs/how_to/sequence/)："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"English\",\n",
        "        \"output_language\": \"German\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 工具调用\n\n首先，它的工作方式与 OpenAI 的函数调用（Function Calling）大体相同。\n\nOpenAI 拥有一个 [工具调用](https://platform.openai.com/docs/guides/function-calling)（我们在此互换使用“工具调用”和“函数调用”）API，它允许你描述工具及其参数，并让模型返回一个 JSON 对象，其中包含要调用的工具和该工具的输入。工具调用对于构建使用工具的链和代理，以及更广泛地从模型获取结构化输出极其有用。\n\n通过 `ChatLlamaCpp.bind_tools`，我们可以轻松地将 Pydantic 类、字典模式、LangChain 工具，甚至函数作为工具传递给模型。在底层，它们会被转换为 OpenAI 工具模式，如下所示：\n```\n{\n    \"name\": \"...\",\n    \"description\": \"...\",\n    \"parameters\": {...}  # JSONSchema\n}\n```\n并在每次模型调用中传递。\n\n但是，它不能自动触发函数/工具，我们需要通过指定 'tool choice' 参数来强制它。此参数通常格式如下。\n\n```{\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class WeatherInput(BaseModel):\n",
        "    location: str = Field(description=\"The city and state, e.g. San Francisco, CA\")\n",
        "    unit: str = Field(enum=[\"celsius\", \"fahrenheit\"])\n",
        "\n",
        "\n",
        "@tool(\"get_current_weather\", args_schema=WeatherInput)\n",
        "def get_weather(location: str, unit: str):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    return f\"Now the weather in {location} is 22 {unit}\"\n",
        "\n",
        "\n",
        "llm_with_tools = llm.bind_tools(\n",
        "    tools=[get_weather],\n",
        "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ai_msg = llm_with_tools.invoke(\n",
        "    \"what is the weather like in HCMC in celsius\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'get_current_weather',\n",
              "  'args': {'location': 'Ho Chi Minh City', 'unit': 'celsius'},\n",
              "  'id': 'call__0_get_current_weather_cmpl-394d9943-0a1f-425b-8139-d2826c1431f2'}]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_msg.tool_calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MagicFunctionInput(BaseModel):\n",
        "    magic_function_input: int = Field(description=\"The input value for magic function\")\n",
        "\n",
        "\n",
        "@tool(\"get_magic_function\", args_schema=MagicFunctionInput)\n",
        "def magic_function(magic_function_input: int):\n",
        "    \"\"\"Get the value of magic function for an input.\"\"\"\n",
        "    return magic_function_input + 2\n",
        "\n",
        "\n",
        "llm_with_tools = llm.bind_tools(\n",
        "    tools=[magic_function],\n",
        "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_magic_function\"}},\n",
        ")\n",
        "\n",
        "ai_msg = llm_with_tools.invoke(\n",
        "    \"What is magic function of 3?\",\n",
        ")\n",
        "\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'get_magic_function',\n",
              "  'args': {'magic_function_input': 3},\n",
              "  'id': 'call__0_get_magic_function_cmpl-cd83a994-b820-4428-957c-48076c68335a'}]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_msg.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 结构化输出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Joke(BaseModel):\n",
        "    \"\"\"A setup to a joke and the punchline.\"\"\"\n",
        "\n",
        "    setup: str\n",
        "    punchline: str\n",
        "\n",
        "\n",
        "dict_schema = convert_to_openai_tool(Joke)\n",
        "structured_llm = llm.with_structured_output(dict_schema)\n",
        "result = structured_llm.invoke(\"Tell me a joke about birds\")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'setup': '- Why did the chicken cross the playground?',\n",
              " 'punchline': '\\n\\n- To get to its gilded cage on the other side!'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 流式处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for chunk in llm.stream(\"what is 25x5\"):\n",
        "    print(chunk.content, end=\"\\n\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API 参考\n\n如需了解 ChatLlamaCpp 所有功能和配置的详细文档，请访问 API 参考：https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.llamacpp.ChatLlamaCpp.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}