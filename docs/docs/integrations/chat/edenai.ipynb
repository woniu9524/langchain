{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Eden AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Eden AI 通过整合最优秀的 AI 提供商，正在革新 AI 领域，使用户能够解锁无限可能，并挖掘人工智能的真正潜力。凭借一个全方位、无忧的平台，它允许用户以闪电般的速度将 AI 功能部署到生产环境中，并通过单一 API 轻松访问全部 AI 功能。 (网站：https://edenai.co/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "本示例将介绍如何使用 LangChain 与 Eden AI 模型进行交互。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`EdenAI` 不仅仅是模型调用，它还为您提供了高级功能，包括：\n\n- **多供应商**: 访问由不同供应商提供的各种语言模型，使您可以自由选择最适合您用例的模型。\n\n- **备用机制**: 设置备用机制以确保主要供应商不可用时也能无缝运行，您可以轻松切换到备用供应商。\n\n- **使用情况跟踪**: 按项目和按 API 密钥跟踪使用情况统计信息。此功能使您可以有效地监控和管理资源消耗。\n\n- **监控与可观测性**: `EdenAI` 在平台上提供了全面的监控和可观测性工具。监控语言模型的性能，分析使用模式，并获得宝贵的见解来优化您的应用程序。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "访问 EDENAI 的 API 需要一个 API 密钥，\n\n您可以通过创建账户 https://app.edenai.run/user/register 并前往 https://app.edenai.run/admin/iam/api-keys 获取。\n\n获得密钥后，我们需要通过运行以下命令将其设置为环境变量：\n\n```bash\nexport EDENAI_API_KEY=\"...\"\n```\n\n您可以在 API 参考中找到更多详细信息：https://docs.edenai.co/reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "如果你不想设置环境变量，可以在初始化 EdenAI Chat Model 类时，通过 `edenai_api_key` 命名参数直接传递 API 密钥。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.chat_models.edenai import ChatEdenAI\n",
        "from langchain_core.messages import HumanMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat = ChatEdenAI(\n",
        "    edenai_api_key=\"...\", provider=\"openai\", temperature=0.2, max_tokens=250\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [HumanMessage(content=\"Hello !\")]\n",
        "chat.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await chat.ainvoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 流式传输与批量处理\n\n`ChatEdenAI` 支持流式传输和批量处理。下面是一个示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I assist you today?"
          ]
        }
      ],
      "source": [
        "for chunk in chat.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content='Hello! How can I assist you today?')]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat.batch([messages])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fallback 机制\n\n借助 Eden AI，您可以设置回退机制，确保即使主要提供商不可用也能实现无缝运行，您可以轻松切换到备用提供商。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat = ChatEdenAI(\n",
        "    edenai_api_key=\"...\",\n",
        "    provider=\"openai\",\n",
        "    temperature=0.2,\n",
        "    max_tokens=250,\n",
        "    fallback_providers=\"google\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "在此示例中，如果 OpenAI 遇到任何问题，您可以使用 Google 作为备用提供商。\n\n有关 Eden AI 的更多信息和详细信息，请访问此链接：: https://docs.edenai.co/docs/additional-parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 链式调用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is a good name for a company that makes {product}?\"\n",
        ")\n",
        "chain = prompt | chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='VitalBites')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"product\": \"healthy snacks\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 工具\n\n### bind_tools()\n\n使用 `ChatEdenAI.bind_tools`，我们可以轻松地将 Pydantic 类、dict schemas、LangChain 工具，甚至函数作为工具传递给模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "llm = ChatEdenAI(provider=\"openai\", temperature=0.2, max_tokens=500)\n",
        "\n",
        "\n",
        "class GetWeather(BaseModel):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "\n",
        "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
        "\n",
        "\n",
        "llm_with_tools = llm.bind_tools([GetWeather])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', response_metadata={'openai': {'status': 'success', 'generated_text': None, 'message': [{'role': 'user', 'message': 'what is the weather like in San Francisco', 'tools': [{'name': 'GetWeather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'description': 'The city and state, e.g. San Francisco, CA', 'type': 'string'}}, 'required': ['location']}}], 'tool_calls': None}, {'role': 'assistant', 'message': None, 'tools': None, 'tool_calls': [{'id': 'call_tRpAO7KbQwgTjlka70mCQJdo', 'name': 'GetWeather', 'arguments': '{\"location\":\"San Francisco\"}'}]}], 'cost': 0.000194}}, id='run-5c44c01a-d7bb-4df6-835e-bda596080399-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco'}, 'id': 'call_tRpAO7KbQwgTjlka70mCQJdo'}])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_msg = llm_with_tools.invoke(\n",
        "    \"what is the weather like in San Francisco\",\n",
        ")\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'GetWeather',\n",
              "  'args': {'location': 'San Francisco'},\n",
              "  'id': 'call_tRpAO7KbQwgTjlka70mCQJdo'}]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_msg.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### with_structured_output()\n\nBaseChatModel.with_structured_output 接口可以方便地从聊天模型获取结构化输出。你可以使用 ChatEdenAI.with_structured_output（其底层使用工具调用），让模型更可靠地以特定格式返回输出："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GetWeather(location='San Francisco')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "structured_llm = llm.with_structured_output(GetWeather)\n",
        "structured_llm.invoke(\n",
        "    \"what is the weather like in San Francisco\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 将工具结果传递给模型\n\n这是一个使用工具的完整示例。将工具输出传递给模型，并从模型获取结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'11 + 11 = 22'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "llm = ChatEdenAI(\n",
        "    provider=\"openai\",\n",
        "    max_tokens=1000,\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "llm_with_tools = llm.bind_tools([add], tool_choice=\"required\")\n",
        "\n",
        "query = \"What is 11 + 11?\"\n",
        "\n",
        "messages = [HumanMessage(query)]\n",
        "ai_msg = llm_with_tools.invoke(messages)\n",
        "messages.append(ai_msg)\n",
        "\n",
        "tool_call = ai_msg.tool_calls[0]\n",
        "tool_output = add.invoke(tool_call[\"args\"])\n",
        "\n",
        "# This append the result from our tool to the model\n",
        "messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
        "\n",
        "llm_with_tools.invoke(messages).content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 流式传输\n\nEden AI 目前不支持流式工具调用。尝试流式传输将只返回一条最终消息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/eden/Projects/edenai-langchain/libs/community/langchain_community/chat_models/edenai.py:603: UserWarning: stream: Tool use is not yet supported in streaming mode.\n",
            "  warnings.warn(\"stream: Tool use is not yet supported in streaming mode.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[AIMessageChunk(content='', id='run-fae32908-ec48-4ab2-ad96-bb0d0511754f', tool_calls=[{'name': 'add', 'args': {'a': 9, 'b': 9}, 'id': 'call_n0Tm7I9zERWa6UpxCAVCweLN'}], tool_call_chunks=[{'name': 'add', 'args': '{\"a\": 9, \"b\": 9}', 'id': 'call_n0Tm7I9zERWa6UpxCAVCweLN', 'index': 0}])]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(llm_with_tools.stream(\"What's 9 + 9\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}