{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Snowflake Cortex\n\n[Snowflake Cortex](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions) 可让您即时访问業界領先的大型语言模型 (LLM)，这些模型由 Mistral、Reka、Meta 和 Google 等公司的研究人员训练，包括 [Snowflake Arctic](https://www.snowflake.com/en/data-cloud/arctic/)，这是 Snowflake 开发的一种开放的企业级模型。\n\n本示例将介绍如何使用 LangChain 与 Snowflake Cortex 进行交互。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 安装和设置\n\n我们首先通过以下命令安装 `snowflake-snowpark-python` 库。然后，我们将连接 Snowflake 的凭据配置为环境变量，或者直接传递它们。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet snowflake-snowpark-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# First step is to set up the environment variables, to connect to Snowflake,\n",
        "# you can also pass these snowflake credentials while instantiating the model\n",
        "\n",
        "if os.environ.get(\"SNOWFLAKE_ACCOUNT\") is None:\n",
        "    os.environ[\"SNOWFLAKE_ACCOUNT\"] = getpass.getpass(\"Account: \")\n",
        "\n",
        "if os.environ.get(\"SNOWFLAKE_USERNAME\") is None:\n",
        "    os.environ[\"SNOWFLAKE_USERNAME\"] = getpass.getpass(\"Username: \")\n",
        "\n",
        "if os.environ.get(\"SNOWFLAKE_PASSWORD\") is None:\n",
        "    os.environ[\"SNOWFLAKE_PASSWORD\"] = getpass.getpass(\"Password: \")\n",
        "\n",
        "if os.environ.get(\"SNOWFLAKE_DATABASE\") is None:\n",
        "    os.environ[\"SNOWFLAKE_DATABASE\"] = getpass.getpass(\"Database: \")\n",
        "\n",
        "if os.environ.get(\"SNOWFLAKE_SCHEMA\") is None:\n",
        "    os.environ[\"SNOWFLAKE_SCHEMA\"] = getpass.getpass(\"Schema: \")\n",
        "\n",
        "if os.environ.get(\"SNOWFLAKE_WAREHOUSE\") is None:\n",
        "    os.environ[\"SNOWFLAKE_WAREHOUSE\"] = getpass.getpass(\"Warehouse: \")\n",
        "\n",
        "if os.environ.get(\"SNOWFLAKE_ROLE\") is None:\n",
        "    os.environ[\"SNOWFLAKE_ROLE\"] = getpass.getpass(\"Role: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.chat_models import ChatSnowflakeCortex\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# By default, we'll be using the cortex provided model: `mistral-large`, with function: `complete`\n",
        "chat = ChatSnowflakeCortex()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "上面的单元格假定您的 Snowflake 凭证已设置为环境变量。如果您希望手动指定它们，请使用以下代码：\n\n```python\nchat = ChatSnowflakeCortex(\n    # Change the default cortex model and function\n    model=\"mistral-large\",\n    cortex_function=\"complete\",\n\n    # Change the default generation parameters\n    temperature=0,\n    max_tokens=10,\n    top_p=0.95,\n\n    # Specify your Snowflake Credentials\n    account=\"YOUR_SNOWFLAKE_ACCOUNT\",\n    username=\"YOUR_SNOWFLAKE_USERNAME\",\n    password=\"YOUR_SNOWFLAKE_PASSWORD\",\n    database=\"YOUR_SNOWFLAKE_DATABASE\",\n    schema=\"YOUR_SNOWFLAKE_SCHEMA\",\n    role=\"YOUR_SNOWFLAKE_ROLE\",\n    warehouse=\"YOUR_SNOWFLAKE_WAREHOUSE\"\n)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 调用聊天模型\n我们现在可以使用 `invoke` 或 `stream` 方法来调用聊天模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "messages = [\n    SystemMessage(content=\"你是一位友好的助手。\"),\n    HumanMessage(content=\"什么是大型语言模型？\"),\n]\nchat.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample input prompt\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a friendly assistant.\"),\n",
        "    HumanMessage(content=\"What are large language models?\"),\n",
        "]\n",
        "\n",
        "# Invoke the stream method and print each chunk as it arrives\n",
        "print(\"Stream Method Response:\")\n",
        "for chunk in chat._stream(messages):\n",
        "    print(chunk.message.content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}