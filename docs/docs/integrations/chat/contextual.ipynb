{
  "cells": [
    {
      "cell_type": "raw",
      "id": "afaf8039",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "sidebar_label: ContextualAI\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49f1e0d",
      "metadata": {},
      "source": [
        "# ChatContextual\n\n这将帮助您开始使用 Contextual AI 的 Grounded Language Model [聊天模型](/docs/concepts/chat_models/)。\n\n要了解更多关于 Contextual AI 的信息，请访问我们的[文档](https://docs.contextual.ai/)。\n\n此集成需要 `contextual-client` Python SDK。在此[处](https://github.com/ContextualAI/contextual-client-python)了解更多信息。\n\n## 概述\n\n此集成调用 Contextual AI 的 Grounded Language Model。\n\n### 集成详情\n\n| 类 | 包 | 本地 | 可序列化 | JS 支持 | 包下载 | 包最新版本 |\n| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n| [ChatContextual](https://github.com/ContextualAI//langchain-contextual) | [langchain-contextual](https://pypi.org/project/langchain-contextual/) | ❌ | beta | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-contextual?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-contextual?style=flat-square&label=%20) |\n\n### 模型功能\n| [工具调用](/docs/how_to/tool_calling) | [结构化输出](/docs/how_to/structured_output/) | JSON 模式 | [图像输入](/docs/how_to/multimodal_inputs/) | 音频输入 | 视频输入 | [Token 级别流式传输](/docs/how_to/chat_streaming/) | 原生异步 | [Token 用量跟踪](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n| ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ |\n\n## 设置\n\n要访问 Contextual 模型，您需要创建一个 Contextual AI 账户，获取 API 密钥，并安装 `langchain-contextual` 集成包。\n\n### 凭证\n\n前往 [app.contextual.ai](https://app.contextual.ai) 注册 Contextual 并生成 API 密钥。完成此操作后，请设置 CONTEXTUAL_AI_API_KEY 环境变量："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "433e8d2b-9519-4b49-b2c4-7ab65b046c94",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"CONTEXTUAL_AI_API_KEY\"):\n",
        "    os.environ[\"CONTEXTUAL_AI_API_KEY\"] = getpass.getpass(\n",
        "        \"Enter your Contextual API key: \"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72ee0c4b-9764-423a-9dbf-95129e185210",
      "metadata": {},
      "source": [
        "如果你想自动跟踪你的模型调用，你也可以通过取消注释以下部分来设置你的 [LangSmith](https://docs.smith.langchain.com/) API 密钥："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15d341e-3e26-4ca3-830b-5aab30ed66de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0730d6a1-c893-4840-9817-5e5251676d5d",
      "metadata": {},
      "source": [
        "### 安装\n\nLangChain Contextual 集成位于 `langchain-contextual` 包中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "652d6238-1f87-422a-b135-f5abbb8652fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU langchain-contextual"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a38cde65-254d-4219-a441-068766c0d4b5",
      "metadata": {},
      "source": [
        "## 实例化\n\n现在我们可以实例化我们的模型对象并生成聊天完成。\n\n可以使用以下附加设置来实例化聊天客户端：\n\n| 参数 | 类型 | 描述 | 默认值 |\n|---|---|---|---|\n| temperature | Optional[float] | 采样温度，影响响应的随机性。请注意，较高的温度值可能会降低基准性。 | 0 |\n| top_p | Optional[float] | 核心采样参数，是温度的替代选项，同样影响响应的随机性。请注意，较高的 top_p 值可能会降低基准性。 | 0.9 |\n| max_new_tokens | Optional[int] | 模型在响应中可以生成的最大 token 数。最小为 1，最大为 2048。 | 1024 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_contextual import ChatContextual\n",
        "\n",
        "llm = ChatContextual(\n",
        "    model=\"v1\",  # defaults to `v1`\n",
        "    api_key=\"\",\n",
        "    temperature=0,  # defaults to 0\n",
        "    top_p=0.9,  # defaults to 0.9\n",
        "    max_new_tokens=1024,  # defaults to 1024\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4f3e15",
      "metadata": {},
      "source": [
        "## 调用\n\n`ChatContextual.invoke` 方法在调用时接受额外的 `kwargs`。\n\n这些额外的输入包括：\n\n| 参数 | 类型 | 描述 |\n|-----------|------|-------------|\n| knowledge | list[str] | 必需：一个字符串列表，包含模型在生成响应时可使用的知识来源。 |\n| system_prompt | Optional[str] | 可选：模型在生成响应时应遵循的指令。请注意，我们不保证模型会严格遵守这些指令。 |\n| avoid_commentary | Optional[bool] | 可选（默认为 `False`）：一个标志，指示模型在响应中是否应避免提供额外评论。评论具有对话性质，不包含可验证的声明；因此，评论并非严格基于可用上下文。然而，评论可能提供有用的上下文，从而提高响应的帮助性。 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e0dbc3",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# include a system prompt (optional)\n",
        "system_prompt = \"You are a helpful assistant that uses all of the provided knowledge to answer the user's query to the best of your ability.\"\n",
        "\n",
        "# provide your own knowledge from your knowledge-base here in an array of string\n",
        "knowledge = [\n",
        "    \"There are 2 types of dogs in the world: good dogs and best dogs.\",\n",
        "    \"There are 2 types of cats in the world: good cats and best cats.\",\n",
        "]\n",
        "\n",
        "# create your message\n",
        "messages = [\n",
        "    (\"human\", \"What type of cats are there in the world and what are the types?\"),\n",
        "]\n",
        "\n",
        "# invoke the GLM by providing the knowledge strings, optional system prompt\n",
        "# if you want to turn off the GLM's commentary, pass True to the `avoid_commentary` argument\n",
        "ai_msg = llm.invoke(\n",
        "    messages, knowledge=knowledge, system_prompt=system_prompt, avoid_commentary=True\n",
        ")\n",
        "\n",
        "print(ai_msg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c35a9e0",
      "metadata": {},
      "source": [
        "## 链式调用\n\n我们可以将语境化模型与输出解析器进行链式调用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "545e1e16",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = llm | StrOutputParser\n",
        "\n",
        "chain.invoke(\n",
        "    messages, knowledge=knowledge, systemp_prompt=system_prompt, avoid_commentary=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a5bb5ca-c3ae-4a58-be67-2cd18574b9a3",
      "metadata": {},
      "source": [
        "## API 参考\n\n如需了解 ChatContextual 的所有功能和配置的详细文档，请访问 Github 页面：https://github.com/ContextualAI//langchain-contextual"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}