{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f666798-8635-4bc0-a515-04d318588d67",
      "metadata": {},
      "source": [
        "---\nsidebar_label: NVIDIA AI 端点\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8eb20e-4db8-45e3-9e79-c595f4f274da",
      "metadata": {},
      "source": [
        "# ChatNVIDIA\n\n这将帮助您开始使用 NVIDIA [聊天模型](/docs/concepts/chat_models)。如需 `ChatNVIDIA` 所有功能和配置的详细文档，请前往 [API 参考文档](https://python.langchain.com/api_reference/nvidia_ai_endpoints/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html)。\n\n## 概述\n`langchain-nvidia-ai-endpoints` 包包含 LangChain 集成，用于构建基于 NVIDIA NIM 推理微服务上模型的应用程序。NIM 支持来自社区以及 NVIDIA 的聊天、嵌入和重排模型等领域内的模型。这些模型由 NVIDIA 优化，可在 NVIDIA 加速基础设施上提供最佳性能，并作为 NIM 部署。NIM 是易于使用的预构建容器，只需在 NVIDIA 加速基础设施上运行单个命令即可在任何地方进行部署。\n\nNVIDIA 托管的 NIM 部署可在 [NVIDIA API 目录](https://build.nvidia.com/) 上进行测试。测试后，可以使用 NVIDIA AI Enterprise 许可证从 NVIDIA 的 API 目录导出 NIM，并在本地或云端运行，从而使企业能够拥有并完全控制其 IP 和 AI 应用程序。\n\nNIM 按模型打包为容器镜像，并通过 NVIDIA NGC Catalog 分发为 NGC 容器镜像。从本质上讲，NIM 为运行 AI 模型推理提供了简单、一致且熟悉的 API。\n\n此示例将介绍如何通过 `ChatNVIDIA` 类使用 LangChain 与 NVIDIA 进行交互。\n\n有关通过此 API 访问聊天模型的更多信息，请查看 [ChatNVIDIA](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) 文档。\n\n### 集成详情\n\n| 类 | 包 | 本地 | 可序列化 | JS 支持 | 包下载量 | 最新包版本 |\n| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n| [ChatNVIDIA](https://python.langchain.com/api_reference/nvidia_ai_endpoints/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html) | [langchain_nvidia_ai_endpoints](https://python.langchain.com/api_reference/nvidia_ai_endpoints/index.html) | ✅ | beta | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain_nvidia_ai_endpoints?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain_nvidia_ai_endpoints?style=flat-square&label=%20) |\n\n### 模型功能\n| [工具调用](/docs/how_to/tool_calling) | [结构化输出](/docs/how_to/structured_output/) | JSON 模式 | [图像输入](/docs/how_to/multimodal_inputs/) | 音频输入 | 视频输入 | [Token 级别流式传输](/docs/how_to/chat_streaming/) | 原生异步 | [Token 使用量跟踪](/docs/how_to/chat_token_usage_tracking/) | Logprobs |\n| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | ❌ |\n\n## 设置\n\n**开始之前：**\n\n1. 创建一个免费的 [NVIDIA](https://build.nvidia.com/) 账户，该平台托管 NVIDIA AI Foundation 模型。\n\n2. 选择您想使用的模型。\n\n3. 在 `输入` 部分选择 `Python` 选项卡，然后点击 `获取 API 密钥`。接着点击 `生成密钥`。\n\n4. 复制生成的密钥并另存为 `NVIDIA_API_KEY`。从这时起，您应该可以访问相应的端点。\n\n### 凭证"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208b72da-1535-4249-bbd3-2500028e25e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"NVIDIA_API_KEY\"):\n",
        "    # Note: the API key should start with \"nvapi-\"\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52dc8dcb-0a48-4a4e-9947-764116d2ffd4",
      "metadata": {},
      "source": "要启用模型调用的自动化跟踪，请设置您的 [LangSmith](https://docs.smith.langchain.com/) API 密钥："
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd9cb12-6ca5-432a-9e42-8a57da073c7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2be90a9",
      "metadata": {},
      "source": [
        "### 安装\n\nLangChain NVIDIA AI Endpoints 集成位于 `langchain_nvidia_ai_endpoints` 包中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e13eb331",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet langchain-nvidia-ai-endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af0ce26b",
      "metadata": {},
      "source": [
        "## 实例化\n\n现在我们可以访问 NVIDIA API 目录中的模型了："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jdl2NUfMhi4J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdl2NUfMhi4J",
        "outputId": "e9c4cc72-8db6-414b-d8e9-95de93fc5db4"
      },
      "outputs": [],
      "source": [
        "## Core LC Chat Interface\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "469c8c7f-de62-457f-a30f-674763a8b717",
      "metadata": {},
      "source": [
        "## 调用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9512c81b-1f3a-4eca-9470-f52cedff5c74",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = llm.invoke(\"Write a ballad about LangChain.\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d35686b",
      "metadata": {},
      "source": [
        "## 使用 NVIDIA NIM\n\n准备部署时，您可以使用 NVIDIA NIM 自行托管模型。NVIDIA NIM 包含在 NVIDIA AI Enterprise 软件许可中，可以随处运行，让您完全拥有自己的定制内容以及知识产权 (IP) 和 AI 应用。\n\n[了解更多关于 NIM 的信息](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49838930",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "# connect to an embedding NIM running at localhost:8000, specifying a specific model\n",
        "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d37987-d568-4a73-9d2a-8bd86323f8bf",
      "metadata": {},
      "source": [
        "## 流式、批处理和异步\n\n这些模型原生支持流式处理，并且正如 LangChain 所有 LLM 一样，它们暴露了 batch 方法来处理并发请求，以及用于 invoke、stream 和 batch 的异步方法。以下是一些示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01fa5095-be72-47b0-8247-e9fac799435d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(llm.batch([\"What's 2*3?\", \"What's 2*6?\"]))\n",
        "# Or via the async API\n",
        "# await llm.abatch([\"What's 2*3?\", \"What's 2*6?\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75189ac6-e13f-414f-9064-075c77d6e754",
      "metadata": {},
      "outputs": [],
      "source": [
        "for chunk in llm.stream(\"How far can a seagull fly in one day?\"):\n",
        "    # Show the token separations\n",
        "    print(chunk.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9a4122-7a10-40c0-a979-82a769ce7f6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "async for chunk in llm.astream(\n",
        "    \"How long does it take for monarch butterflies to migrate?\"\n",
        "):\n",
        "    print(chunk.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6RrXHC_XqWc1",
      "metadata": {
        "id": "6RrXHC_XqWc1"
      },
      "source": [
        "## 支持的模型\n\n查询 `available_models` 将继续为您提供 API 凭证提供的所有其他模型。\n\n`playground_` 前缀是可选的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8a312d-38e9-4528-843e-59451bdadbac",
      "metadata": {},
      "outputs": [],
      "source": [
        "ChatNVIDIA.get_available_models()\n",
        "# llm.get_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8a407c6-e38b-4cfc-9a33-bcafadc18cf2",
      "metadata": {},
      "source": [
        "## 模型类型"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WMW79Iegqj4e",
      "metadata": {
        "id": "WMW79Iegqj4e"
      },
      "source": [
        "以上所有模型均受支持，并且可以通过 `ChatNVIDIA` 访问。\n\n某些模型类型支持独特的提示技术和聊天消息。我们将在下面回顾几个重要的模型。\n\n**要了解特定模型的更多信息，请导航到 AI Foundation 模型 [的 API 部分，链接在此](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/codellama-13b/api)。**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d65053-59fe-40cf-a2d0-55d3dbb13585",
      "metadata": {},
      "source": [
        "### 通用聊天\n\n诸如 `meta/llama3-8b-instruct` 和 `mistralai/mixtral-8x22b-instruct-v0.1` 等模型是优秀的通用模型，您可以将其与任何 LangChain 聊天消息一起使用。示例如下。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f7aee8-e90c-4d5a-ac97-0dd3d45c3f4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"You are a helpful AI assistant named Fred.\"), (\"user\", \"{input}\")]\n",
        ")\n",
        "chain = prompt | ChatNVIDIA(model=\"meta/llama3-8b-instruct\") | StrOutputParser()\n",
        "\n",
        "for txt in chain.stream({\"input\": \"What's your name?\"}):\n",
        "    print(txt, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04146118-281b-42ef-b781-2fadeeeea6c8",
      "metadata": {},
      "source": [
        "### 代码生成\n\n这些模型接受与常规聊天模型相同的参数和输入结构，但在代码生成和结构化代码任务方面往往表现更好。例如 `meta/codellama-70b`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49aa569b-5f33-47b3-9edc-df58313eb038",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert coding AI. Respond only in valid python; no narration whatsoever.\",\n",
        "        ),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "chain = prompt | ChatNVIDIA(model=\"meta/codellama-70b\") | StrOutputParser()\n",
        "\n",
        "for txt in chain.stream({\"input\": \"How do I solve this fizz buzz problem?\"}):\n",
        "    print(txt, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f465ff6-5922-41d8-8abb-1d1e4095cc27",
      "metadata": {},
      "source": [
        "## 多模态\n\nNVIDIA 也支持多模态输入，这意味着您可以同时提供图像和文本供模型进行推理。一个支持多模态输入的示例模型是 `nvidia/neva-22b`。\n\n以下是一个使用示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26625437-1695-440f-b792-b85e6add9a90",
      "metadata": {},
      "outputs": [],
      "source": [
        "import IPython\n",
        "import requests\n",
        "\n",
        "image_url = \"https://www.nvidia.com/content/dam/en-zz/Solutions/research/ai-playground/nvidia-picasso-3c33-p@2x.jpg\"  ## Large Image\n",
        "image_content = requests.get(image_url).content\n",
        "\n",
        "IPython.display.Image(image_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfbbe57c-27a5-4cbb-b967-19c4e7d29fd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "llm = ChatNVIDIA(model=\"nvidia/neva-22b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ddcb8f1-9cd8-4376-963d-af61c29b2a3c",
      "metadata": {},
      "source": [
        "#### 将图片作为 URL 传递"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "432ea2a2-4d39-43f8-a236-041294171f14",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "llm.invoke(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": \"Describe this image:\"},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0573dd1f-9a17-4c99-ab2a-8d930b89d283",
      "metadata": {},
      "source": [
        "#### 将图片作为 base64 编码字符串传递"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be1688a5",
      "metadata": {},
      "source": "目前，在客户端会进行一些额外的处理来支持像上面那样的大型图片。但对于小型图片（以及为了更好地说明底层正在进行的流程），我们可以直接传入图片，如下所示："
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c58f1dd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import IPython\n",
        "import requests\n",
        "\n",
        "image_url = \"https://picsum.photos/seed/kitten/300/200\"\n",
        "image_content = requests.get(image_url).content\n",
        "\n",
        "IPython.display.Image(image_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c721629-42eb-4006-bf68-0296f7925ebc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "## Works for simpler images. For larger images, see actual implementation\n",
        "b64_string = base64.b64encode(image_content).decode(\"utf-8\")\n",
        "\n",
        "llm.invoke(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": \"Describe this image:\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/png;base64,{b64_string}\"},\n",
        "                },\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba958424-28d7-4bc2-9c8e-bd571066853f",
      "metadata": {},
      "source": [
        "#### 直接嵌入字符串中\n\nNVIDIA API 独特之处在于它接受以 base64 编码的图片，并将其内联在 `<img/>` HTML 标签中。虽然这与其他 LLM 的互操作性不佳，但你可以直接相应地提示模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00c06a9a-497b-4192-a842-b075e27401aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "base64_with_mime_type = f\"data:image/png;base64,{b64_string}\"\n",
        "llm.invoke(f'What\\'s in this image?\\n<img src=\"{base64_with_mime_type}\" />')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "137662a6",
      "metadata": {
        "id": "137662a6"
      },
      "source": [
        "## 在 `RunnableWithMessageHistory` 中使用示例"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79efa62d",
      "metadata": {
        "id": "79efa62d"
      },
      "source": [
        "与任何其他集成一样，ChatNVIDIA 可以很好地支持聊天工具，例如 RunnableWithMessageHistory，它类似于使用 `ConversationChain`。下面，我们展示了应用于 `mistralai/mixtral-8x22b-instruct-0.1` 模型的 [LangChain RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) 示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "082ccb21-91e1-4e71-a9ba-4bff1e89f105",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2c6bc1",
      "metadata": {
        "id": "fd2c6bc1"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# store is a dictionary that maps session IDs to their corresponding chat histories.\n",
        "store = {}  # memory is maintained outside the chain\n",
        "\n",
        "\n",
        "# A function that returns the chat history for a given session ID.\n",
        "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "\n",
        "chat = ChatNVIDIA(\n",
        "    model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=100,\n",
        "    top_p=1.0,\n",
        ")\n",
        "\n",
        "#  Define a RunnableConfig object, with a `configurable` key. session_id determines thread\n",
        "config = {\"configurable\": {\"session_id\": \"1\"}}\n",
        "\n",
        "conversation = RunnableWithMessageHistory(\n",
        "    chat,\n",
        "    get_session_history,\n",
        ")\n",
        "\n",
        "conversation.invoke(\n",
        "    \"Hi I'm Srijan Dubey.\",  # input or query\n",
        "    config=config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uHIMZxVSVNBC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "uHIMZxVSVNBC",
        "outputId": "79acc89d-a820-4f2c-bac2-afe99da95580"
      },
      "outputs": [],
      "source": [
        "conversation.invoke(\n",
        "    \"I'm doing well! Just having a conversation with an AI.\",\n",
        "    config=config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LyD1xVKmVSs4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "LyD1xVKmVSs4",
        "outputId": "a1714513-a8fd-4d14-f974-233e39d5c4f5"
      },
      "outputs": [],
      "source": [
        "conversation.invoke(\n",
        "    \"Tell me about yourself.\",\n",
        "    config=config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3cbbba0",
      "metadata": {},
      "source": [
        "## 工具调用\n\n从 v0.2 开始，`ChatNVIDIA` 支持 [bind_tools](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.bind_tools)。\n\n`ChatNVIDIA` 提供了与 [build.nvidia.com](https://build.nvidia.com) 上各种模型以及本地 NIM 的集成。并非所有这些模型都经过工具调用训练。请务必为您的实验和应用程序选择一个支持工具调用的模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f7b535e",
      "metadata": {},
      "source": [
        "您可以获取已知支持工具调用的模型列表，"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e36c8911",
      "metadata": {},
      "outputs": [],
      "source": [
        "tool_models = [\n",
        "    model for model in ChatNVIDIA.get_available_models() if model.supports_tools\n",
        "]\n",
        "tool_models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b01d75a7",
      "metadata": {},
      "source": [
        "使用一个能力强大的模型，"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd54f174",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from pydantic import Field\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_current_weather(\n",
        "    location: str = Field(..., description=\"The location to get the weather for.\"),\n",
        "):\n",
        "    \"\"\"Get the current weather for a location.\"\"\"\n",
        "    ...\n",
        "\n",
        "\n",
        "llm = ChatNVIDIA(model=tool_models[0].id).bind_tools(tools=[get_current_weather])\n",
        "response = llm.invoke(\"What is the weather in Boston?\")\n",
        "response.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e08df68c",
      "metadata": {},
      "source": [
        "请参阅 [如何使用聊天模型调用工具](https://python.langchain.com/docs/how_to/tool_calling/) 获取更多示例。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a3c438-121d-46eb-8fb5-b8d5a13cd4a4",
      "metadata": {},
      "source": [
        "## 链式调用\n\n我们可以像这样将模型与提示模板进行[链式调用](/docs/how_to/sequence/)："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af585c6b-fe0a-4833-9860-a4209a71b3c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"English\",\n",
        "        \"output_language\": \"German\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2f25dd3-0b4a-465f-a53e-95521cdc253c",
      "metadata": {},
      "source": [
        "## API 参考\n\n有关 `ChatNVIDIA` 所有功能和配置的详细文档，请访问 API 参考：https://python.langchain.com/api_reference/nvidia_ai_endpoints/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}