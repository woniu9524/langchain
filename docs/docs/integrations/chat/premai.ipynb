{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_label: PremAI\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChatPremAI\n\n[PremAI](https://premai.io/) æ˜¯ä¸€ä¸ªä¸€ç«™å¼å¹³å°ï¼Œå¯ç®€åŒ–ç”Ÿæˆå¼ AI é©±åŠ¨çš„ã€ç¨³å¥çš„ã€ç”Ÿäº§å°±ç»ªå‹åº”ç”¨çš„åˆ›å»ºè¿‡ç¨‹ã€‚é€šè¿‡ä¼˜åŒ–å¼€å‘æµç¨‹ï¼ŒPremAI ä½¿æ‚¨èƒ½å¤Ÿä¸“æ³¨äºæå‡ç”¨æˆ·ä½“éªŒå’Œæ¨åŠ¨åº”ç”¨çš„æ•´ä½“å¢é•¿ã€‚æ‚¨å¯ä»¥é€šè¿‡ [æ­¤å¤„](https://docs.premai.io/quick-start) å¿«é€Ÿå¼€å§‹ä½¿ç”¨æˆ‘ä»¬çš„å¹³å°ã€‚\n\næœ¬ç¤ºä¾‹å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨ LangChain é€šè¿‡ `ChatPremAI` ä¸ä¸åŒçš„èŠå¤©æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å®‰è£…å’Œè®¾ç½®\n\næˆ‘ä»¬é¦–å…ˆå®‰è£… `langchain` å’Œ `premai-sdk`ã€‚æ‚¨å¯ä»¥é€šè¿‡é”®å…¥ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…ï¼š\n\n```bash\npip install premai langchain\n```\n\nåœ¨ç»§ç»­æ“ä½œä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²åœ¨ PremAI ä¸Šåˆ›å»ºè´¦æˆ·å¹¶å·²åˆ›å»ºé¡¹ç›®ã€‚å¦‚æœå°šæœªå®Œæˆï¼Œè¯·å‚é˜…[å¿«é€Ÿå…¥é—¨](https://docs.premai.io/introduction)æŒ‡å—ä»¥å¼€å§‹ä½¿ç”¨ PremAI å¹³å°ã€‚åˆ›å»ºæ‚¨çš„ç¬¬ä¸€ä¸ªé¡¹ç›®å¹¶è·å– API å¯†é’¥ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.chat_models import ChatPremAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### åœ¨ LangChain ä¸­è®¾ç½® PremAI å®¢æˆ·ç«¯\n\nå¯¼å…¥æ‰€éœ€æ¨¡å—åï¼Œæˆ‘ä»¬æ¥è®¾ç½®å®¢æˆ·ç«¯ã€‚ç›®å‰æˆ‘ä»¬å‡è®¾ `project_id` æ˜¯ `8`ã€‚ä½†è¯·ç¡®ä¿ä½¿ç”¨æ‚¨è‡ªå·±çš„ project-idï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚\n\nè¦å°† langchain ä¸ prem ç»“åˆä½¿ç”¨ï¼Œæ‚¨æ— éœ€ä¼ é€’ä»»ä½•æ¨¡å‹åç§°æˆ–ä½¿ç”¨æˆ‘ä»¬çš„ chat-client è®¾ç½®ä»»ä½•å‚æ•°ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒå°†ä½¿ç”¨ [LaunchPad](https://docs.premai.io/get-started/launchpad) ä¸­ä½¿ç”¨çš„æ¨¡å‹åç§°å’Œå‚æ•°ã€‚\n\n> æ³¨æ„ï¼šå¦‚æœæ‚¨åœ¨è®¾ç½®å®¢æˆ·ç«¯æ—¶æ›´æ”¹äº† `model` æˆ– `temperature` æˆ– `max_tokens` ç­‰å…¶ä»–å‚æ•°ï¼Œå®ƒå°†è¦†ç›– LaunchPad ä¸­ä½¿ç”¨çš„ç°æœ‰é»˜è®¤é…ç½®ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# First step is to set up the env variable.\n",
        "# you can also pass the API key while instantiating the model but this\n",
        "# comes under a best practices to set it as env variable.\n",
        "\n",
        "if os.environ.get(\"PREMAI_API_KEY\") is None:\n",
        "    os.environ[\"PREMAI_API_KEY\"] = getpass.getpass(\"PremAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# By default it will use the model which was deployed through the platform\n",
        "# in my case it will is \"gpt-4o\"\n",
        "\n",
        "chat = ChatPremAI(project_id=1234, model_name=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### èŠå¤©è¡¥å…¨\n\n`ChatPremAI` æ”¯æŒä¸¤ç§æ–¹æ³•ï¼š`invoke`ï¼ˆä¸ `generate` ç›¸åŒï¼‰å’Œ `stream`ã€‚\n\nç¬¬ä¸€ç§æ–¹æ³•ä¼šç»™æˆ‘ä»¬ä¸€ä¸ªé™æ€ç»“æœã€‚è€Œç¬¬äºŒç§æ–¹æ³•ä¼šé€ä¸ªæµå¼ä¼ è¾“ tokenã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ç”Ÿæˆç±»ä¼¼èŠå¤©çš„è¡¥å…¨ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am an AI language model created by OpenAI, designed to assist with answering questions and providing information based on the context provided. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "human_message = HumanMessage(content=\"Who are you?\")\n",
        "\n",
        "response = chat.invoke([human_message])\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "è¿™çœ‹èµ·æ¥å¾ˆæœ‰è¶£ï¼Œå¯¹å§ï¼Ÿæˆ‘å°†æˆ‘çš„é»˜è®¤å¯åŠ¨æ¿ç³»ç»Ÿæç¤ºè®¾ç½®ä¸ºï¼šâ€œå§‹ç»ˆåƒæµ·ç›—ä¸€æ ·è¯´è¯â€ã€‚æ‚¨ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦è¦†ç›–é»˜è®¤ç³»ç»Ÿæç¤ºã€‚æ–¹æ³•å¦‚ä¸‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm your friendly assistant! How can I help you today?\", response_metadata={'document_chunks': [{'repository_id': 1985, 'document_id': 1306, 'chunk_id': 173899, 'document_name': '[D] Difference between sparse and dense informatiâ€¦', 'similarity_score': 0.3209080100059509, 'content': \"with the difference or anywhere\\nwhere I can read about it?\\n\\n\\n      17                  9\\n\\n\\n      u/ScotiabankCanada        â€¢  Promoted\\n\\n\\n                       Accelerate your study permit process\\n                       with Scotiabank's Student GIC\\n                       Program. We're here to help you turâ€¦\\n\\n\\n                       startright.scotiabank.com         Learn More\\n\\n\\n                            Add a Comment\\n\\n\\nSort by:   Best\\n\\n\\n      DinosParkour      â€¢ 1y ago\\n\\n\\n     Dense Retrieval (DR) m\"}]}, id='run-510bbd0e-3f8f-4095-9b1f-c2d29fd89719-0')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "system_message = SystemMessage(content=\"You are a friendly assistant.\")\n",
        "human_message = HumanMessage(content=\"Who are you?\")\n",
        "\n",
        "chat.invoke([system_message, human_message])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ä½ å¯ä»¥åœ¨è¿™é‡Œè¿™æ ·æä¾› system promptï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/anindya/prem/langchain/libs/community/langchain_community/chat_models/premai.py:355: UserWarning: WARNING: Parameter top_p is not supported in kwargs.\n",
            "  warnings.warn(f\"WARNING: Parameter {key} is not supported in kwargs.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! I'm your friendly assistant. How can I\", response_metadata={'document_chunks': [{'repository_id': 1985, 'document_id': 1306, 'chunk_id': 173899, 'document_name': '[D] Difference between sparse and dense informatiâ€¦', 'similarity_score': 0.3209080100059509, 'content': \"with the difference or anywhere\\nwhere I can read about it?\\n\\n\\n      17                  9\\n\\n\\n      u/ScotiabankCanada        â€¢  Promoted\\n\\n\\n                       Accelerate your study permit process\\n                       with Scotiabank's Student GIC\\n                       Program. We're here to help you turâ€¦\\n\\n\\n                       startright.scotiabank.com         Learn More\\n\\n\\n                            Add a Comment\\n\\n\\nSort by:   Best\\n\\n\\n      DinosParkour      â€¢ 1y ago\\n\\n\\n     Dense Retrieval (DR) m\"}]}, id='run-c4b06b98-4161-4cca-8495-fd2fc98fa8f8-0')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat.invoke([system_message, human_message], temperature=0.7, max_tokens=10, top_p=0.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> å¦‚æœä½ åœ¨è¿™é‡Œæ”¾ç½®ç³»ç»Ÿæç¤ºï¼Œå®ƒå°†è¦†ç›–ä½ åœ¨ä»å¹³å°éƒ¨ç½²åº”ç”¨ç¨‹åºæ—¶è®¾ç½®çš„å›ºå®šç³»ç»Ÿæç¤ºã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å¯¹ Prem å­˜å‚¨åº“çš„åŸç”Ÿ RAG æ”¯æŒ\n\nPrem å­˜å‚¨åº“å…è®¸ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£ï¼ˆ.txtã€.pdf ç­‰ï¼‰ï¼Œå¹¶å°†è¿™äº›å­˜å‚¨åº“è¿æ¥åˆ° LLMã€‚æ‚¨å¯ä»¥å°† Prem å­˜å‚¨åº“è§†ä¸ºåŸç”Ÿ RAGï¼Œå…¶ä¸­æ¯ä¸ªå­˜å‚¨åº“éƒ½å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå‘é‡æ•°æ®åº“ã€‚æ‚¨å¯ä»¥è¿æ¥å¤šä¸ªå­˜å‚¨åº“ã€‚æ‚¨å¯ä»¥åœ¨[æ­¤å¤„](https://docs.premai.io/get-started/repositories)äº†è§£æ›´å¤šå…³äºå­˜å‚¨åº“çš„ä¿¡æ¯ã€‚\n\nLangchain Premai ä¹Ÿæ”¯æŒå­˜å‚¨åº“ã€‚æ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è¿›è¡Œæ“ä½œã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"Which models are used for dense retrieval\"\n",
        "repository_ids = [\n",
        "    1985,\n",
        "]\n",
        "repositories = dict(ids=repository_ids, similarity_threshold=0.3, limit=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å®šä¹‰ä¸€äº›ä»“åº“ ID æ¥å®šä¹‰æˆ‘ä»¬çš„ä»“åº“ã€‚è¯·ç¡®ä¿è¿™äº› ID æ˜¯æœ‰æ•ˆçš„ä»“åº“ IDã€‚æ‚¨å¯ä»¥é€šè¿‡[æ­¤å¤„](https://docs.premai.io/get-started/repositories)äº†è§£æ›´å¤šå…³äºå¦‚ä½•è·å–ä»“åº“ ID çš„ä¿¡æ¯ã€‚\n\n> è¯·æ³¨æ„ï¼šä¸ `model_name` ç±»ä¼¼ï¼Œå½“æ‚¨è°ƒç”¨ `repositories` å‚æ•°æ—¶ï¼Œæ‚¨å¯èƒ½ä¼šè¦†ç›–åœ¨ launchpad ä¸­è¿æ¥çš„ä»“åº“ã€‚\n\nç°åœ¨ï¼Œæˆ‘ä»¬å°†ä»“åº“ä¸æˆ‘ä»¬çš„èŠå¤©å¯¹è±¡è¿æ¥èµ·æ¥ï¼Œä»¥è°ƒç”¨åŸºäº RAG çš„ç”Ÿæˆã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dense retrieval models typically include:\n",
            "\n",
            "1. **BERT-based Models**: Such as DPR (Dense Passage Retrieval) which uses BERT for encoding queries and passages.\n",
            "2. **ColBERT**: A model that combines BERT with late interaction mechanisms.\n",
            "3. **ANCE (Approximate Nearest Neighbor Negative Contrastive Estimation)**: Uses BERT and focuses on efficient retrieval.\n",
            "4. **TCT-ColBERT**: A variant of ColBERT that uses a two-tower\n",
            "{\n",
            "    \"document_chunks\": [\n",
            "        {\n",
            "            \"repository_id\": 1985,\n",
            "            \"document_id\": 1306,\n",
            "            \"chunk_id\": 173899,\n",
            "            \"document_name\": \"[D] Difference between sparse and dense informati\\u2026\",\n",
            "            \"similarity_score\": 0.3209080100059509,\n",
            "            \"content\": \"with the difference or anywhere\\nwhere I can read about it?\\n\\n\\n      17                  9\\n\\n\\n      u/ScotiabankCanada        \\u2022  Promoted\\n\\n\\n                       Accelerate your study permit process\\n                       with Scotiabank's Student GIC\\n                       Program. We're here to help you tur\\u2026\\n\\n\\n                       startright.scotiabank.com         Learn More\\n\\n\\n                            Add a Comment\\n\\n\\nSort by:   Best\\n\\n\\n      DinosParkour      \\u2022 1y ago\\n\\n\\n     Dense Retrieval (DR) m\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "response = chat.invoke(query, max_tokens=100, repositories=repositories)\n",
        "\n",
        "print(response.content)\n",
        "print(json.dumps(response.response_metadata, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ç†æƒ³æƒ…å†µä¸‹ï¼Œæ‚¨æ— éœ€åœ¨æ­¤å¤„è¿æ¥ä»“åº“ ID å³å¯è·å¾—æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚å¦‚æœæ‚¨åœ¨ prem å¹³å°ä¸­è¿æ¥äº†ä»“åº“ï¼Œæ‚¨ä»ç„¶å¯ä»¥è·å¾—ç›¸åŒçš„ç»“æœã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prem æ¨¡æ¿\n\nç¼–å†™ Prompt Templates å¯èƒ½éå¸¸æ··ä¹±ã€‚Prompt æ¨¡æ¿åˆé•¿åˆéš¾ç®¡ç†ï¼Œå¹¶ä¸”å¿…é¡»ä¸æ–­è°ƒæ•´æ‰èƒ½æ”¹è¿›å¹¶ä¿æŒåœ¨æ•´ä¸ªåº”ç”¨ç¨‹åºä¸­çš„ä¸€è‡´æ€§ã€‚\n\nä½¿ç”¨ **Prem**ï¼Œç¼–å†™å’Œç®¡ç† prompt å¯ä»¥å˜å¾—éå¸¸ç®€å•ã€‚[Launchpad](https://docs.premai.io/get-started/launchpad) ä¸­çš„ **_Templates_** é€‰é¡¹å¡å¯ä»¥å¸®åŠ©æ‚¨ç¼–å†™æ‰€éœ€çš„ä»»æ„æ•°é‡çš„ promptï¼Œå¹¶åœ¨ SDK ä¸­ä½¿ç”¨å®ƒä»¬ï¼Œä»¥ä¾¿ä½¿ç”¨è¿™äº› prompt è¿è¡Œæ‚¨çš„åº”ç”¨ç¨‹åºã€‚æ‚¨å¯ä»¥ [æ­¤å¤„](https://docs.premai.io/get-started/prem-templates) é˜…è¯»æœ‰å…³ Prompt Templates çš„æ›´å¤šä¿¡æ¯ã€‚\n\nä¸ºäº†åœ¨ LangChain ä¸­æœ¬æœºä½¿ç”¨ Prem æ¨¡æ¿ï¼Œæ‚¨éœ€è¦å°† ID ä¼ é€’ç»™ `HumanMessage`ã€‚æ­¤ ID åº”è¯¥æ˜¯æ‚¨çš„ prompt æ¨¡æ¿å˜é‡çš„åç§°ã€‚`HumanMessage` ä¸­çš„ `content` åº”è¯¥æ˜¯è¯¥å˜é‡çš„å€¼ã€‚\n\nå‡è®¾ä¾‹å¦‚ï¼Œæ‚¨çš„ prompt æ¨¡æ¿æ˜¯è¿™æ ·çš„ï¼š\n\n```text\nSay hello to my name and say a feel-good quote\nfrom my age. My name is: {name} and age is {age}\n```\n\næ‰€ä»¥ç°åœ¨æ‚¨çš„ `human_messages` åº”è¯¥çœ‹èµ·æ¥åƒï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "human_messages = [\n",
        "    HumanMessage(content=\"Shawn\", id=\"name\"),\n",
        "    HumanMessage(content=\"22\", id=\"age\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "å°†æ­¤ `human_messages` ä¼ é€’ç»™ ChatPremAI Clientã€‚è¯·æ³¨æ„ï¼šä¸è¦å¿˜è®°ä¼ é€’é¢å¤–çš„ `template_id` æ¥è°ƒç”¨ Prem æ¨¡æ¿è¿›è¡Œç”Ÿæˆã€‚å¦‚æœæ‚¨ä¸äº†è§£ `template_id`ï¼Œå¯ä»¥åœ¨[æˆ‘ä»¬çš„æ–‡æ¡£](https://docs.premai.io/get-started/prem-templates)ä¸­äº†è§£æ›´å¤šã€‚ç¤ºä¾‹å¦‚ä¸‹ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template_id = \"78069ce8-xxxxx-xxxxx-xxxx-xxx\"\n",
        "response = chat.invoke([human_messages], template_id=template_id)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prem æ¨¡æ¿åŠŸèƒ½åœ¨æµå¼ä¼ è¾“ä¸­ä¹Ÿå¯ç”¨ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### æµå¼ä¼ è¾“\n\nåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•ä½¿ç”¨ langchain å’Œ PremAI æµå¼ä¼ è¾“ tokensã€‚æ–¹æ³•å¦‚ä¸‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like your message got cut off. If you need information about Dense Retrieval (DR) or any other topic, please provide more details or clarify your question."
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "for chunk in chat.stream(\"hello how are you\"):\n",
        "    sys.stdout.write(chunk.content)\n",
        "    sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ä¸ä¸Šé¢ç±»ä¼¼ï¼Œå¦‚æœæ‚¨æƒ³è¦†ç›– system-prompt å’Œ generation å‚æ•°ï¼Œåˆ™éœ€è¦æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Woof! ğŸ¾ How can I help you today? Want to play fetch or maybe go for a walk ğŸ¶ğŸ¦´"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# For some experimental reasons if you want to override the system prompt then you\n",
        "# can pass that here too. However it is not recommended to override system prompt\n",
        "# of an already deployed model.\n",
        "\n",
        "for chunk in chat.stream(\n",
        "    \"hello how are you\",\n",
        "    system_prompt=\"act like a dog\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=200,\n",
        "):\n",
        "    sys.stdout.write(chunk.content)\n",
        "    sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å·¥å…·/å‡½æ•°è°ƒç”¨\n\nLangChain PremAI æ”¯æŒå·¥å…·/å‡½æ•°è°ƒç”¨ã€‚å·¥å…·/å‡½æ•°è°ƒç”¨å…è®¸æ¨¡å‹é€šè¿‡ç”Ÿæˆç¬¦åˆç”¨æˆ·å®šä¹‰æ¨¡å¼çš„è¾“å‡ºæ¥å“åº”ç»™å®šæç¤ºã€‚\n\n- æ‚¨å¯ä»¥åœ¨[æ­¤å¤„çš„æ–‡æ¡£](https://docs.premai.io/get-started/function-calling)ä¸­è¯¦ç»†äº†è§£æ‰€æœ‰å·¥å…·è°ƒç”¨ã€‚\n- æ‚¨å¯ä»¥åœ¨[æ–‡æ¡£çš„è¿™éƒ¨åˆ†](https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling)äº†è§£æ›´å¤šå…³äº langchain å·¥å…·è°ƒç”¨çš„ä¿¡æ¯ã€‚\n\n**æ³¨æ„ï¼š**\nLangChain ChatPremAI çš„å½“å‰ç‰ˆæœ¬ä¸æ”¯æŒå¸¦æµå¼ä¼ è¾“çš„å‡½æ•°/å·¥å…·è°ƒç”¨ã€‚æµå¼ä¼ è¾“æ”¯æŒå°†å¾ˆå¿«æ¨å‡ºã€‚\n\n#### å°†å·¥å…·ä¼ é€’ç»™æ¨¡å‹\n\nä¸ºäº†ä¼ é€’å·¥å…·å¹¶è®© LLM é€‰æ‹©å®ƒéœ€è¦è°ƒç”¨çš„å·¥å…·ï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’ä¸€ä¸ªå·¥å…· schemaã€‚å·¥å…· schema æ˜¯å‡½æ•°å®šä¹‰ï¼Œä»¥åŠå…³äºå‡½æ•°åšä»€ä¹ˆã€å‡½æ•°çš„æ¯ä¸ªå‚æ•°æ˜¯ä»€ä¹ˆç­‰çš„æ­£ç¡®æ–‡æ¡£å­—ç¬¦ä¸²ã€‚ä¸‹é¢æ˜¯ä¸€äº›ç®€å•çš„ç®—æœ¯å‡½æ•°åŠå…¶ schemaã€‚\n\n**æ³¨æ„ï¼š** åœ¨å®šä¹‰å‡½æ•°/å·¥å…· schema æ—¶ï¼Œä¸è¦å¿˜è®°æ·»åŠ æœ‰å…³å‡½æ•°å‚æ•°çš„ä¿¡æ¯ï¼Œå¦åˆ™ä¼šå¼•å‘é”™è¯¯ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Define the schema for function arguments\n",
        "class OperationInput(BaseModel):\n",
        "    a: int = Field(description=\"First number\")\n",
        "    b: int = Field(description=\"Second number\")\n",
        "\n",
        "\n",
        "# Now define the function where schema for argument will be OperationInput\n",
        "@tool(\"add\", args_schema=OperationInput, return_direct=True)\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@tool(\"multiply\", args_schema=OperationInput, return_direct=True)\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplies a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### å°†å·¥å…·æ¶æ„ä¸æˆ‘ä»¬çš„ LLM ç»‘å®š\n\næˆ‘ä»¬ç°åœ¨å°†ä½¿ç”¨ `bind_tools` æ–¹æ³•å°†ä¸Šè¿°å‡½æ•°è½¬æ¢ä¸ºä¸€ä¸ªâ€œå·¥å…·â€ï¼Œå¹¶å°†å…¶ä¸æ¨¡å‹ç»‘å®šã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†åœ¨æ¯æ¬¡è°ƒç”¨æ¨¡å‹æ—¶ä¼ é€’è¿™äº›å·¥å…·ä¿¡æ¯ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "tools = [add, multiply]\n",
        "llm_with_tools = chat.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "åœ¨æ­¤ä¹‹åï¼Œæˆ‘ä»¬ä»å·²ç»‘å®šå·¥å…·çš„æ¨¡å‹é‚£é‡Œè·å¾—å“åº”ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
        "\n",
        "messages = [HumanMessage(query)]\n",
        "ai_msg = llm_with_tools.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œå½“æˆ‘ä»¬çš„èŠå¤©æ¨¡å‹ä¸å·¥å…·ç»‘å®šæ—¶ï¼Œå®ƒä¼šæ ¹æ®ç»™å®šçš„æç¤ºï¼ŒæŒ‰é¡ºåºè°ƒç”¨æ­£ç¡®çš„å·¥å…·é›†ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'multiply',\n",
              "  'args': {'a': 3, 'b': 12},\n",
              "  'id': 'call_A9FL20u12lz6TpOLaiS6rFa8'},\n",
              " {'name': 'add',\n",
              "  'args': {'a': 11, 'b': 49},\n",
              "  'id': 'call_MPKYGLHbf39csJIyb5BZ9xIk'}]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_msg.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "æˆ‘ä»¬å°†æ­¤æ¶ˆæ¯é™„åŠ åˆ° LLMï¼ŒLLM ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¹¶è®© LLM çŸ¥æ™“å®ƒå·²è°ƒç”¨äº†å“ªäº›å‡½æ•°ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages.append(ai_msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ç”±äºå·¥å…·è°ƒç”¨åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š\n\n1. åœ¨æˆ‘ä»¬çš„ç¬¬ä¸€æ¬¡è°ƒç”¨ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†äº† LLM å†³å®šä½¿ç”¨çš„æ‰€æœ‰å·¥å…·ï¼Œä»¥ä¾¿å°†å…¶ç»“æœä½œä¸ºé™„åŠ ä¸Šä¸‹æ–‡ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®ã€æ— è‡†æƒ³çš„ç»“æœã€‚\n\n2. åœ¨æˆ‘ä»¬çš„ç¬¬äºŒæ¬¡è°ƒç”¨ä¸­ï¼Œæˆ‘ä»¬å°†è§£æ LLM å†³å®šçš„è¿™ç»„å·¥å…·å¹¶è¿è¡Œå®ƒä»¬ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå°†æ˜¯æˆ‘ä»¬å®šä¹‰çš„å‡½æ•°ï¼Œä»¥åŠ LLM æå–çš„å‚æ•°ï¼‰ï¼Œç„¶åå°†æ­¤ç»“æœä¼ é€’ç»™ LLMã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "for tool_call in ai_msg.tool_calls:\n",
        "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
        "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
        "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "æœ€åï¼Œæˆ‘ä»¬å°†å‡½æ•°å“åº”æ·»åŠ åˆ°å…¶ä¸Šä¸‹æ–‡ä¸­ï¼Œè°ƒç”¨ LLMï¼ˆä¸å·¥å…·ç»‘å®šï¼‰ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The final answers are:\n",
            "\n",
            "- 3 * 12 = 36\n",
            "- 11 + 49 = 60\n"
          ]
        }
      ],
      "source": [
        "response = llm_with_tools.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å®šä¹‰å·¥å…· schemaï¼šPydantic ç±»\n\nä¸Šé¢æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `tool` è£…é¥°å™¨æ¥å®šä¹‰ schemaï¼Œä¸è¿‡æˆ‘ä»¬ä¹Ÿå¯ä»¥ç­‰æ•ˆåœ°ä½¿ç”¨ Pydantic æ¥å®šä¹‰ schemaã€‚å½“æ‚¨çš„å·¥å…·è¾“å…¥æ›´åŠ å¤æ‚æ—¶ï¼ŒPydantic éå¸¸æœ‰ç”¨ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
        "\n",
        "\n",
        "class add(BaseModel):\n",
        "    \"\"\"Add two integers together.\"\"\"\n",
        "\n",
        "    a: int = Field(..., description=\"First integer\")\n",
        "    b: int = Field(..., description=\"Second integer\")\n",
        "\n",
        "\n",
        "class multiply(BaseModel):\n",
        "    \"\"\"Multiply two integers together.\"\"\"\n",
        "\n",
        "    a: int = Field(..., description=\"First integer\")\n",
        "    b: int = Field(..., description=\"Second integer\")\n",
        "\n",
        "\n",
        "tools = [add, multiply]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ç»‘å®šåˆ°èŠå¤©æ¨¡å‹å¹¶ç›´æ¥è·å–ç»“æœï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[multiply(a=3, b=12), add(a=11, b=49)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = llm_with_tools | PydanticToolsParser(tools=[multiply, add])\n",
        "chain.invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ç°åœ¨ï¼Œå¦‚ä¸Šæ‰€ç¤ºï¼Œæˆ‘ä»¬è§£æå®ƒå¹¶è¿è¡Œè¿™äº›å‡½æ•°ï¼Œå†æ¬¡è°ƒç”¨ LLM ä»¥è·å–ç»“æœã€‚"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}