{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RunPod 聊天模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "开始使用 RunPod 聊天模型。\n\n## 概述\n\n本指南介绍了如何使用 LangChain 的 `ChatRunPod` 类与托管在 [RunPod Serverless](https://www.runpod.io/serverless-gpu) 上的聊天模型进行交互。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 设置\n\n1.  **安装包：**\n    ```bash\n    pip install -qU langchain-runpod\n    ```\n2.  **部署聊天模型端点：** 按照 [RunPod 提供商指南](/docs/integrations/providers/runpod#setup) 中的设置步骤，在 RunPod Serverless 上部署兼容的聊天模型端点并获取其 Endpoint ID。\n3.  **设置环境变量：** 确保设置了 `RUNPOD_API_KEY` 和 `RUNPOD_ENDPOINT_ID`（或特定的 `RUNPOD_CHAT_ENDPOINT_ID`）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Make sure environment variables are set (or pass them directly to ChatRunPod)\n",
        "if \"RUNPOD_API_KEY\" not in os.environ:\n",
        "    os.environ[\"RUNPOD_API_KEY\"] = getpass.getpass(\"Enter your RunPod API Key: \")\n",
        "\n",
        "if \"RUNPOD_ENDPOINT_ID\" not in os.environ:\n",
        "    os.environ[\"RUNPOD_ENDPOINT_ID\"] = input(\n",
        "        \"Enter your RunPod Endpoint ID (used if RUNPOD_CHAT_ENDPOINT_ID is not set): \"\n",
        "    )\n",
        "\n",
        "# Optionally use a different endpoint ID specifically for chat models\n",
        "# if \"RUNPOD_CHAT_ENDPOINT_ID\" not in os.environ:\n",
        "#     os.environ[\"RUNPOD_CHAT_ENDPOINT_ID\"] = input(\"Enter your RunPod Chat Endpoint ID (Optional): \")\n",
        "\n",
        "chat_endpoint_id = os.environ.get(\n",
        "    \"RUNPOD_CHAT_ENDPOINT_ID\", os.environ.get(\"RUNPOD_ENDPOINT_ID\")\n",
        ")\n",
        "if not chat_endpoint_id:\n",
        "    raise ValueError(\n",
        "        \"No RunPod Endpoint ID found. Please set RUNPOD_ENDPOINT_ID or RUNPOD_CHAT_ENDPOINT_ID.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 实例化\n\n初始化 `ChatRunPod` 类。您可以通过 `model_kwargs` 传递模型专属参数，并配置轮询行为。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from langchain_runpod import ChatRunPod\n",
        "\n",
        "chat = ChatRunPod(\n",
        "    runpod_endpoint_id=chat_endpoint_id,  # Specify the correct endpoint ID\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.9,\n",
        "        # Add other parameters supported by your endpoint handler\n",
        "    },\n",
        "    # Optional: Adjust polling\n",
        "    # poll_interval=0.2,\n",
        "    # max_polling_attempts=150\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 调用\n\n使用标准的 LangChain `.invoke()` 和 `.ainvoke()` 方法来调用模型。还支持通过 `.stream()` 和 `.astream()` 进行流式调用（通过轮询 RunPod `/stream` 端点模拟）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
        "    HumanMessage(content=\"What is the RunPod Serverless API flow?\"),\n",
        "]\n",
        "\n",
        "# Invoke (Sync)\n",
        "try:\n",
        "    response = chat.invoke(messages)\n",
        "    print(\"--- Sync Invoke Response ---\")\n",
        "    print(response.content)\n",
        "except Exception as e:\n",
        "    print(\n",
        "        f\"Error invoking Chat Model: {e}. Ensure endpoint ID/API key are correct and endpoint is active/compatible.\"\n",
        "    )\n",
        "\n",
        "# Stream (Sync, simulated via polling /stream)\n",
        "print(\"\\n--- Sync Stream Response ---\")\n",
        "try:\n",
        "    for chunk in chat.stream(messages):\n",
        "        print(chunk.content, end=\"\", flush=True)\n",
        "    print()  # Newline\n",
        "except Exception as e:\n",
        "    print(\n",
        "        f\"\\nError streaming Chat Model: {e}. Ensure endpoint handler supports streaming output format.\"\n",
        "    )\n",
        "\n",
        "### Async Usage\n",
        "\n",
        "# AInvoke (Async)\n",
        "try:\n",
        "    async_response = await chat.ainvoke(messages)\n",
        "    print(\"--- Async Invoke Response ---\")\n",
        "    print(async_response.content)\n",
        "except Exception as e:\n",
        "    print(f\"Error invoking Chat Model asynchronously: {e}.\")\n",
        "\n",
        "# AStream (Async)\n",
        "print(\"\\n--- Async Stream Response ---\")\n",
        "try:\n",
        "    async for chunk in chat.astream(messages):\n",
        "        print(chunk.content, end=\"\", flush=True)\n",
        "    print()  # Newline\n",
        "except Exception as e:\n",
        "    print(\n",
        "        f\"\\nError streaming Chat Model asynchronously: {e}. Ensure endpoint handler supports streaming output format.\\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 链式调用\n\n聊天模型可以与 LangChain Expression Language (LCEL) 链无缝集成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | chat | parser\n",
        "\n",
        "try:\n",
        "    chain_response = chain.invoke(\n",
        "        {\"input\": \"Explain the concept of serverless computing in simple terms.\"}\n",
        "    )\n",
        "    print(\"--- Chain Response ---\")\n",
        "    print(chain_response)\n",
        "except Exception as e:\n",
        "    print(f\"Error running chain: {e}\")\n",
        "\n",
        "\n",
        "# Async chain\n",
        "try:\n",
        "    async_chain_response = await chain.ainvoke(\n",
        "        {\"input\": \"What are the benefits of using RunPod for AI/ML workloads?\"}\n",
        "    )\n",
        "    print(\"--- Async Chain Response ---\")\n",
        "    print(async_chain_response)\n",
        "except Exception as e:\n",
        "    print(f\"Error running async chain: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 模型功能（取决于端点）\n\n高级功能的可用性**严重**依赖于您 RunPod 端点处理程序的具体实现。`ChatRunPod` 集成提供了基本框架，但处理程序必须支持底层功能。\n\n| 功能                                                       | 集成支持 | 是否取决于端点？ | 备注                                                                                                                                                                       |\n| :--------------------------------------------------------- | :------: | :--------------: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [工具调用](/docs/how_to/tool_calling)                    |    ❌    |        ✅        | 要求处理程序处理工具定义并返回工具调用（例如，OpenAI 格式）。集成需要解析逻辑。                                                                                             |\n| [结构化输出](/docs/how_to/structured_output)              |    ❌    |        ✅        | 要求处理程序支持强制结构化输出（JSON 模式、函数调用）。集成需要解析逻辑。                                                                                                 |\n| JSON 模式                                                  |    ❌    |        ✅        | 要求处理程序接受 `json_mode` 参数（或类似参数）并保证 JSON 输出。                                                                                                         |\n| [图像输入](/docs/how_to/multimodal_inputs)                |    ❌    |        ✅        | 要求多模态处理程序接受图像数据（例如，base64）。集成不支持多模态消息。                                                                                                    |\n| 音频输入                                                   |    ❌    |        ✅        | 要求处理程序接受音频数据。集成不支持音频消息。                                                                                                                          |\n| 视频输入                                                   |    ❌    |        ✅        | 要求处理程序接受视频数据。集成不支持视频消息。                                                                                                                          |\n| [Token 流式输出](/docs/how_to/chat_streaming)             |  ✅ (模拟) |        ✅        | 轮询 `/stream`。要求处理程序在状态响应中填充 `stream` 列表，其中包含 token 块（例如，`[{\"output\": \"token\"}]`）。真正的低延迟流式输出并非内置。                           |\n| 原生异步                                                   |    ✅    |        ✅        | 已实现核心的 `ainvoke`/`astream`。依赖于端点处理程序的性能。                                                                                                             |\n| [Token 使用量](/docs/how_to/chat_token_usage_tracking)    |    ❌    |        ✅        | 要求处理程序在最终响应中返回 `prompt_tokens`、`completion_tokens`。集成目前不解析此项。                                                                                 |\n| [Logprobs](/docs/how_to/logprobs)                          |    ❌    |        ✅        | 要求处理程序返回 log probabilities。集成目前不解析此项。                                                                                                                 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**关键要点：** 如果端点遵循 RunPod API 基本约定，则标准的聊天调用和模拟流式传输将正常工作。高级功能需要特定的处理程序实现，并可能需要扩展或自定义此集成包。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API 参考\n\n有关 `ChatRunPod` 类、参数和方法的详细文档，请参阅源代码或生成的 API 参考（如果可用）。\n\n源代码链接：[https://github.com/runpod/langchain-runpod/blob/main/langchain_runpod/chat_models.py](https://github.com/runpod/langchain-runpod/blob/main/langchain_runpod/chat_models.py)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}