{
  "cells": [
    {
      "cell_type": "raw",
      "id": "63ee3f93",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_position: 0\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9316da0d",
      "metadata": {},
      "source": [
        "# 使用聊天模型和提示模板构建简单的 LLM 应用程序\n\n在本快速入门指南中，我们将展示如何使用 LangChain 构建一个简单的 LLM 应用程序。此应用程序会将英文文本翻译成另一种语言。这是一个相对简单的 LLM 应用程序——它只是一次 LLM 调用加上一些提示。尽管如此，这仍然是开始使用 LangChain 的绝佳方式——许多功能都可以通过一些提示和一次 LLM 调用来构建！\n\n阅读本教程后，您将对以下内容有一个高层次的概述：\n\n- 使用 [语言模型](/docs/concepts/chat_models)\n\n- 使用 [提示模板](/docs/concepts/prompt_templates)\n\n- 使用 [LangSmith](https://docs.smith.langchain.com/) 调试和跟踪您的应用程序\n\n让我们开始吧！\n\n## 设置\n\n### Jupyter Notebook\n\n本教程及其他教程可能在 [Jupyter notebooks](https://jupyter.org/) 中运行最为方便。在交互式环境中学习指南是更好地理解它们的好方法。有关如何安装的说明，请参见 [此处](https://jupyter.org/install)。\n\n### 安装\n\n要安装 LangChain，请运行：\n\n<!-- HIDE_IN_NB\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\nimport CodeBlock from \"@theme/CodeBlock\";\n\n<Tabs>\n  <TabItem value=\"pip\" label=\"Pip\" default>\n    <CodeBlock language=\"bash\">pip install langchain</CodeBlock>\n  </TabItem>\n  <TabItem value=\"conda\" label=\"Conda\">\n    <CodeBlock language=\"bash\">conda install langchain -c conda-forge</CodeBlock>\n  </TabItem>\n</Tabs>\nHIDE_IN_NB -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "86874822",
      "metadata": {},
      "outputs": [],
      "source": [
        "# | output: false\n",
        "\n",
        "# %pip install langchain\n",
        "# OR\n",
        "# %conda install langchain -c conda-forge"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a546a5bc",
      "metadata": {},
      "source": [
        "更多详情，请参阅我们的 [安装指南](/docs/how_to/installation)。\n\n### LangSmith\n\n你用 LangChain 构建的许多应用程序将包含多个步骤和多次对 LLM 的调用。\n随着这些应用程序变得越来越复杂，能够检查你的链或代理内部具体发生了什么至关重要。\n做到这一点最好的方法是使用 [LangSmith](https://smith.langchain.com)。\n\n在上面的链接处注册后，请确保设置你的环境变量以开始记录跟踪：\n\n```shell\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\nexport LANGSMITH_PROJECT=\"default\" # 或任何其他项目名称\n```\n\n或者，如果在笔记本中，你可以使用以下方法设置它们：\n\n```python\nimport getpass\nimport os\n\ntry:\n    # 从 .env 文件加载环境变量 (需要 `python-dotenv`)\n    from dotenv import load_dotenv\n\n    load_dotenv()\nexcept ImportError:\n    pass\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nif \"LANGSMITH_API_KEY\" not in os.environ:\n    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n        prompt=\"请输入你的 LangSmith API 密钥 (可选): \"\n    )\nif \"LANGSMITH_PROJECT\" not in os.environ:\n    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n        prompt='请输入你的 LangSmith 项目名称 (默认为 \"default\"): '\n    )\n    if not os.environ.get(\"LANGSMITH_PROJECT\"):\n        os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5558ca9",
      "metadata": {},
      "source": [
        "## 使用语言模型\n\n首先，让我们学习如何单独使用语言模型。LangChain 支持许多不同的语言模型，您可以互换使用它们。有关开始使用特定模型的详细信息，请参阅 [支持的集成](/docs/integrations/chat/)。\n\n<!-- HIDE_IN_NB>\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\n\n<ChatModelTabs overrideParams={{openai: {model: \"gpt-4o-mini\"}}} />\nHIDE_IN_NB -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e4b41234",
      "metadata": {},
      "outputs": [],
      "source": [
        "# | output: false\n",
        "# | echo: false\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca5642ff",
      "metadata": {},
      "source": [
        "我们首先直接使用模型。`[ChatModels](/docs/concepts/chat_models)` 是 LangChain `[Runnables](/docs/concepts/runnables/)` 的实例，这意味着它们公开了一个与它们交互的标准接口。要简单地调用模型，我们可以向 `.invoke` 方法传递一个 [messages](/docs/concepts/messages/) 列表。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1b2481f0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"Translate the following from English into Italian\"),\n",
        "    HumanMessage(\"hi!\"),\n",
        "]\n",
        "\n",
        "model.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f83373db",
      "metadata": {},
      "source": [
        ":::tip\n\n如果已启用 LangSmith，我们可以看到此运行已记录到 LangSmith，并且可以看到 [LangSmith 追踪](https://smith.langchain.com/public/88baa0b2-7c1a-4d09-ba30-a47985dde2ea/r)。LangSmith 追踪报告了 [token](/docs/concepts/tokens/) 使用情况、延迟、[标准模型参数](/docs/concepts/chat_models/#standard-parameters)（如 temperature）以及其他信息。\n\n:::\n\n请注意，ChatModels 接收 [message](/docs/concepts/messages/) 对象作为输入，并生成 message 对象作为输出。除了文本内容，message 对象还传递对话的 [角色](/docs/concepts/messages/#role) 并包含重要数据，例如 [工具调用](/docs/concepts/tool_calling/) 和 token 使用计数。\n\nLangChain 还通过字符串或 [OpenAI 格式](/docs/concepts/messages/#openai-format) 支持聊天模型输入。以下内容是等效的：\n\n```python\nmodel.invoke(\"Hello\")\n\nmodel.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n\nmodel.invoke([HumanMessage(\"Hello\")])\n```\n\n### 串流\n\n因为聊天模型是 [Runnables](/docs/concepts/runnables/)，所以它们公开了一个标准接口，其中包括异步和串流调用模式。这使我们能够从聊天模型串流单个 token："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0abb0863-bee7-448d-b013-79d8db01e330",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|C|iao|!||"
          ]
        }
      ],
      "source": [
        "for token in model.stream(messages):\n",
        "    print(token.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5963141-468c-4570-8f2e-5f7cfb6eb3db",
      "metadata": {},
      "source": [
        "你可以在[此指南](/docs/how_to/chat_streaming/)中找到有关流式聊天模型输出的更多详细信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab8da31",
      "metadata": {},
      "source": [
        "## 提示词模板\n\n目前，我们将消息列表直接传递给语言模型。这个消息列表从何而来？通常，它是由用户输入和应用程序逻辑组合而成的。应用程序逻辑通常会接收原始用户输入，并将其转换为一个已准备好传递给语言模型的消息列表。常见的转换包括添加系统消息或使用用户输入格式化模板。\n\n[提示词模板](/docs/concepts/prompt_templates/) 是 LangChain 中的一个概念，旨在协助进行此转换。它们接收原始用户输入，并返回数据（一个提示词），该数据已准备好传递给语言模型。\n\n我们在这里创建一个提示词模板。它将接收两个用户变量：\n\n- `language`：要将文本翻译成的语言\n- `text`：要翻译的文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3e73cc20",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following from English into {language}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e876c2a",
      "metadata": {},
      "source": [
        "请注意，`ChatPromptTemplate` 支持在单个模板中使用多种[消息角色](/docs/concepts/messages/#role)。我们将 `language` 参数格式化为系统消息，并将用户 `text` 格式化为用户消息。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9711ba6",
      "metadata": {},
      "source": [
        "此提示模板的输入是一个字典。我们可以单独玩玩这个提示模板，看看它自己有什么作用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f781b3cb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a49ba9e",
      "metadata": {},
      "source": [
        "我们可以看到它返回了一个 `ChatPromptValue`，其中包含两个消息。如果要直接访问这些消息，我们可以这样做："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2159b619",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt.to_messages()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47e70ee6-f0e0-4ae0-a290-002799ebf828",
      "metadata": {},
      "source": [
        "最后，我们可以在格式化后的提示上调用聊天模型："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3a509d8c-e122-4641-b9ee-91bc23aa155a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ciao!\n"
          ]
        }
      ],
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7f0bf25-6efb-4853-9a8f-242f2855c84a",
      "metadata": {},
      "source": [
        ":::tip\nMessage `content` 可以同时包含文本和具有附加结构的 [content blocks](/docs/concepts/messages/#aimessage)。更多信息请参阅[本指南](/docs/how_to/output_parser_string/)。\n:::\n\n如果我们查看 [LangSmith trace](https://smith.langchain.com/public/3ccc2d5e-2869-467b-95d6-33a577df99a2/r)，我们可以确切地看到聊天模型接收到的提示，以及 [token](/docs/concepts/tokens/) 使用信息、延迟、[standard model parameters](/docs/concepts/chat_models/#standard-parameters)（如 temperature）和其他信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "befdb168",
      "metadata": {},
      "source": [
        "## 结论\n\n就是这样！在本教程中，您学习了如何创建您的第一个简单的 LLM 应用程序。您学会了如何使用语言模型，如何创建提示模板，以及如何为使用 LangSmith 构建的应用程序获得出色的可观测性。\n\n这仅仅是您要成为一名熟练的 AI 工程师需要学习内容的冰山一角。幸运的是，我们还有许多其他资源！\n\n如需进一步阅读 LangChain 的核心概念，请参阅我们详细的[概念指南](/docs/concepts)。\n\n如果您对这些概念有更具体的问题，请查看以下部分的操作指南：\n\n- [聊天模型](/docs/how_to/#chat-models)\n- [提示模板](/docs/how_to/#prompt-templates)\n\n以及 LangSmith 文档：\n\n- [LangSmith](https://docs.smith.langchain.com)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}