{
  "cells": [
    {
      "cell_type": "raw",
      "id": "df29b30a-fd27-4e08-8269-870df5631f9e",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_position: 4\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d28530a6-ddfd-49c0-85dc-b723551f6614",
      "metadata": {},
      "source": [
        "# 构建一个提取链\n\n在本教程中，我们将使用 [聊天模型](/docs/concepts/chat_models) 的 [工具调用](/docs/concepts/tool_calling) 功能从非结构化文本中提取结构化信息。我们还将演示如何在此场景中使用 [少样本提示](/docs/concepts/few_shot_prompting/) 来提高性能。\n\n:::important\n此教程需要 `langchain-core>=0.3.20`，并且仅适用于支持 **工具调用** 的模型。\n:::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4412def2-38e3-4bd0-bbf0-fb09ff9e5985",
      "metadata": {},
      "source": [
        "## 设置\n\n### Jupyter Notebook\n\n本教程以及其他教程，也许在 [Jupyter notebooks](https://jupyter.org/) 中运行是最方便的。在交互式环境中学习指南是更好地理解它们的好方法。有关安装说明，请参见 [此处](https://jupyter.org/install)。\n\n### 安装\n\n要安装 LangChain，请运行：\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\nimport CodeBlock from \"@theme/CodeBlock\";\n\n<Tabs>\n  <TabItem value=\"pip\" label=\"Pip\" default>\n    <CodeBlock language=\"bash\">pip install --upgrade langchain-core</CodeBlock>\n  </TabItem>\n  <TabItem value=\"conda\" label=\"Conda\">\n    <CodeBlock language=\"bash\">conda install langchain-core -c conda-forge</CodeBlock>\n  </TabItem>\n</Tabs>\n\n\n\n有关更多详细信息，请参阅我们的 [安装指南](/docs/how_to/installation)。\n\n### LangSmith\n\n您使用 LangChain 构建的许多应用程序将包含多个步骤和对 LLM 的多次调用。\n随着这些应用程序变得越来越复杂，能够检查链或代理内部究竟发生了什么就变得至关重要。\n做到这一点最好方法是使用 [LangSmith](https://smith.langchain.com)。\n\n在上面的链接注册后，请确保设置您的环境变量以开始记录跟踪：\n\n```shell\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n或者，如果您在 notebook 中，可以使用以下方式设置：\n\n```python\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d6b970-2ea3-4192-951e-21237212b359",
      "metadata": {},
      "source": [
        "## Schema\n\n首先，我们需要描述我们想从文本中提取哪些信息。\n\n我们将使用 Pydantic 来定义一个示例 Schema，用于提取个人信息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c141084c-fb94-4093-8d6a-81175d688e40",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    # ^ Doc-string for the entity Person.\n",
        "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
        "    # and it can help to improve extraction results.\n",
        "\n",
        "    # Note that:\n",
        "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
        "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
        "    # Having a good description can help improve extraction results.\n",
        "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
        "    hair_color: Optional[str] = Field(\n",
        "        default=None, description=\"The color of the person's hair if known\"\n",
        "    )\n",
        "    height_in_meters: Optional[str] = Field(\n",
        "        default=None, description=\"Height measured in meters\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f248dd54-e36d-435a-b154-394ab4ed6792",
      "metadata": {},
      "source": [
        "定义 schema 时，有两个最佳实践：\n\n1.  记录 **属性** 和 **schema** 本身：此信息将发送给 LLM，用于提高信息提取的质量。\n2.  不要强迫 LLM 编造信息！我们在上面使用了 `Optional` 来定义属性，允许 LLM 在不知道答案时输出 `None`。\n\n:::important\n为达到最佳性能，请务必充分记录 schema，并确保模型在文本中没有可提取的信息时，不会被强迫返回结果。\n:::\n\n## Extractor\n\n让我们使用上面定义的 schema 来创建一个信息提取器。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5e490f6-35ad-455e-8ae4-2bae021583ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Define a custom prompt to provide instructions and any additional context.\n",
        "# 1) You can add examples into the prompt template to improve extraction quality\n",
        "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
        "#    about the document from which the text was extracted.)\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert extraction algorithm. \"\n",
        "            \"Only extract relevant information from the text. \"\n",
        "            \"If you do not know the value of an attribute asked to extract, \"\n",
        "            \"return null for the attribute's value.\",\n",
        "        ),\n",
        "        # Please see the how-to about improving performance with\n",
        "        # reference examples.\n",
        "        # MessagesPlaceholder('examples'),\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "832bf6a1-8e0c-4b6a-aa37-12fe9c42a6d9",
      "metadata": {},
      "source": [
        "我们需要使用一个支持函数/工具调用的模型。\n\n请参阅[文档](/docs/concepts/tool_calling)了解所有可与此 API 结合使用的模型。\n\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\n\n<ChatModelTabs customVarName=\"llm\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "77c1311c-5252-41d6-83e6-fdb40b172e47",
      "metadata": {},
      "outputs": [],
      "source": [
        "# | output: false\n",
        "# | echo: false\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "04d846a6-d5cb-4009-ac19-61e3aac0177e",
      "metadata": {},
      "outputs": [],
      "source": [
        "structured_llm = llm.with_structured_output(schema=Person)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23582c0b-00ed-403f-a10e-3aeabf921f12",
      "metadata": {},
      "source": [
        "让我们来测试一下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dd42a935-022f-4860-b9e0-84268f55b22a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Alan Smith is 6 feet tall and has blond hair.\"\n",
        "prompt = prompt_template.invoke({\"text\": text})\n",
        "structured_llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd1c493d-f9dc-4236-8da9-50f6919f5710",
      "metadata": {},
      "source": [
        ":::important\n\n提取是生成式的 🤯\n\nLLM 是生成模型，因此它们可以做一些很酷的事情，例如正确地提取人的身高（米），即使它原本是以英尺提供的！\n:::\n\n我们可以在[这里](https://smith.langchain.com/public/44b69a63-3b3b-47b8-8a6d-61b46533f015/r)查看 LangSmith trace。请注意，[trace 的聊天模型部分](https://smith.langchain.com/public/44b69a63-3b3b-47b8-8a6d-61b46533f015/r/dd1f6305-f1e9-4919-bd8f-339d03a12d01)显示了发送给模型的精确消息序列、调用的工具以及其他元数据。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28c5ef0c-b8d1-4e12-bd0e-e2528de87fcc",
      "metadata": {},
      "source": [
        "## 多个实体\n\n在**大多数情况**下，你应该提取一系列实体，而不是单个实体。\n\n通过将模型相互嵌套，可以使用 pydantic 轻松实现这一点。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "591a0c16-7a17-4883-91ee-0d6d2fdb265c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    # ^ Doc-string for the entity Person.\n",
        "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
        "    # and it can help to improve extraction results.\n",
        "\n",
        "    # Note that:\n",
        "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
        "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
        "    # Having a good description can help improve extraction results.\n",
        "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
        "    hair_color: Optional[str] = Field(\n",
        "        default=None, description=\"The color of the person's hair if known\"\n",
        "    )\n",
        "    height_in_meters: Optional[str] = Field(\n",
        "        default=None, description=\"Height measured in meters\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Data(BaseModel):\n",
        "    \"\"\"Extracted data about people.\"\"\"\n",
        "\n",
        "    # Creates a model so that we can extract multiple entities.\n",
        "    people: List[Person]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f5cda33-fd7b-481e-956a-703f45e40e1d",
      "metadata": {},
      "source": [
        ":::important\n提取结果可能不尽完美。请继续阅读，了解如何使用 **Reference Examples** 来提高提取质量，并参阅我们的提取 [操作指南](/docs/how_to/#extraction) 以获取更多详细信息。\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "83ecf0db-757b-4ae3-a9d2-eb1c9f6b2631",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "structured_llm = llm.with_structured_output(schema=Data)\n",
        "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
        "prompt = prompt_template.invoke({\"text\": text})\n",
        "structured_llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fba1d770-bf4d-4de4-9e4f-7384872ef0dc",
      "metadata": {},
      "source": [
        ":::tip\n当 Schema 能够提取**多个实体**时，它也允许模型在文本中没有相关信息的情况下提取**零个实体**，只需提供一个空列表即可。\n\n这通常是一件**好事**！它允许我们在一个实体上指定**必需**的属性，而不必强制模型检测到该实体。\n:::\n\n我们可以在这里查看 LangSmith 的追踪记录：[here](https://smith.langchain.com/public/7173764d-5e76-45fe-8496-84460bd9cdef/r)。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c590f366-050a-43d4-8c78-acf84ccfbf9b",
      "metadata": {},
      "source": [
        "## 参考示例\n\nLLM 应用的行为可以通过 [少样本提示 (few-shot prompting)](/docs/concepts/few_shot_prompting/) 进行引导。对于 [聊天模型 (chat models)](/docs/concepts/chat_models/) 来说，这可以表现为一系列的输入和响应消息对，以展示期望的行为。\n\n例如，我们可以通过交替出现的 `user` 和 `assistant` [消息 (messages)](/docs/concepts/messages/#role) 来传达一个符号的含义："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0bb138d7-116e-4542-aa5f-bebf0c301ec6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"2 🦜 2\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"4\"},\n",
        "    {\"role\": \"user\", \"content\": \"2 🦜 3\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"5\"},\n",
        "    {\"role\": \"user\", \"content\": \"3 🦜 4\"},\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5691d07-e2b8-4ab3-a943-9b0b503e2549",
      "metadata": {},
      "source": [
        "[结构化输出](/docs/concepts/structured_outputs/) 通常会在底层使用 [工具调用](/docs/concepts/tool_calling/)。这通常涉及生成包含工具调用的 [AI 消息](/docs/concepts/messages/#aimessage)，以及包含工具调用结果的 [工具消息](/docs/concepts/messages/#toolmessage)。在这种情况下，消息序列应该是什么样的？\n\n不同的 [聊天模型提供商](/docs/integrations/chat/) 对有效的消息序列有不同的要求。有些接受如下（重复的）消息序列：\n\n- 用户消息\n- 包含工具调用的 AI 消息\n- 包含结果的工具消息\n\n另一些则需要一个最终的 AI 消息来包含某种响应。\n\nLangChain 包含一个实用函数 [tool_example_to_messages](https://python.langchain.com/api_reference/core/utils/langchain_core.utils.function_calling.tool_example_to_messages.html)，它能为大多数模型提供商生成有效的序列。该函数通过仅要求 Pydantic 表示法来简化结构化少样本示例的生成。\n\n让我们试一试。我们可以将输入字符串对和期望的 Pydantic 对象转换为一系列消息，提供给聊天模型。在底层，LangChain 会将工具调用格式化为每个提供商所需 的格式。\n\n请注意：此版本的 `tool_example_to_messages` 需要 `langchain-core>=0.3.20`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c604e476-a2be-4eda-b128-71399e280732",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import tool_example_to_messages\n",
        "\n",
        "examples = [\n",
        "    (\n",
        "        \"The ocean is vast and blue. It's more than 20,000 feet deep.\",\n",
        "        Data(people=[]),\n",
        "    ),\n",
        "    (\n",
        "        \"Fiona traveled far from France to Spain.\",\n",
        "        Data(people=[Person(name=\"Fiona\", height_in_meters=None, hair_color=None)]),\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "messages = []\n",
        "\n",
        "for txt, tool_call in examples:\n",
        "    if tool_call.people:\n",
        "        # This final message is optional for some providers\n",
        "        ai_response = \"Detected people.\"\n",
        "    else:\n",
        "        ai_response = \"Detected no people.\"\n",
        "    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beecc7a6-e423-4ca1-82b7-c2a751362fd6",
      "metadata": {},
      "source": [
        "检查结果时，我们会看到这两个示例对生成了八条消息："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "628f67dd-aee0-4200-ac38-24a9fb16f1d1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "The ocean is vast and blue. It's more than 20,000 feet deep.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Data (d8f2e054-7fb9-417f-b28f-0447a775b2c3)\n",
            " Call ID: d8f2e054-7fb9-417f-b28f-0447a775b2c3\n",
            "  Args:\n",
            "    people: []\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "\n",
            "You have correctly called this tool.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Detected no people.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Fiona traveled far from France to Spain.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Data (0178939e-a4b1-4d2a-a93e-b87f665cdfd6)\n",
            " Call ID: 0178939e-a4b1-4d2a-a93e-b87f665cdfd6\n",
            "  Args:\n",
            "    people: [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "\n",
            "You have correctly called this tool.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Detected people.\n"
          ]
        }
      ],
      "source": [
        "for message in messages:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc8846f0-8bd1-48e1-bc4d-a62fbfa6a9f4",
      "metadata": {},
      "source": [
        "让我们来比较一下是否包含这些消息时的性能。例如，让我们传递一个我们不打算提取任何人的消息："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6b73d4e2-d18d-4d47-89ec-99b5eb6b234f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(people=[Person(name='Earth', hair_color='None', height_in_meters='0.00')])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message_no_extraction = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"The solar system is large, but earth has only 1 moon.\",\n",
        "}\n",
        "\n",
        "structured_llm = llm.with_structured_output(schema=Data)\n",
        "structured_llm.invoke([message_no_extraction])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "350e1298-14f1-48e4-b11c-534af643e3a6",
      "metadata": {},
      "source": [
        "在此示例中，模型可能会错误地生成人物记录。\n\n由于我们的少样本示例包含“负面”示例，因此我们鼓励模型在这种情况下表现正确："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "eb1b3a99-4750-45bc-ad28-5d12751ed9f8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(people=[])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "structured_llm.invoke(messages + [message_no_extraction])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d1ae320-14bc-45ee-aeeb-8a986f3e6808",
      "metadata": {},
      "source": [
        ":::tip\n\n[LangSmith](https://smith.langchain.com/public/b3433f57-7905-4430-923c-fed214525bf1/r) 的运行追踪记录显示了发送到聊天模型的准确消息顺序、生成的工具调用、延迟、Token 数量以及其他元数据。\n\n:::\n\n有关提取工作流和参考示例的更多详细信息，请参阅[本指南](/docs/how_to/extraction_examples/)，其中包括如何整合 Prompt 模板和自定义示例消息的生成。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f07a7455-7de6-4a6f-9772-0477ef65e3dc",
      "metadata": {},
      "source": [
        "## 后续步骤\n\n既然您已经了解了 LangChain 的提取基础知识，就可以继续阅读其余的操作指南了：\n\n- [添加示例](/docs/how_to/extraction_examples)：关于使用**参考示例**来提高性能的更多详细信息 。\n- [处理长文本](/docs/how_to/extraction_long_text)：如果文本不适合 LLM 的上下文窗口，您应该怎么做？\n- [使用解析方法](/docs/how_to/extraction_parse)：对于不支持**工具/函数调用**的模型，使用基于提示的方法进行提取。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3deb47ba",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}