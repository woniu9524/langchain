{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c298a5c9-b9af-481d-9eba-cbd65f987a8a",
      "metadata": {},
      "source": [
        "# 如何将 BaseChatMessageHistory 与 LangGraph 结合使用\n\n:::info 先决条件\n\n本指南假设您熟悉以下概念：\n* [聊天历史](/docs/concepts/chat_history)\n* [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)\n* [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/)\n* [记忆](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#memory)\n:::\n\n我们建议新的 LangChain 应用程序利用 [内置的 LangGraph 持久化](https://langchain-ai.github.io/langgraph/concepts/persistence/) 来实现记忆。\n\n在某些情况下，用户可能需要继续使用现有的聊天消息历史持久化解决方案。\n\n在这里，我们将展示如何将 [LangChain 聊天消息历史记录](https://python.langchain.com/docs/integrations/memory/)（[BaseChatMessageHistory](https://python.langchain.com/api_reference/core/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html) 的实现）与 LangGraph 结合使用。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "548bc988-167b-43f1-860a-d247e28b2b42",
      "metadata": {},
      "source": [
        "## 设置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6cbfd2ab-7537-4269-8249-646fa89bf016",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --upgrade --quiet langchain-anthropic langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0694febf-dfa8-46ef-babc-f8b16b5a2926",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c5e08659-b68c-48f2-8b33-e79b0c6999e1",
      "metadata": {},
      "source": [
        "## ChatMessageHistory\n\n消息历史需要通过会话 ID 或用户 ID 和会话 ID 的二元组进行参数化。\n\n许多 [LangChain 聊天消息历史](https://python.langchain.com/docs/integrations/memory/) 都包含 `session_id` 或某个 `namespace`，以便跟踪不同的对话。请参考具体的实现来检查它是如何参数化的。\n\n内置的 `InMemoryChatMessageHistory` 不包含此类参数化，因此我们将创建一个字典来跟踪消息历史。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28049308-2543-48e6-90d0-37a88951a637",
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "chats_by_session_id = {}\n",
        "\n",
        "\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    chat_history = chats_by_session_id.get(session_id)\n",
        "    if chat_history is None:\n",
        "        chat_history = InMemoryChatMessageHistory()\n",
        "        chats_by_session_id[session_id] = chat_history\n",
        "    return chat_history"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "94c53ce3-4212-41e6-8ad3-f0ab5df6130f",
      "metadata": {},
      "source": [
        "## 配合 LangGraph 使用\n\n接下来，我们将使用 LangGraph 来设置一个基础聊天机器人。如果你不熟悉 LangGraph，请查阅以下[快速入门教程](https://langchain-ai.github.io/langgraph/tutorials/introduction/)。\n\n我们将为聊天模型创建一个[LangGraph 节点](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes)，并手动管理对话历史，同时考虑作为 RunnableConfig 的一部分传入的对话 ID。\n\n对话 ID 可以作为 RunnableConfig 的一部分（就像我们在这里所做的那样）传入，也可以作为[图状态](https://langchain-ai.github.io/langgraph/concepts/low_level/#state)的一部分传入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a6633dd2-2d6a-4121-b087-4907c9f588ca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi! I'm bob\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Bob! It's nice to meet you. I'm Claude, an AI assistant created by Anthropic. How are you doing today?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what was my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You introduced yourself as Bob when you said \"hi! I'm bob\".\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "# Define a new graph\n",
        "builder = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define a chat model\n",
        "model = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState, config: RunnableConfig) -> list[BaseMessage]:\n",
        "    # Make sure that config is populated with the session id\n",
        "    if \"configurable\" not in config or \"session_id\" not in config[\"configurable\"]:\n",
        "        raise ValueError(\n",
        "            \"Make sure that the config includes the following information: {'configurable': {'session_id': 'some_value'}}\"\n",
        "        )\n",
        "    # Fetch the history of messages and append to it any new messages.\n",
        "    # highlight-start\n",
        "    chat_history = get_chat_history(config[\"configurable\"][\"session_id\"])\n",
        "    messages = list(chat_history.messages) + state[\"messages\"]\n",
        "    # highlight-end\n",
        "    ai_message = model.invoke(messages)\n",
        "    # Finally, update the chat message history to include\n",
        "    # the new input message from the user together with the\n",
        "    # response from the model.\n",
        "    # highlight-next-line\n",
        "    chat_history.add_messages(state[\"messages\"] + [ai_message])\n",
        "    return {\"messages\": ai_message}\n",
        "\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "builder.add_edge(START, \"model\")\n",
        "builder.add_node(\"model\", call_model)\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "# Here, we'll create a unique session ID to identify the conversation\n",
        "session_id = uuid.uuid4()\n",
        "config = {\"configurable\": {\"session_id\": session_id}}\n",
        "\n",
        "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
        "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "# Here, let's confirm that the AI remembers our name!\n",
        "input_message = HumanMessage(content=\"what was my name?\")\n",
        "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0766af-a3b3-4293-b253-3a10f365ab5d",
      "metadata": {},
      "source": [
        ":::tip\n\n如果使用 langgraph >= 0.2.28，这也支持逐个 token 流式传输 LLM 内容。\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "044b63dd-fb15-4a03-89c5-aaaf7346ea76",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You| sai|d your| name was Bob.|"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessageChunk\n",
        "\n",
        "first = True\n",
        "\n",
        "for msg, metadata in graph.stream(\n",
        "    {\"messages\": input_message}, config, stream_mode=\"messages\"\n",
        "):\n",
        "    if msg.content and not isinstance(msg, HumanMessage):\n",
        "        print(msg.content, end=\"|\", flush=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "da0536dd-9a0b-49e3-b0b6-e8c7abf3b1f9",
      "metadata": {},
      "source": [
        "## 使用 RunnableWithMessageHistory\n\n此操作指南直接使用了 `BaseChatMessageHistory` 的 `messages` 和 `add_messages` 接口。\n\n另外，你可以使用 [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)，因为 [LCEL](/docs/concepts/lcel/) 可以在任何 [LangGraph 节点](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes) 中使用。\n\n要做到这一点，请替换以下代码：\n\n```python\ndef call_model(state: MessagesState, config: RunnableConfig) -> list[BaseMessage]:\n    # highlight-start\n    # 确保 config 已填充了 session id\n    if \"configurable\" not in config or \"session_id\" not in config[\"configurable\"]:\n        raise ValueError(\n            \"您需要确保 config 包含以下信息：{'configurable': {'session_id': 'some_value'}}\"\n        )\n    # 获取消息历史，并将任何新消息追加到其中。\n    chat_history = get_chat_history(config[\"configurable\"][\"session_id\"])\n    messages = list(chat_history.messages) + state[\"messages\"]\n    ai_message = model.invoke(messages)\n    # 最后，更新聊天消息历史，包含\n    # 用户的新输入消息以及\n    # 模型的响应。\n    chat_history.add_messages(state[\"messages\"] + [ai_message])\n    # hilight-end\n    return {\"messages\": ai_message}\n```\n\n替换为在你当前应用程序中定义的 `RunnableWithMessageHistory` 的相应实例。\n\n```python\nrunnable = RunnableWithMessageHistory(...) # 来自现有代码\n\ndef call_model(state: MessagesState, config: RunnableConfig) -> list[BaseMessage]:\n    # RunnableWithMessageHistory 负责读取消息历史\n    # 并用新的人类消息和 AI 响应进行更新。\n    ai_message = runnable.invoke(state['messages'], config)\n    return {\n        \"messages\": ai_message\n    }\n```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}