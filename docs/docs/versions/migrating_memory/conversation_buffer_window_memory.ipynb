{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ce8457ed-c0b1-4a74-abbd-9d3d2211270f",
      "metadata": {},
      "source": [
        "# 从 ConversationBufferWindowMemory 或 ConversationTokenBufferMemory 迁移\n\n如果您正尝试从以下旧内存类迁移，请遵循本指南：\n\n| 内存类型                      | 描述                                                                                                                                                       |\n|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `ConversationBufferWindowMemory` | 保留对话的最后 `n` 条消息。当消息数超过 `n` 条时，将丢弃最旧的消息。                                                                                                               |\n| `ConversationTokenBufferMemory`  | 仅保留对话中最近的消息，但前提是对话中的总令牌数不超过某个限制。 |\n\n`ConversationBufferWindowMemory` 和 `ConversationTokenBufferMemory` 对原始对话历史进行额外的处理，以将对话历史修剪到适合聊天模型上下文窗口的大小。\n\n此处理功能可以使用 LangChain 内置的 [trim_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) 函数来完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79935247-acc7-4a05-a387-5d72c9c8c8cb",
      "metadata": {},
      "source": [
        ":::important\n\n我们将首先探讨一种将处理逻辑应用于整个对话历史的简单方法。\n\n虽然这种方法易于实现，但有一个缺点：随着对话的进行，延迟也会随之增加，因为在每个回合，逻辑都会重新应用于对话中的所有先前交流。\n\n更高级的策略侧重于增量更新对话历史以避免重复处理。\n\n例如，langgraph [关于对话摘要的操作指南](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/) 演示了如何在丢弃旧消息的同时维护对话的运行摘要，从而确保在后续回合中不会重复处理它们。\n:::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d07f9459-9fb6-4942-99c9-64558aedd7d4",
      "metadata": {},
      "source": [
        "## 设置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b99b47ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --upgrade --quiet langchain-openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7127478f-4413-48be-bfec-d0cd91b8cf70",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a7bc93-21a9-44c8-842e-9cc82f1ada7c",
      "metadata": {},
      "source": [
        "## LLMChain / Conversation Chain 的旧用法\n\n<details open>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "371616e1-ca41-4a57-99e0-5fbf7d63f2ad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 'Nice to meet you, Bob! How can I assist you today?', 'chat_history': []}\n",
            "{'text': 'Your name is Bob. How can I assist you further, Bob?', 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})]}\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        ")\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# highlight-start\n",
        "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "# highlight-end\n",
        "\n",
        "legacy_chain = LLMChain(\n",
        "    llm=ChatOpenAI(),\n",
        "    prompt=prompt,\n",
        "    # highlight-next-line\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "legacy_result = legacy_chain.invoke({\"text\": \"my name is bob\"})\n",
        "print(legacy_result)\n",
        "\n",
        "legacy_result = legacy_chain.invoke({\"text\": \"what was my name\"})\n",
        "print(legacy_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f48cac47-c8b6-444c-8e1b-f7115c0b2d8d",
      "metadata": {},
      "source": [
        "</details>\n\n## 重写 ConversationBufferWindowMemory 逻辑\n\n我们首先创建适当的逻辑来处理对话历史，然后展示如何将其集成到应用程序中。之后，您可以根据特定需求将此基本设置替换为更高级的逻辑。\n\n我们将使用 `trim_messages` 来实现仅保留对话中最后 `n` 条消息的逻辑。当消息数量超过 `n` 时，它将丢弃最旧的消息。\n\n此外，如果存在系统消息，我们也会保留它——当存在时，它将是包含聊天模型指令的对话中的第一条消息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0a92b3f3-0315-46ac-bb28-d07398dd23ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    trim_messages,\n",
        ")\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
        "    HumanMessage(\"i wonder why it's called langchain\"),\n",
        "    AIMessage(\n",
        "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
        "    ),\n",
        "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
        "    AIMessage(\n",
        "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
        "    ),\n",
        "    HumanMessage(\"why is 42 always the answer?\"),\n",
        "    AIMessage(\n",
        "        \"Because it’s the only number that’s constantly right, even when it doesn’t add up!\"\n",
        "    ),\n",
        "    HumanMessage(\"What did the cow say?\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e7ddf8dc-ea27-43e2-8800-9f7c1d4abdc1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "you're a good assistant, you always respond with a joke.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hmmm let me think.\n",
            "\n",
            "Why, he's probably chasing after the last cup of coffee in the office!\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "why is 42 always the answer?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Because it’s the only number that’s constantly right, even when it doesn’t add up!\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What did the cow say?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import trim_messages\n",
        "\n",
        "selected_messages = trim_messages(\n",
        "    messages,\n",
        "    token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n",
        "    max_tokens=5,  # <-- allow up to 5 messages.\n",
        "    strategy=\"last\",\n",
        "    # Most chat models expect that chat history starts with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a SystemMessage followed by a HumanMessage\n",
        "    # start_on=\"human\" makes sure we produce a valid chat history\n",
        "    start_on=\"human\",\n",
        "    # Usually, we want to keep the SystemMessage\n",
        "    # if it's present in the original history.\n",
        "    # The SystemMessage has special instructions for the model.\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        ")\n",
        "\n",
        "for msg in selected_messages:\n",
        "    msg.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18f73819-05e0-41f3-a0e7-a5fd6701d9ef",
      "metadata": {},
      "source": [
        "## 重写 `ConversationTokenBufferMemory` 逻辑\n\n在这里，我们将使用 `trim_messages` 来在对话的总 token 数不超过特定限制的约束下，保留系统消息和对话中最新的消息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6442f74b-2c36-48fd-a3d1-c7c5d18c050f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "you're a good assistant, you always respond with a joke.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "why is 42 always the answer?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Because it’s the only number that’s constantly right, even when it doesn’t add up!\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What did the cow say?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import trim_messages\n",
        "\n",
        "selected_messages = trim_messages(\n",
        "    messages,\n",
        "    # Please see API reference for trim_messages for other ways to specify a token counter.\n",
        "    token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
        "    max_tokens=80,  # <-- token limit\n",
        "    # The start_on is specified\n",
        "    # Most chat models expect that chat history starts with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a SystemMessage followed by a HumanMessage\n",
        "    # start_on=\"human\" makes sure we produce a valid chat history\n",
        "    start_on=\"human\",\n",
        "    # Usually, we want to keep the SystemMessage\n",
        "    # if it's present in the original history.\n",
        "    # The SystemMessage has special instructions for the model.\n",
        "    include_system=True,\n",
        "    strategy=\"last\",\n",
        ")\n",
        "\n",
        "for msg in selected_messages:\n",
        "    msg.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f05d272-2d22-44b7-9fa6-e617a48584b4",
      "metadata": {},
      "source": [
        "## 现代用法，结合 LangGraph\n\n下面的示例展示了如何使用 LangGraph 添加简单的对话预处理逻辑。\n\n:::note\n\n如果你想避免每次都对整个对话历史进行计算，可以遵循[关于对话摘要的指南](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)，该指南演示了\n如何丢弃较早的消息，确保它们在后续回合中不会被重复处理。\n\n:::\n\n<details open>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7d6f79a3-fda7-48fd-9128-bbe4aad84199",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi! I'm bob\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Bob! How can I assist you today?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what was my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob. How can I help you, Bob?\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "\n",
        "from IPython.display import Image, display\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define a chat model\n",
        "model = ChatOpenAI()\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    # highlight-start\n",
        "    selected_messages = trim_messages(\n",
        "        state[\"messages\"],\n",
        "        token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n",
        "        max_tokens=5,  # <-- allow up to 5 messages.\n",
        "        strategy=\"last\",\n",
        "        # Most chat models expect that chat history starts with either:\n",
        "        # (1) a HumanMessage or\n",
        "        # (2) a SystemMessage followed by a HumanMessage\n",
        "        # start_on=\"human\" makes sure we produce a valid chat history\n",
        "        start_on=\"human\",\n",
        "        # Usually, we want to keep the SystemMessage\n",
        "        # if it's present in the original history.\n",
        "        # The SystemMessage has special instructions for the model.\n",
        "        include_system=True,\n",
        "        allow_partial=False,\n",
        "    )\n",
        "\n",
        "    # highlight-end\n",
        "    response = model.invoke(selected_messages)\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "\n",
        "# Adding memory is straight forward in langgraph!\n",
        "# highlight-next-line\n",
        "memory = MemorySaver()\n",
        "\n",
        "app = workflow.compile(\n",
        "    # highlight-next-line\n",
        "    checkpointer=memory\n",
        ")\n",
        "\n",
        "\n",
        "# The thread id is a unique key that identifies\n",
        "# this particular conversation.\n",
        "# We'll just generate a random uuid here.\n",
        "thread_id = uuid.uuid4()\n",
        "# highlight-next-line\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
        "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "# Here, let's confirm that the AI remembers our name!\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "input_message = HumanMessage(content=\"what was my name?\")\n",
        "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84229e2e-a578-4b21-840a-814223406402",
      "metadata": {},
      "source": [
        "</details>\n\n## 使用预构建的 langgraph agent\n\n此示例展示了如何将 Agent Executor 与使用 [create_tool_calling_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) 函数构建的预构建 agent 结合使用。\n\n如果您使用的是 [旧版 LangChain 预构建 agent](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/)，您应该能够用新的 [langgraph 预构建 agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/) 替换该代码。后者利用了 chat model 原生的工具调用功能，并且开箱即用的效果可能更好。\n\n<details open>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f671db87-8f01-453e-81fd-4e603140a512",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi! I'm bob. What is my age?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_user_age (call_jsMvoIFv970DhqqLCJDzPKsp)\n",
            " Call ID: call_jsMvoIFv970DhqqLCJDzPKsp\n",
            "  Args:\n",
            "    name: bob\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_user_age\n",
            "\n",
            "42 years old\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Bob, you are 42 years old.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "do you remember my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Yes, your name is Bob.\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    trim_messages,\n",
        ")\n",
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_user_age(name: str) -> str:\n",
        "    \"\"\"Use this tool to find the user's age.\"\"\"\n",
        "    # This is a placeholder for the actual implementation\n",
        "    if \"bob\" in name.lower():\n",
        "        return \"42 years old\"\n",
        "    return \"41 years old\"\n",
        "\n",
        "\n",
        "memory = MemorySaver()\n",
        "model = ChatOpenAI()\n",
        "\n",
        "\n",
        "# highlight-start\n",
        "def prompt(state) -> list[BaseMessage]:\n",
        "    \"\"\"Given the agent state, return a list of messages for the chat model.\"\"\"\n",
        "    # We're using the message processor defined above.\n",
        "    return trim_messages(\n",
        "        state[\"messages\"],\n",
        "        token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n",
        "        max_tokens=5,  # <-- allow up to 5 messages.\n",
        "        strategy=\"last\",\n",
        "        # Most chat models expect that chat history starts with either:\n",
        "        # (1) a HumanMessage or\n",
        "        # (2) a SystemMessage followed by a HumanMessage\n",
        "        # start_on=\"human\" makes sure we produce a valid chat history\n",
        "        start_on=\"human\",\n",
        "        # Usually, we want to keep the SystemMessage\n",
        "        # if it's present in the original history.\n",
        "        # The SystemMessage has special instructions for the model.\n",
        "        include_system=True,\n",
        "        allow_partial=False,\n",
        "    )\n",
        "\n",
        "\n",
        "# highlight-end\n",
        "\n",
        "app = create_react_agent(\n",
        "    model,\n",
        "    tools=[get_user_age],\n",
        "    checkpointer=memory,\n",
        "    # highlight-next-line\n",
        "    prompt=prompt,\n",
        ")\n",
        "\n",
        "# The thread id is a unique key that identifies\n",
        "# this particular conversation.\n",
        "# We'll just generate a random uuid here.\n",
        "thread_id = uuid.uuid4()\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "# Tell the AI that our name is Bob, and ask it to use a tool to confirm\n",
        "# that it's capable of working like an agent.\n",
        "input_message = HumanMessage(content=\"hi! I'm bob. What is my age?\")\n",
        "\n",
        "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "# Confirm that the chat bot has access to previous conversation\n",
        "# and can respond to the user saying that the user's name is Bob.\n",
        "input_message = HumanMessage(content=\"do you remember my name?\")\n",
        "\n",
        "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f4d16e09-1d90-4153-8576-6d3996cb5a6c",
      "metadata": {},
      "source": [
        "</details>\n\n## LCEL：添加预处理步骤\n\n添加复杂对话管理的最简单方法是在聊天模型前面引入一个预处理步骤，并将完整的对话历史传递给预处理步骤。\n\n这种方法在概念上很简单，并且在许多情况下都适用；例如，如果你使用的是 [RunnableWithMessageHistory](/docs/how_to/message_history/) 而不是包装聊天模型，则用预处理器包装聊天模型。\n\n这种方法明显的缺点是，由于两个原因，随着对话历史的增长，延迟开始增加：\n\n1. 随着对话的变长，可能需要从你用于存储对话历史的任何存储中获取更多数据（如果不是存储在内存中）。\n2. 预处理逻辑最终会执行大量冗余计算，重复对话先前步骤中的计算。\n\n:::caution\n\n如果你想使用聊天模型的工具调用功能，请记住在向其添加历史预处理步骤之前，先将工具绑定到模型！\n\n:::\n\n<details open>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "072046bb-3892-4206-8ae5-025e93110dcc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  what_did_the_cow_say (call_urHTB5CShhcKz37QiVzNBlIS)\n",
            " Call ID: call_urHTB5CShhcKz37QiVzNBlIS\n",
            "  Args:\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    trim_messages,\n",
        ")\n",
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "\n",
        "@tool\n",
        "def what_did_the_cow_say() -> str:\n",
        "    \"\"\"Check to see what the cow said.\"\"\"\n",
        "    return \"foo\"\n",
        "\n",
        "\n",
        "# highlight-start\n",
        "message_processor = trim_messages(  # Returns a Runnable if no messages are provided\n",
        "    token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n",
        "    max_tokens=5,  # <-- allow up to 5 messages.\n",
        "    strategy=\"last\",\n",
        "    # The start_on is specified\n",
        "    # to make sure we do not generate a sequence where\n",
        "    # a ToolMessage that contains the result of a tool invocation\n",
        "    # appears before the AIMessage that requested a tool invocation\n",
        "    # as this will cause some chat models to raise an error.\n",
        "    start_on=(\"human\", \"ai\"),\n",
        "    include_system=True,  # <-- Keep the system message\n",
        "    allow_partial=False,\n",
        ")\n",
        "# highlight-end\n",
        "\n",
        "# Note that we bind tools to the model first!\n",
        "model_with_tools = model.bind_tools([what_did_the_cow_say])\n",
        "\n",
        "# highlight-next-line\n",
        "model_with_preprocessor = message_processor | model_with_tools\n",
        "\n",
        "full_history = [\n",
        "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
        "    HumanMessage(\"i wonder why it's called langchain\"),\n",
        "    AIMessage(\n",
        "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
        "    ),\n",
        "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
        "    AIMessage(\n",
        "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
        "    ),\n",
        "    HumanMessage(\"why is 42 always the answer?\"),\n",
        "    AIMessage(\n",
        "        \"Because it’s the only number that’s constantly right, even when it doesn’t add up!\"\n",
        "    ),\n",
        "    HumanMessage(\"What did the cow say?\"),\n",
        "]\n",
        "\n",
        "\n",
        "# We pass it explicity to the model_with_preprocesor for illustrative purposes.\n",
        "# If you're using `RunnableWithMessageHistory` the history will be automatically\n",
        "# read from the source the you configure.\n",
        "model_with_preprocessor.invoke(full_history).pretty_print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5da7225a-5e94-4f53-bb0d-86b6b528d150",
      "metadata": {},
      "source": [
        "</details>\n\n如果你需要实现更高效的逻辑，并且目前想使用 `RunnableWithMessageHistory`，那么实现的方式是继承 [BaseChatMessageHistory](https://python.langchain.com/api_reference/core/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html) 并为 `add_messages` 定义适当的逻辑（该逻辑不只是简单地追加历史记录，而是重写它）。\n\n除非你有充分的理由来实现此解决方案，否则你应该使用 LangGraph。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2717810",
      "metadata": {},
      "source": [
        "## 后续步骤\n\n探索 LangGraph 中的持久化：\n\n* [LangGraph 快速入门教程](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n* [如何为图添加持久化（“内存”）](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n* [如何管理对话历史](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)\n* [如何为对话历史添加摘要](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)\n\n使用简单的 LCEL 添加持久化（更复杂的用例建议使用 LangGraph）：\n\n* [如何添加消息历史](/docs/how_to/message_history/)\n\n处理消息历史：\n\n* [如何修剪消息](/docs/how_to/trim_messages)\n* [如何过滤消息](/docs/how_to/filter_messages/)\n* [如何合并消息运行](/docs/how_to/merge_message_runs/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4adad0b-3e25-47d9-a8e6-6a9c6c616f14",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}