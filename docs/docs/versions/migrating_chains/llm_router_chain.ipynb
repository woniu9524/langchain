{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "14625d35-efca-41cf-b203-be9f4c375700",
      "metadata": {},
      "source": [
        "# 从 LLMRouterChain 迁移\n\n[`LLMRouterChain`](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.router.llm_router.LLMRouterChain.html) 将输入查询路由到一个或多个目标链——也就是说，给定一个输入查询，它使用 LLM 从目标链列表中选择一个，并将输入传递给选定的链。\n\n`LLMRouterChain` 不支持常见的 [聊天模型](/docs/concepts/chat_models) 功能，例如消息角色和 [工具调用](/docs/concepts/tool_calling)。在底层，`LLMRouterChain` 通过指示 LLM 生成 JSON 格式的文本并解析出目标目的地来路由查询。\n\n可以参考 [MultiPromptChain](/docs/versions/migrating_chains/multi_prompt_chain) 中使用 `LLMRouterChain` 的示例。下面是默认提示的一个（示例）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "364814a5-d15c-41bb-bf3f-581df51a4721",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
            "\n",
            "<< FORMATTING >>\n",
            "Return a markdown code snippet with a JSON object formatted to look like:\n",
            "'''json\n",
            "{{\n",
            "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
            "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
            "}}\n",
            "'''\n",
            "\n",
            "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
            "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
            "\n",
            "<< CANDIDATE PROMPTS >>\n",
            "\n",
            "animals: prompt for animal expert\n",
            "vegetables: prompt for a vegetable expert\n",
            "\n",
            "\n",
            "<< INPUT >>\n",
            "{input}\n",
            "\n",
            "<< OUTPUT (must include '''json at the start of the response) >>\n",
            "<< OUTPUT (must end with ''') >>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains.router.multi_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "\n",
        "destinations = \"\"\"\n",
        "animals: prompt for animal expert\n",
        "vegetables: prompt for a vegetable expert\n",
        "\"\"\"\n",
        "\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations)\n",
        "\n",
        "print(router_template.replace(\"`\", \"'\"))  # for rendering purposes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "934937d1-fc0a-4d3f-b297-29f96e6a8f5e",
      "metadata": {},
      "source": [
        "大部分行为是通过一个自然语言提示来确定的。支持 [工具调用](/docs/how_to/tool_calling/) 功能的聊天模型为此任务带来了一些优势：\n\n- 支持聊天提示模板，包括带有 `system` 和其他角色的消息；\n- 工具调用模型经过微调，可生成结构化输出；\n- 支持流式传输和异步操作等可运行方法。\n\n现在，让我们将 `LLMRouterChain` 与使用工具调用的 LCEL 实现进行并排比较。请注意，在本指南中，我们将使用 `langchain-openai >= 0.1.20`："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed12b22b-5452-4776-aee3-b67d9f965082",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU langchain-core langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0edbba1-a497-49ef-ade7-4fe7967360eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d4dc41c-3fdc-4093-ba5e-31a9ebb54e13",
      "metadata": {},
      "source": [
        "## 旧版\n\n<details open>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c58c9269-5a1d-4234-88b5-7168944618bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "router_prompt = PromptTemplate(\n",
        "    # Note: here we use the prompt template from above. Generally this would need\n",
        "    # to be customized.\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a22ebdca-5f53-459e-9cff-a97b2354ffe0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vegetables\n"
          ]
        }
      ],
      "source": [
        "result = chain.invoke({\"input\": \"What color are carrots?\"})\n",
        "\n",
        "print(result[\"destination\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd48120-056f-4c58-a04f-da5198c23068",
      "metadata": {},
      "source": [
        "</details>\n\n## LCEL\n\n<details open>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5bbebac2-df19-4f59-8a69-f61cd7286e59",
      "metadata": {},
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "route_system = \"Route the user's query to either the animal or vegetable expert.\"\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", route_system),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Define schema for output:\n",
        "class RouteQuery(TypedDict):\n",
        "    \"\"\"Route query to destination expert.\"\"\"\n",
        "\n",
        "    destination: Literal[\"animal\", \"vegetable\"]\n",
        "\n",
        "\n",
        "# Instead of writing formatting instructions into the prompt, we\n",
        "# leverage .with_structured_output to coerce the output into a simple\n",
        "# schema.\n",
        "chain = route_prompt | llm.with_structured_output(RouteQuery)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "88012e10-8def-44fa-833f-989935824182",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vegetable\n"
          ]
        }
      ],
      "source": [
        "result = chain.invoke({\"input\": \"What color are carrots?\"})\n",
        "\n",
        "print(result[\"destination\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baf7ba9e-65b4-48af-8a39-453c01a7b7cb",
      "metadata": {},
      "source": [
        "</details>\n\n## 后续步骤\n\n请参阅[此教程](/docs/tutorials/llm_chain)，了解有关使用 prompt 模板、LLM 和输出解析器进行构建的更多详细信息。\n\n请查看[LCEL 概念文档](/docs/concepts/lcel)，获取更多背景信息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "353e4bab-3b8a-4e89-89e2-200a8d8eb8dd",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}