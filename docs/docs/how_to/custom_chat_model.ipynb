{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e3da9a3f-f583-4ba6-994e-0e8c1158f5eb",
      "metadata": {},
      "source": [
        "# å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰èŠå¤©æ¨¡å‹ç±»\n\n:::info å…ˆå†³æ¡ä»¶\n\næœ¬æŒ‡å—å‡å®šæ‚¨ç†Ÿæ‚‰ä»¥ä¸‹æ¦‚å¿µï¼š\n- [èŠå¤©æ¨¡å‹](/docs/concepts/chat_models)\n\n:::\n\nåœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ LangChain æŠ½è±¡æ¥åˆ›å»ºè‡ªå®šä¹‰ [èŠå¤©æ¨¡å‹](/docs/concepts/chat_models/)ã€‚\n\nä½¿ç”¨æ ‡å‡†çš„ [`BaseChatModel`](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) æ¥å£åŒ…è£…æ‚¨çš„ LLMï¼Œæ‚¨å¯ä»¥è®©æ‚¨çš„ LLM ç”¨äºç°æœ‰çš„ LangChain ç¨‹åºï¼Œåªéœ€å°‘é‡ä»£ç ä¿®æ”¹ï¼\n\nä½œä¸ºä¸€é¡¹é¢å¤–çš„å¥½å¤„ï¼Œæ‚¨çš„ LLM å°†è‡ªåŠ¨æˆä¸º LangChain [å¯è¿è¡Œå¯¹è±¡](/docs/concepts/runnables/)ï¼Œå¹¶å¯ä»¥ç›´æ¥è·å¾—ä¸€äº›ä¼˜åŒ–ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡çº¿ç¨‹æ± è¿›è¡Œæ‰¹å¤„ç†ï¼‰ã€å¼‚æ­¥æ”¯æŒã€`astream_events` API ç­‰ã€‚\n\n## è¾“å…¥å’Œè¾“å‡º\n\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è®¨è®º **[æ¶ˆæ¯](/docs/concepts/messages/)**ï¼Œå®ƒä»¬æ˜¯èŠå¤©æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºã€‚\n\n### æ¶ˆæ¯\n\nèŠå¤©æ¨¡å‹ä»¥æ¶ˆæ¯ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æ¶ˆæ¯ä½œä¸ºè¾“å‡ºã€‚\n\nLangChain æœ‰å‡ ç§ [å†…ç½®æ¶ˆæ¯ç±»å‹](/docs/concepts/messages/)ï¼š\n\n| æ¶ˆæ¯ç±»å‹          | æè¿°                                                                                                                                   |\n|-----------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n| `SystemMessage`       | ç”¨äºè°ƒæ•´ AI è¡Œä¸ºï¼Œé€šå¸¸ä½œä¸ºä¸€ç³»åˆ—è¾“å…¥æ¶ˆæ¯çš„ç¬¬ä¸€ä¸ªä¼ å…¥ã€‚                                                                                                                               |\n| `HumanMessage`        | ä»£è¡¨ä¸èŠå¤©æ¨¡å‹äº’åŠ¨çš„äººå‘˜çš„æ¶ˆæ¯ã€‚                                                                                                                                  |\n| `AIMessage`           | ä»£è¡¨æ¥è‡ªèŠå¤©æ¨¡å‹çš„æ¶ˆæ¯ã€‚è¿™å¯ä»¥æ˜¯æ–‡æœ¬ï¼Œä¹Ÿå¯ä»¥æ˜¯è°ƒç”¨å·¥å…·çš„è¯·æ±‚ã€‚                                                                                                                                 |\n| `FunctionMessage` / `ToolMessage` | ç”¨äºå°†å·¥å…·è°ƒç”¨çš„ç»“æœä¼ é€’å›æ¨¡å‹çš„æ¶ˆæ¯ã€‚                                                                                                                                 |\n| `AIMessageChunk` / `HumanMessageChunk` / ... | æ¯ç§æ¶ˆæ¯ç±»å‹çš„åˆ†å—å˜ä½“ã€‚ |\n\n\n:::note\n`ToolMessage` å’Œ `FunctionMessage` ç´§éš OpenAI çš„ `function` å’Œ `tool` è§’è‰²ã€‚\n\nè¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œéšç€è¶Šæ¥è¶Šå¤šçš„æ¨¡å‹æ·»åŠ å‡½æ•°è°ƒç”¨åŠŸèƒ½ã€‚é¢„è®¡è¯¥æ¶æ„å°†æœ‰æ‰€æ·»åŠ ã€‚\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c5046e6a-8b09-4a99-b6e6-7a605aac5738",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    FunctionMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    ToolMessage,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53033447-8260-4f53-bd6f-b2f744e04e75",
      "metadata": {},
      "source": [
        "### æµå¼ä¼ è¾“å˜ä½“\n\næ‰€æœ‰çš„èŠå¤©æ¶ˆæ¯éƒ½æœ‰ä¸€ä¸ªæµå¼ä¼ è¾“å˜ä½“ï¼Œå…¶åç§°åŒ…å« `Chunk`ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d4656e9d-bfa1-4703-8f79-762fe6421294",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    AIMessageChunk,\n",
        "    FunctionMessageChunk,\n",
        "    HumanMessageChunk,\n",
        "    SystemMessageChunk,\n",
        "    ToolMessageChunk,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ebf3f4-c760-4898-b921-fdb469453d4a",
      "metadata": {},
      "source": [
        "å½“æµå¼è¾“å‡ºèŠå¤©æ¨¡å‹æ—¶ï¼Œä¼šä½¿ç”¨è¿™äº›åˆ†å—ï¼Œå®ƒä»¬éƒ½å®šä¹‰äº†ä¸€ä¸ªé™„åŠ å±æ€§ï¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9c15c299-6f8a-49cf-a072-09924fd44396",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessageChunk(content='Hello World!')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AIMessageChunk(content=\"Hello\") + AIMessageChunk(content=\" World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbfebea1",
      "metadata": {},
      "source": [
        "## åŸºç¡€èŠå¤©æ¨¡å‹\n\nè®©æˆ‘ä»¬æ¥å®ç°ä¸€ä¸ªèŠå¤©æ¨¡å‹ï¼Œå®ƒä¼šå›æ˜¾æç¤ºä¸­æœ€åä¸€æ¡æ¶ˆæ¯çš„å‰ `n` ä¸ªå­—ç¬¦ï¼\n\nä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ç»§æ‰¿è‡ª `BaseChatModel`ï¼Œå¹¶ä¸”éœ€è¦å®ç°ä»¥ä¸‹å†…å®¹ï¼š\n\n| æ–¹æ³•/å±æ€§                    | æè¿°                                                       | å¿…éœ€/å¯é€‰  |\n|------------------------------------|-------------------------------------------------------------------|--------------------|\n| `_generate`                        | ç”¨äºä»æç¤ºç”ŸæˆèŠå¤©ç»“æœ                                                      | å¿…éœ€           |\n| `_llm_type` ï¼ˆå±æ€§ï¼‰             | ç”¨äºå”¯ä¸€æ ‡è¯†æ¨¡å‹ç±»å‹ã€‚ç”¨äºæ—¥å¿—è®°å½•ã€‚                                                 | å¿…éœ€           |\n| `_identifying_params` ï¼ˆå±æ€§ï¼‰   | ä»£è¡¨æ¨¡å‹å‚æ•°åŒ–ä»¥ç”¨äºè·Ÿè¸ªç›®çš„ã€‚                                                    | å¯é€‰           |\n| `_stream`                          | ç”¨äºå®ç°æµå¼ä¼ è¾“ã€‚                                                      | å¯é€‰           |\n| `_agenerate`                       | ç”¨äºå®ç°åŸç”Ÿçš„å¼‚æ­¥æ–¹æ³•ã€‚                                                     | å¯é€‰           |\n| `_astream`                         | ç”¨äºå®ç° `_stream` çš„å¼‚æ­¥ç‰ˆæœ¬ã€‚                                                 | å¯é€‰           |\n\n\n:::tip\n`_astream` çš„å®ç°ä½¿ç”¨ `run_in_executor` åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­å¯åŠ¨åŒæ­¥çš„ `_stream`ï¼ˆå¦‚æœ `_stream` å·²å®ç°ï¼‰ï¼Œå¦åˆ™å®ƒå°†å›é€€ä½¿ç”¨ `_agenerate`ã€‚\n\nå¦‚æœä½ æƒ³é‡ç”¨ `_stream` çš„å®ç°ï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªæŠ€å·§ï¼Œä½†å¦‚æœä½ èƒ½å®ç°åŸç”Ÿå¼‚æ­¥ä»£ç ï¼Œé‚£å°†æ˜¯æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºè¯¥ä»£ç å°†ä»¥æ›´ä½çš„å¼€é”€è¿è¡Œã€‚\n:::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e7047bd-c235-46f6-85e1-d6d7e0868eb1",
      "metadata": {},
      "source": [
        "### å®ç°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ba32e5-5a6d-49f4-bb68-911827b84d61",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, Iterator, List, Optional\n",
        "\n",
        "from langchain_core.callbacks import (\n",
        "    CallbackManagerForLLMRun,\n",
        ")\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    AIMessageChunk,\n",
        "    BaseMessage,\n",
        ")\n",
        "from langchain_core.messages.ai import UsageMetadata\n",
        "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
        "from pydantic import Field\n",
        "\n",
        "\n",
        "class ChatParrotLink(BaseChatModel):\n",
        "    \"\"\"A custom chat model that echoes the first `parrot_buffer_length` characters\n",
        "    of the input.\n",
        "\n",
        "    When contributing an implementation to LangChain, carefully document\n",
        "    the model including the initialization parameters, include\n",
        "    an example of how to initialize the model and include any relevant\n",
        "    links to the underlying models documentation or API.\n",
        "\n",
        "    Example:\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            model = ChatParrotLink(parrot_buffer_length=2, model=\"bird-brain-001\")\n",
        "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
        "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
        "                                 [HumanMessage(content=\"world\")]])\n",
        "    \"\"\"\n",
        "\n",
        "    model_name: str = Field(alias=\"model\")\n",
        "    \"\"\"The name of the model\"\"\"\n",
        "    parrot_buffer_length: int\n",
        "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
        "    temperature: Optional[float] = None\n",
        "    max_tokens: Optional[int] = None\n",
        "    timeout: Optional[int] = None\n",
        "    stop: Optional[List[str]] = None\n",
        "    max_retries: int = 2\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        \"\"\"Override the _generate method to implement the chat model logic.\n",
        "\n",
        "        This can be a call to an API, a call to a local model, or any other\n",
        "        implementation that generates a response to the input prompt.\n",
        "\n",
        "        Args:\n",
        "            messages: the prompt composed of a list of messages.\n",
        "            stop: a list of strings on which the model should stop generating.\n",
        "                  If generation stops due to a stop token, the stop token itself\n",
        "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
        "                  across models right now, but it's a good practice to follow since\n",
        "                  it makes it much easier to parse the output of the model\n",
        "                  downstream and understand why generation stopped.\n",
        "            run_manager: A run manager with callbacks for the LLM.\n",
        "        \"\"\"\n",
        "        # Replace this with actual logic to generate a response from a list\n",
        "        # of messages.\n",
        "        last_message = messages[-1]\n",
        "        tokens = last_message.content[: self.parrot_buffer_length]\n",
        "        ct_input_tokens = sum(len(message.content) for message in messages)\n",
        "        ct_output_tokens = len(tokens)\n",
        "        message = AIMessage(\n",
        "            content=tokens,\n",
        "            additional_kwargs={},  # Used to add additional payload to the message\n",
        "            response_metadata={  # Use for response metadata\n",
        "                \"time_in_seconds\": 3,\n",
        "                \"model_name\": self.model_name,\n",
        "            },\n",
        "            usage_metadata={\n",
        "                \"input_tokens\": ct_input_tokens,\n",
        "                \"output_tokens\": ct_output_tokens,\n",
        "                \"total_tokens\": ct_input_tokens + ct_output_tokens,\n",
        "            },\n",
        "        )\n",
        "        ##\n",
        "\n",
        "        generation = ChatGeneration(message=message)\n",
        "        return ChatResult(generations=[generation])\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Iterator[ChatGenerationChunk]:\n",
        "        \"\"\"Stream the output of the model.\n",
        "\n",
        "        This method should be implemented if the model can generate output\n",
        "        in a streaming fashion. If the model does not support streaming,\n",
        "        do not implement it. In that case streaming requests will be automatically\n",
        "        handled by the _generate method.\n",
        "\n",
        "        Args:\n",
        "            messages: the prompt composed of a list of messages.\n",
        "            stop: a list of strings on which the model should stop generating.\n",
        "                  If generation stops due to a stop token, the stop token itself\n",
        "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
        "                  across models right now, but it's a good practice to follow since\n",
        "                  it makes it much easier to parse the output of the model\n",
        "                  downstream and understand why generation stopped.\n",
        "            run_manager: A run manager with callbacks for the LLM.\n",
        "        \"\"\"\n",
        "        last_message = messages[-1]\n",
        "        tokens = str(last_message.content[: self.parrot_buffer_length])\n",
        "        ct_input_tokens = sum(len(message.content) for message in messages)\n",
        "\n",
        "        for token in tokens:\n",
        "            usage_metadata = UsageMetadata(\n",
        "                {\n",
        "                    \"input_tokens\": ct_input_tokens,\n",
        "                    \"output_tokens\": 1,\n",
        "                    \"total_tokens\": ct_input_tokens + 1,\n",
        "                }\n",
        "            )\n",
        "            ct_input_tokens = 0\n",
        "            chunk = ChatGenerationChunk(\n",
        "                message=AIMessageChunk(content=token, usage_metadata=usage_metadata)\n",
        "            )\n",
        "\n",
        "            if run_manager:\n",
        "                # This is optional in newer versions of LangChain\n",
        "                # The on_llm_new_token will be called automatically\n",
        "                run_manager.on_llm_new_token(token, chunk=chunk)\n",
        "\n",
        "            yield chunk\n",
        "\n",
        "        # Let's add some other information (e.g., response metadata)\n",
        "        chunk = ChatGenerationChunk(\n",
        "            message=AIMessageChunk(\n",
        "                content=\"\",\n",
        "                response_metadata={\"time_in_sec\": 3, \"model_name\": self.model_name},\n",
        "            )\n",
        "        )\n",
        "        if run_manager:\n",
        "            # This is optional in newer versions of LangChain\n",
        "            # The on_llm_new_token will be called automatically\n",
        "            run_manager.on_llm_new_token(token, chunk=chunk)\n",
        "        yield chunk\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
        "        return \"echoing-chat-model-advanced\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return a dictionary of identifying parameters.\n",
        "\n",
        "        This information is used by the LangChain callback system, which\n",
        "        is used for tracing purposes make it possible to monitor LLMs.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            # The model name allows users to specify custom token counting\n",
        "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
        "            # can provide per token pricing for their model and monitor\n",
        "            # costs for the given LLM.)\n",
        "            \"model_name\": self.model_name,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e9af284-f2d3-44e2-ac6a-09b73d89ada3",
      "metadata": {},
      "source": [
        "### æ¥æµ‹è¯•ä¸€ä¸‹ ğŸ§ª\n\nèŠå¤©æ¨¡å‹å°†å®ç° LangChain çš„æ ‡å‡† `Runnable` æ¥å£ï¼Œè®¸å¤š LangChain çš„æŠ½è±¡éƒ½æ”¯æŒè¿™ä¸ªæ¥å£ï¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "27689f30-dcd2-466b-ba9d-f60b7d434110",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Meo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-cf11aeb6-8ab6-43d7-8c68-c1ef89b6d78e-0', usage_metadata={'input_tokens': 26, 'output_tokens': 3, 'total_tokens': 29})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ChatParrotLink(parrot_buffer_length=3, model=\"my_custom_model\")\n",
        "\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"hello!\"),\n",
        "        AIMessage(content=\"Hi there human!\"),\n",
        "        HumanMessage(content=\"Meow!\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "406436df-31bf-466b-9c3d-39db9d6b6407",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-618e5ed4-d611-4083-8cf1-c270726be8d9-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.invoke(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a72ffa46-6004-41ef-bbe4-56fa17a029e2",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-eea4ed7d-d750-48dc-90c0-7acca1ff388f-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8}),\n",
              " AIMessage(content='goo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-07cfc5c1-3c62-485f-b1e0-3d46e1547287-0', usage_metadata={'input_tokens': 7, 'output_tokens': 3, 'total_tokens': 10})]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.batch([\"hello\", \"goodbye\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3633be2c-2ea0-42f9-a72f-3b5240690b55",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c|a|t||"
          ]
        }
      ],
      "source": [
        "for chunk in model.stream(\"cat\"):\n",
        "    print(chunk.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f8a7c42-aec4-4116-adf3-93133d409827",
      "metadata": {},
      "source": [
        "è¯·æŸ¥çœ‹æ¨¡å‹ä¸­ `_astream` çš„å®ç°ï¼å¦‚æœæ‚¨ä¸å®ç°å®ƒï¼Œé‚£ä¹ˆå°†ä¸ä¼šæœ‰ä»»ä½•è¾“å‡ºæµå¼ä¼ è¾“ï¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b7d73995-eeab-48c6-a7d8-32c98ba29fc2",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c|a|t||"
          ]
        }
      ],
      "source": [
        "async for chunk in model.astream(\"cat\"):\n",
        "    print(chunk.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f80dc55b-d159-4527-9191-407a7c6d6042",
      "metadata": {},
      "source": [
        "è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ astream äº‹ä»¶ APIï¼Œè¿™ä¹Ÿæœ‰åŠ©äºä»”ç»†æ£€æŸ¥æ‰€æœ‰å›è°ƒæ˜¯å¦éƒ½å·²å®ç°ï¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "17840eba-8ff4-4e73-8e4f-85f16eb1c9d0",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'event': 'on_chat_model_start', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'name': 'ChatParrotLink', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='c', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 1, 'total_tokens': 4})}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='a', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='t', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a')}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_end', 'name': 'ChatParrotLink', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 3, 'total_tokens': 6})}, 'parent_ids': []}\n"
          ]
        }
      ],
      "source": [
        "async for event in model.astream_events(\"cat\", version=\"v1\"):\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44ee559b-b1da-4851-8c97-420ab394aff9",
      "metadata": {},
      "source": [
        "## è´¡çŒ®\n\næˆ‘ä»¬éå¸¸æ„Ÿè°¢æ‰€æœ‰å¯¹èŠå¤©æ¨¡å‹é›†æˆçš„è´¡çŒ®ã€‚\n\nä»¥ä¸‹æ¸…å•å°†å¸®åŠ©æ‚¨ç¡®ä¿æ‚¨çš„è´¡çŒ®è¢«æ·»åŠ åˆ° LangChainï¼š\n\næ–‡æ¡£ï¼š\n\n* ä¸ºæ‰€æœ‰åˆå§‹åŒ–å‚æ•°æ·»åŠ äº†æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå› ä¸ºè¿™äº›å°†åœ¨**API å‚è€ƒ**ä¸­æ˜¾ç¤ºã€‚\n* å¦‚æœæ¨¡å‹ç”±æŸä¸ªæœåŠ¡æä¾›æ”¯æŒï¼Œå…¶ç±»çš„æ–‡æ¡£å­—ç¬¦ä¸²åº”åŒ…å«æŒ‡å‘è¯¥æ¨¡å‹ API çš„é“¾æ¥ã€‚\n\næµ‹è¯•ï¼š\n\n* [ ] ä¸ºé‡å†™çš„æ–¹æ³•æ·»åŠ å•å…ƒæµ‹è¯•æˆ–é›†æˆæµ‹è¯•ã€‚å¦‚æœæ‚¨é‡å†™äº†ç›¸åº”çš„ä»£ç ï¼Œè¯·éªŒè¯ `invoke`ã€`ainvoke`ã€`batch`ã€`stream` æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚\n\næµå¼ä¼ è¾“ï¼ˆå¦‚æœæ‚¨æ­£åœ¨å®ç°ï¼‰ï¼š\n\n* [ ] å®ç° `_stream` æ–¹æ³•ä»¥å¯ç”¨æµå¼ä¼ è¾“\n\nåœæ­¢ä»¤ç‰Œè¡Œä¸ºï¼š\n\n* [ ] åº”éµå®ˆåœæ­¢ä»¤ç‰Œ\n* [ ] åœæ­¢ä»¤ç‰Œåº”ä½œä¸ºå“åº”çš„ä¸€éƒ¨åˆ†åŒ…å«åœ¨å†…\n\nç§˜å¯† API å¯†é’¥ï¼š\n\n* [ ] å¦‚æœæ‚¨çš„æ¨¡å‹è¿æ¥åˆ° APIï¼Œå®ƒå¯èƒ½ä¼šåœ¨åˆå§‹åŒ–æ—¶æ¥å— API å¯†é’¥ã€‚è¯·ä½¿ç”¨ Pydantic çš„ `SecretStr` ç±»å‹å¤„ç†ç§˜å¯†ä¿¡æ¯ï¼Œä»¥å…å®ƒä»¬åœ¨è¢«æ‰“å°æ—¶æ„å¤–æ³„éœ²ã€‚\n\nè¯†åˆ«å‚æ•°ï¼š\n\n* [ ] åœ¨è¯†åˆ«å‚æ•°ä¸­åŒ…å« `model_name`\n\nä¼˜åŒ–ï¼š\n\nè€ƒè™‘æä¾›æœ¬åœ°å¼‚æ­¥æ”¯æŒä»¥å‡å°‘æ¨¡å‹çš„å¼€é”€ï¼\n\n* [ ] æä¾›äº† `_agenerate`ï¼ˆ`ainvoke` ä½¿ç”¨ï¼‰çš„æœ¬åœ°å¼‚æ­¥ç‰ˆæœ¬\n* [ ] æä¾›äº† `_astream`ï¼ˆ`astream` ä½¿ç”¨ï¼‰çš„æœ¬åœ°å¼‚æ­¥ç‰ˆæœ¬\n\n## åç»­æ­¥éª¤\n\nç°åœ¨æ‚¨å·²ç»å­¦ä¼šäº†å¦‚ä½•åˆ›å»ºè‡ªå·±çš„è‡ªå®šä¹‰èŠå¤©æ¨¡å‹ã€‚\n\næ¥ä¸‹æ¥ï¼ŒæŸ¥çœ‹æ­¤éƒ¨åˆ†ä¸­å…³äºèŠå¤©æ¨¡å‹çš„å…¶ä»–æ“ä½œæŒ‡å—ï¼Œä¾‹å¦‚[å¦‚ä½•è®©æ¨¡å‹è¿”å›ç»“æ„åŒ–è¾“å‡º](/docs/how_to/structured_output)æˆ–[å¦‚ä½•è·Ÿè¸ªèŠå¤©æ¨¡å‹çš„ä»¤ç‰Œä½¿ç”¨æƒ…å†µ](/docs/how_to/chat_token_usage_tracking)ã€‚"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}