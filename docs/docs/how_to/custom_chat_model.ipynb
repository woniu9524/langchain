{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e3da9a3f-f583-4ba6-994e-0e8c1158f5eb",
      "metadata": {},
      "source": [
        "# 如何创建自定义聊天模型类\n\n:::info 先决条件\n\n本指南假定您熟悉以下概念：\n- [聊天模型](/docs/concepts/chat_models)\n\n:::\n\n在本指南中，我们将学习如何使用 LangChain 抽象来创建自定义 [聊天模型](/docs/concepts/chat_models/)。\n\n使用标准的 [`BaseChatModel`](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) 接口包装您的 LLM，您可以让您的 LLM 用于现有的 LangChain 程序，只需少量代码修改！\n\n作为一项额外的好处，您的 LLM 将自动成为 LangChain [可运行对象](/docs/concepts/runnables/)，并可以直接获得一些优化（例如，通过线程池进行批处理）、异步支持、`astream_events` API 等。\n\n## 输入和输出\n\n首先，我们需要讨论 **[消息](/docs/concepts/messages/)**，它们是聊天模型的输入和输出。\n\n### 消息\n\n聊天模型以消息作为输入，并返回消息作为输出。\n\nLangChain 有几种 [内置消息类型](/docs/concepts/messages/)：\n\n| 消息类型          | 描述                                                                                                                                   |\n|-----------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n| `SystemMessage`       | 用于调整 AI 行为，通常作为一系列输入消息的第一个传入。                                                                                                                               |\n| `HumanMessage`        | 代表与聊天模型互动的人员的消息。                                                                                                                                  |\n| `AIMessage`           | 代表来自聊天模型的消息。这可以是文本，也可以是调用工具的请求。                                                                                                                                 |\n| `FunctionMessage` / `ToolMessage` | 用于将工具调用的结果传递回模型的消息。                                                                                                                                 |\n| `AIMessageChunk` / `HumanMessageChunk` / ... | 每种消息类型的分块变体。 |\n\n\n:::note\n`ToolMessage` 和 `FunctionMessage` 紧随 OpenAI 的 `function` 和 `tool` 角色。\n\n这是一个快速发展的领域，随着越来越多的模型添加函数调用功能。预计该架构将有所添加。\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c5046e6a-8b09-4a99-b6e6-7a605aac5738",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    FunctionMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    ToolMessage,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53033447-8260-4f53-bd6f-b2f744e04e75",
      "metadata": {},
      "source": [
        "### 流式传输变体\n\n所有的聊天消息都有一个流式传输变体，其名称包含 `Chunk`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d4656e9d-bfa1-4703-8f79-762fe6421294",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    AIMessageChunk,\n",
        "    FunctionMessageChunk,\n",
        "    HumanMessageChunk,\n",
        "    SystemMessageChunk,\n",
        "    ToolMessageChunk,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ebf3f4-c760-4898-b921-fdb469453d4a",
      "metadata": {},
      "source": [
        "当流式输出聊天模型时，会使用这些分块，它们都定义了一个附加属性！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9c15c299-6f8a-49cf-a072-09924fd44396",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessageChunk(content='Hello World!')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AIMessageChunk(content=\"Hello\") + AIMessageChunk(content=\" World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbfebea1",
      "metadata": {},
      "source": [
        "## 基础聊天模型\n\n让我们来实现一个聊天模型，它会回显提示中最后一条消息的前 `n` 个字符！\n\n为此，我们将继承自 `BaseChatModel`，并且需要实现以下内容：\n\n| 方法/属性                    | 描述                                                       | 必需/可选  |\n|------------------------------------|-------------------------------------------------------------------|--------------------|\n| `_generate`                        | 用于从提示生成聊天结果                                                      | 必需           |\n| `_llm_type` （属性）             | 用于唯一标识模型类型。用于日志记录。                                                 | 必需           |\n| `_identifying_params` （属性）   | 代表模型参数化以用于跟踪目的。                                                    | 可选           |\n| `_stream`                          | 用于实现流式传输。                                                      | 可选           |\n| `_agenerate`                       | 用于实现原生的异步方法。                                                     | 可选           |\n| `_astream`                         | 用于实现 `_stream` 的异步版本。                                                 | 可选           |\n\n\n:::tip\n`_astream` 的实现使用 `run_in_executor` 在单独的线程中启动同步的 `_stream`（如果 `_stream` 已实现），否则它将回退使用 `_agenerate`。\n\n如果你想重用 `_stream` 的实现，可以使用这个技巧，但如果你能实现原生异步代码，那将是更好的解决方案，因为该代码将以更低的开销运行。\n:::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e7047bd-c235-46f6-85e1-d6d7e0868eb1",
      "metadata": {},
      "source": [
        "### 实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ba32e5-5a6d-49f4-bb68-911827b84d61",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, Iterator, List, Optional\n",
        "\n",
        "from langchain_core.callbacks import (\n",
        "    CallbackManagerForLLMRun,\n",
        ")\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    AIMessageChunk,\n",
        "    BaseMessage,\n",
        ")\n",
        "from langchain_core.messages.ai import UsageMetadata\n",
        "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
        "from pydantic import Field\n",
        "\n",
        "\n",
        "class ChatParrotLink(BaseChatModel):\n",
        "    \"\"\"A custom chat model that echoes the first `parrot_buffer_length` characters\n",
        "    of the input.\n",
        "\n",
        "    When contributing an implementation to LangChain, carefully document\n",
        "    the model including the initialization parameters, include\n",
        "    an example of how to initialize the model and include any relevant\n",
        "    links to the underlying models documentation or API.\n",
        "\n",
        "    Example:\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            model = ChatParrotLink(parrot_buffer_length=2, model=\"bird-brain-001\")\n",
        "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
        "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
        "                                 [HumanMessage(content=\"world\")]])\n",
        "    \"\"\"\n",
        "\n",
        "    model_name: str = Field(alias=\"model\")\n",
        "    \"\"\"The name of the model\"\"\"\n",
        "    parrot_buffer_length: int\n",
        "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
        "    temperature: Optional[float] = None\n",
        "    max_tokens: Optional[int] = None\n",
        "    timeout: Optional[int] = None\n",
        "    stop: Optional[List[str]] = None\n",
        "    max_retries: int = 2\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        \"\"\"Override the _generate method to implement the chat model logic.\n",
        "\n",
        "        This can be a call to an API, a call to a local model, or any other\n",
        "        implementation that generates a response to the input prompt.\n",
        "\n",
        "        Args:\n",
        "            messages: the prompt composed of a list of messages.\n",
        "            stop: a list of strings on which the model should stop generating.\n",
        "                  If generation stops due to a stop token, the stop token itself\n",
        "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
        "                  across models right now, but it's a good practice to follow since\n",
        "                  it makes it much easier to parse the output of the model\n",
        "                  downstream and understand why generation stopped.\n",
        "            run_manager: A run manager with callbacks for the LLM.\n",
        "        \"\"\"\n",
        "        # Replace this with actual logic to generate a response from a list\n",
        "        # of messages.\n",
        "        last_message = messages[-1]\n",
        "        tokens = last_message.content[: self.parrot_buffer_length]\n",
        "        ct_input_tokens = sum(len(message.content) for message in messages)\n",
        "        ct_output_tokens = len(tokens)\n",
        "        message = AIMessage(\n",
        "            content=tokens,\n",
        "            additional_kwargs={},  # Used to add additional payload to the message\n",
        "            response_metadata={  # Use for response metadata\n",
        "                \"time_in_seconds\": 3,\n",
        "                \"model_name\": self.model_name,\n",
        "            },\n",
        "            usage_metadata={\n",
        "                \"input_tokens\": ct_input_tokens,\n",
        "                \"output_tokens\": ct_output_tokens,\n",
        "                \"total_tokens\": ct_input_tokens + ct_output_tokens,\n",
        "            },\n",
        "        )\n",
        "        ##\n",
        "\n",
        "        generation = ChatGeneration(message=message)\n",
        "        return ChatResult(generations=[generation])\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Iterator[ChatGenerationChunk]:\n",
        "        \"\"\"Stream the output of the model.\n",
        "\n",
        "        This method should be implemented if the model can generate output\n",
        "        in a streaming fashion. If the model does not support streaming,\n",
        "        do not implement it. In that case streaming requests will be automatically\n",
        "        handled by the _generate method.\n",
        "\n",
        "        Args:\n",
        "            messages: the prompt composed of a list of messages.\n",
        "            stop: a list of strings on which the model should stop generating.\n",
        "                  If generation stops due to a stop token, the stop token itself\n",
        "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
        "                  across models right now, but it's a good practice to follow since\n",
        "                  it makes it much easier to parse the output of the model\n",
        "                  downstream and understand why generation stopped.\n",
        "            run_manager: A run manager with callbacks for the LLM.\n",
        "        \"\"\"\n",
        "        last_message = messages[-1]\n",
        "        tokens = str(last_message.content[: self.parrot_buffer_length])\n",
        "        ct_input_tokens = sum(len(message.content) for message in messages)\n",
        "\n",
        "        for token in tokens:\n",
        "            usage_metadata = UsageMetadata(\n",
        "                {\n",
        "                    \"input_tokens\": ct_input_tokens,\n",
        "                    \"output_tokens\": 1,\n",
        "                    \"total_tokens\": ct_input_tokens + 1,\n",
        "                }\n",
        "            )\n",
        "            ct_input_tokens = 0\n",
        "            chunk = ChatGenerationChunk(\n",
        "                message=AIMessageChunk(content=token, usage_metadata=usage_metadata)\n",
        "            )\n",
        "\n",
        "            if run_manager:\n",
        "                # This is optional in newer versions of LangChain\n",
        "                # The on_llm_new_token will be called automatically\n",
        "                run_manager.on_llm_new_token(token, chunk=chunk)\n",
        "\n",
        "            yield chunk\n",
        "\n",
        "        # Let's add some other information (e.g., response metadata)\n",
        "        chunk = ChatGenerationChunk(\n",
        "            message=AIMessageChunk(\n",
        "                content=\"\",\n",
        "                response_metadata={\"time_in_sec\": 3, \"model_name\": self.model_name},\n",
        "            )\n",
        "        )\n",
        "        if run_manager:\n",
        "            # This is optional in newer versions of LangChain\n",
        "            # The on_llm_new_token will be called automatically\n",
        "            run_manager.on_llm_new_token(token, chunk=chunk)\n",
        "        yield chunk\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
        "        return \"echoing-chat-model-advanced\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return a dictionary of identifying parameters.\n",
        "\n",
        "        This information is used by the LangChain callback system, which\n",
        "        is used for tracing purposes make it possible to monitor LLMs.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            # The model name allows users to specify custom token counting\n",
        "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
        "            # can provide per token pricing for their model and monitor\n",
        "            # costs for the given LLM.)\n",
        "            \"model_name\": self.model_name,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e9af284-f2d3-44e2-ac6a-09b73d89ada3",
      "metadata": {},
      "source": [
        "### 来测试一下 🧪\n\n聊天模型将实现 LangChain 的标准 `Runnable` 接口，许多 LangChain 的抽象都支持这个接口！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "27689f30-dcd2-466b-ba9d-f60b7d434110",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Meo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-cf11aeb6-8ab6-43d7-8c68-c1ef89b6d78e-0', usage_metadata={'input_tokens': 26, 'output_tokens': 3, 'total_tokens': 29})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ChatParrotLink(parrot_buffer_length=3, model=\"my_custom_model\")\n",
        "\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"hello!\"),\n",
        "        AIMessage(content=\"Hi there human!\"),\n",
        "        HumanMessage(content=\"Meow!\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "406436df-31bf-466b-9c3d-39db9d6b6407",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-618e5ed4-d611-4083-8cf1-c270726be8d9-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.invoke(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a72ffa46-6004-41ef-bbe4-56fa17a029e2",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-eea4ed7d-d750-48dc-90c0-7acca1ff388f-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8}),\n",
              " AIMessage(content='goo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-07cfc5c1-3c62-485f-b1e0-3d46e1547287-0', usage_metadata={'input_tokens': 7, 'output_tokens': 3, 'total_tokens': 10})]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.batch([\"hello\", \"goodbye\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3633be2c-2ea0-42f9-a72f-3b5240690b55",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c|a|t||"
          ]
        }
      ],
      "source": [
        "for chunk in model.stream(\"cat\"):\n",
        "    print(chunk.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f8a7c42-aec4-4116-adf3-93133d409827",
      "metadata": {},
      "source": [
        "请查看模型中 `_astream` 的实现！如果您不实现它，那么将不会有任何输出流式传输！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b7d73995-eeab-48c6-a7d8-32c98ba29fc2",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c|a|t||"
          ]
        }
      ],
      "source": [
        "async for chunk in model.astream(\"cat\"):\n",
        "    print(chunk.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f80dc55b-d159-4527-9191-407a7c6d6042",
      "metadata": {},
      "source": [
        "让我们尝试使用 astream 事件 API，这也有助于仔细检查所有回调是否都已实现！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "17840eba-8ff4-4e73-8e4f-85f16eb1c9d0",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'event': 'on_chat_model_start', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'name': 'ChatParrotLink', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='c', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 1, 'total_tokens': 4})}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='a', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='t', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a')}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_end', 'name': 'ChatParrotLink', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 3, 'total_tokens': 6})}, 'parent_ids': []}\n"
          ]
        }
      ],
      "source": [
        "async for event in model.astream_events(\"cat\", version=\"v1\"):\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44ee559b-b1da-4851-8c97-420ab394aff9",
      "metadata": {},
      "source": [
        "## 贡献\n\n我们非常感谢所有对聊天模型集成的贡献。\n\n以下清单将帮助您确保您的贡献被添加到 LangChain：\n\n文档：\n\n* 为所有初始化参数添加了文档字符串，因为这些将在**API 参考**中显示。\n* 如果模型由某个服务提供支持，其类的文档字符串应包含指向该模型 API 的链接。\n\n测试：\n\n* [ ] 为重写的方法添加单元测试或集成测试。如果您重写了相应的代码，请验证 `invoke`、`ainvoke`、`batch`、`stream` 是否正常工作。\n\n流式传输（如果您正在实现）：\n\n* [ ] 实现 `_stream` 方法以启用流式传输\n\n停止令牌行为：\n\n* [ ] 应遵守停止令牌\n* [ ] 停止令牌应作为响应的一部分包含在内\n\n秘密 API 密钥：\n\n* [ ] 如果您的模型连接到 API，它可能会在初始化时接受 API 密钥。请使用 Pydantic 的 `SecretStr` 类型处理秘密信息，以免它们在被打印时意外泄露。\n\n识别参数：\n\n* [ ] 在识别参数中包含 `model_name`\n\n优化：\n\n考虑提供本地异步支持以减少模型的开销！\n\n* [ ] 提供了 `_agenerate`（`ainvoke` 使用）的本地异步版本\n* [ ] 提供了 `_astream`（`astream` 使用）的本地异步版本\n\n## 后续步骤\n\n现在您已经学会了如何创建自己的自定义聊天模型。\n\n接下来，查看此部分中关于聊天模型的其他操作指南，例如[如何让模型返回结构化输出](/docs/how_to/structured_output)或[如何跟踪聊天模型的令牌使用情况](/docs/how_to/chat_token_usage_tracking)。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}