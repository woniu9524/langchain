{
  "cells": [
    {
      "cell_type": "raw",
      "id": "0bdb3b97-4989-4237-b43b-5943dbbd8302",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "keywords: [stream]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb7d49db-04d3-4399-bfe1-09f82bbe6015",
      "metadata": {},
      "source": [
        "# 如何流式运行可运行对象\n\n:::info 先决条件\n\n本指南假设您熟悉以下概念：\n- [聊天模型](/docs/concepts/chat_models)\n- [LangChain 表达式语言](/docs/concepts/lcel)\n- [输出解析器](/docs/concepts/output_parsers)\n\n:::\n\n流式传输对于让基于 LLM 的应用程序感觉响应迅速至关重要。\n\nLangChain 的重要原始组件，如 [聊天模型](/docs/concepts/chat_models)、[输出解析器](/docs/concepts/output_parsers)、[提示](/docs/concepts/prompt_templates)、[检索器](/docs/concepts/retrievers) 和 [代理](/docs/concepts/agents) 都实现了 LangChain 的 [可运行接口](/docs/concepts/runnables)。\n\n该接口提供了两种通用的流式传输内容的方法：\n\n1. 同步 `stream` 和异步 `astream`：这是流式传输的**默认实现**，它流式传输链的**最终输出**。\n2. 异步 `astream_events` 和异步 `astream_log`：这些方法提供了从链中流式传输**中间步骤**和**最终输出**的方法。\n\n让我们看看这两种方法，并尝试理解如何使用它们。\n\n:::info\n有关 LangChain 中流式传输技术的更高层概述，请参阅概念指南的[此部分](/docs/concepts/streaming)。\n:::\n\n## 使用 Stream\n\n所有 `Runnable` 对象都实现了一个名为 `stream` 的同步方法和一个名为 `astream` 的异步变体。\n\n这些方法旨在将最终输出分块流式传输，一旦每个块可用就立即生成。\n\n只有在程序中的所有步骤都知道如何处理**输入流**时，流式传输才可能实现；即，一次处理一个输入块，并生成一个相应的输出块。\n\n此处理的复杂性可能不同，从生成 LLM 所产生令牌等基本任务，到在整个 JSON 完成之前流式传输 JSON 结果的某些部分等更具挑战性的任务。\n\n探索流式传输的最佳起点是 LLM 应用程序中最重要的组件——LLM 本身！\n\n### LLM 和聊天模型\n\n大型语言模型及其聊天变体是基于 LLM 的应用程序中的主要瓶颈。\n\n大型语言模型可能需要**几秒钟**才能生成对查询的完整响应。这远远慢于应用程序在用户感觉响应时的**约 200-300 毫秒**的阈值。\n\n使应用程序感觉更具响应能力的关键策略是显示中间进度；即，**逐个令牌**地流式传输模型输出。\n\n我们将展示使用聊天模型进行流式传输的示例。从下面的选项中选择一个：\n\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\n\n<ChatModelTabs\n  customVarName=\"model\"\n/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f123bdcb-8c8b-440c-9bbd-aa5ed4e9cd17",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# | output: false\n",
        "# | echo: false\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "keys = [\n",
        "    \"ANTHROPIC_API_KEY\",\n",
        "    \"OPENAI_API_KEY\",\n",
        "]\n",
        "\n",
        "for key in keys:\n",
        "    if key not in os.environ:\n",
        "        os.environ[key] = getpass(f\"Enter API Key for {key}=?\")\n",
        "\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "model = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2464c57-0e89-4159-b21f-5859a21be658",
      "metadata": {},
      "source": [
        "让我们从同步的 `stream` API 开始："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8b44dfb2-0749-487a-8918-f8b6b8233093",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The| sky| appears| blue| during| the| day|.|"
          ]
        }
      ],
      "source": [
        "chunks = []\n",
        "for chunk in model.stream(\"what color is the sky?\"):\n",
        "    chunks.append(chunk)\n",
        "    print(chunk.content, end=\"|\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d835b5c-cbb7-41ab-8905-bdc24d515d29",
      "metadata": {},
      "source": [
        "或者，如果您在异步环境中使用，可以考虑使用异步 `astream` API："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f180b6a0-0027-4bd8-8bab-fde76e282609",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The| sky| appears| blue| during| the| day|.|"
          ]
        }
      ],
      "source": [
        "chunks = []\n",
        "async for chunk in model.astream(\"what color is the sky?\"):\n",
        "    chunks.append(chunk)\n",
        "    print(chunk.content, end=\"|\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66730a87-77d5-40d6-a68f-315121989bd1",
      "metadata": {},
      "source": [
        "让我们检查一下其中一个块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dade3000-1ac4-4f5c-b5c6-a0217f9f8a6b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessageChunk(content='The', id='run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3a47193-2bd1-46bc-9c7e-ea0f6b08c4a5",
      "metadata": {},
      "source": [
        "我们收到一个名为 `AIMessageChunk` 的东西。这个 chunk 代表了 `AIMessage` 的一部分。\n\n消息块在设计上是可累加的——只需将它们加起来，就可以得到响应到目前为止的状态！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d3cf5f38-249c-4da0-94e6-5e5203fad52e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessageChunk(content='The sky appears blue during', id='run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59ffbd9a-3b79-44b6-8883-1371f9460c77",
      "metadata": {},
      "source": [
        "### 链\n\n几乎所有的 LLM 应用都包含比调用语言模型更多的步骤。\n\n让我们使用 `LangChain Expression Language` (`LCEL`) 构建一个简单的链，它结合了提示、模型和解析器，并验证流式传输是否正常工作。\n\n我们将使用 [`StrOutputParser`](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) 来解析模型的输出。这是一个简单的解析器，用于从 `AIMessageChunk` 中提取 `content` 字段，从而获得模型返回的 `token`。\n\n:::tip\nLCEL 是一种通过将不同的 LangChain 原始组件链接在一起来声明性地指定“程序”的方式。使用 LCEL 创建的链可以自动实现 `stream` 和 `astream`，从而允许流式传输最终输出。事实上，使用 LCEL 创建的链实现了完整的标准 Runnable 接口。\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a8562ae2-3fd1-4829-9801-a5a732b1798d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here|'s| a| joke| about| a| par|rot|:|\n",
            "\n",
            "A man| goes| to| a| pet| shop| to| buy| a| par|rot|.| The| shop| owner| shows| him| two| stunning| pa|rr|ots| with| beautiful| pl|um|age|.|\n",
            "\n",
            "\"|There|'s| a| talking| par|rot| an|d a| non|-|talking| par|rot|,\"| the| owner| says|.| \"|The| talking| par|rot| costs| $|100|,| an|d the| non|-|talking| par|rot| is| $|20|.\"|\n",
            "\n",
            "The| man| says|,| \"|I|'ll| take| the| non|-|talking| par|rot| at| $|20|.\"|\n",
            "\n",
            "He| pays| an|d leaves| with| the| par|rot|.| As| he|'s| walking| down| the| street|,| the| par|rot| looks| up| at| him| an|d says|,| \"|You| know|,| you| really| are| a| stupi|d man|!\"|\n",
            "\n",
            "The| man| is| stun|ne|d an|d looks| at| the| par|rot| in| dis|bel|ief|.| The| par|rot| continues|,| \"|Yes|,| you| got| r|ippe|d off| big| time|!| I| can| talk| just| as| well| as| that| other| par|rot|,| an|d you| only| pai|d $|20| |for| me|!\"|"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "parser = StrOutputParser()\n",
        "chain = prompt | model | parser\n",
        "\n",
        "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
        "    print(chunk, end=\"|\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "868bc412",
      "metadata": {},
      "source": [
        "请注意，即使我们在上面的链末尾使用了 `parser`，我们仍然会获得流式输出。`parser` 会单独处理每个流式块。许多 [LCEL 原始组件](/docs/how_to#langchain-expression-language-lcel) 也支持这种转换风格的直通流式输出，这在构建应用程序时会非常方便。\n\n自定义函数可以 [设计成返回生成器](/docs/how_to/functions#streaming)，这些生成器能够处理流。\n\n某些可运行组件，例如 [提示模板](/docs/how_to#prompt-templates) 和 [聊天模型](/docs/how_to#chat-models)，无法处理单个块，而是会聚合所有先前的步骤。此类可运行组件可能会中断流式输出过程。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b399fb4-5e3c-4581-9570-6df9b42b623d",
      "metadata": {},
      "source": [
        ":::note\nLangChain 表达式语言允许您将链的构建与使用它的模式（例如，同步/异步、批量/流式传输等）分开。如果这与您正在构建的内容无关，您也可以依赖标准的 **命令式** 编程方法，通过在每个组件上单独调用 `invoke`、`batch` 或 `stream`，将结果分配给变量，然后在下游按需使用它们。\n\n:::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfff2701-8887-486f-8b3b-eb26383d4bb6",
      "metadata": {},
      "source": [
        "### 使用输入流\n\n如果您想在生成 JSON 的同时以流的方式输出它，该怎么做呢？\n\n如果您依赖 `json.loads` 来解析部分 JSON，解析会失败，因为部分 JSON 不是有效的 JSON。\n\n您可能会完全不知所措，并声称无法流式传输 JSON。\n\n事实证明，有一种方法可以做到这一点——解析器需要**在输入流上**操作，并尝试将部分 JSON“自动完成”为一个有效状态。\n\n让我们看看这样的解析器是如何工作的，以理解这意味着什么。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5ff63cce-715a-4561-951f-9321c82e8d81",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{}\n",
            "{'countries': []}\n",
            "{'countries': [{}]}\n",
            "{'countries': [{'name': ''}]}\n",
            "{'countries': [{'name': 'France'}]}\n",
            "{'countries': [{'name': 'France', 'population': 67}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {'name': ''}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': ''}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan'}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125584}]}\n",
            "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125584000}]}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "chain = (\n",
        "    model | JsonOutputParser()\n",
        ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
        "async for text in chain.astream(\n",
        "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
        "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
        "    \"Each country should have the key `name` and `population`\"\n",
        "):\n",
        "    print(text, flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "151d4323-a6cf-49be-8779-e8797c5e3b00",
      "metadata": {},
      "source": [
        "现在，我们来**解析**流式处理。我们将使用之前的示例，并在末尾附加一个提取函数，该函数从最终确定的 JSON 中提取国家名称。\n\n:::warning\n链中任何在**最终输入**上操作而不是在**输入流**上操作的步骤，都可能通过 `stream` 或 `astream` 破坏流式处理功能。\n:::\n\n:::tip\n稍后我们将讨论 `astream_events` API，它会流式传输中间步骤的结果。即使链包含仅在**最终输入**上操作的步骤，该 API 也会从中间步骤流式传输结果。\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d9c90117-9faa-4a01-b484-0db071808d1f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['France', 'Spain', 'Japan']|"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import (\n",
        "    JsonOutputParser,\n",
        ")\n",
        "\n",
        "\n",
        "# A function that operates on finalized inputs\n",
        "# rather than on an input_stream\n",
        "def _extract_country_names(inputs):\n",
        "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
        "    if not isinstance(inputs, dict):\n",
        "        return \"\"\n",
        "\n",
        "    if \"countries\" not in inputs:\n",
        "        return \"\"\n",
        "\n",
        "    countries = inputs[\"countries\"]\n",
        "\n",
        "    if not isinstance(countries, list):\n",
        "        return \"\"\n",
        "\n",
        "    country_names = [\n",
        "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
        "    ]\n",
        "    return country_names\n",
        "\n",
        "\n",
        "chain = model | JsonOutputParser() | _extract_country_names\n",
        "\n",
        "async for text in chain.astream(\n",
        "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
        "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
        "    \"Each country should have the key `name` and `population`\"\n",
        "):\n",
        "    print(text, end=\"|\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cab6dca2-2027-414d-a196-2db6e3ebb8a5",
      "metadata": {},
      "source": [
        "#### 生成器函数\n\n让我们使用一个可以操作**输入流**的生成器函数来修复流式传输。\n\n:::tip\n生成器函数（即使用 `yield` 的函数）允许编写操作**输入流**的代码。\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "15984b2b-315a-4119-945b-2a3dabea3082",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "France|Spain|Japan|"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "\n",
        "async def _extract_country_names_streaming(input_stream):\n",
        "    \"\"\"A function that operates on input streams.\"\"\"\n",
        "    country_names_so_far = set()\n",
        "\n",
        "    async for input in input_stream:\n",
        "        if not isinstance(input, dict):\n",
        "            continue\n",
        "\n",
        "        if \"countries\" not in input:\n",
        "            continue\n",
        "\n",
        "        countries = input[\"countries\"]\n",
        "\n",
        "        if not isinstance(countries, list):\n",
        "            continue\n",
        "\n",
        "        for country in countries:\n",
        "            name = country.get(\"name\")\n",
        "            if not name:\n",
        "                continue\n",
        "            if name not in country_names_so_far:\n",
        "                yield name\n",
        "                country_names_so_far.add(name)\n",
        "\n",
        "\n",
        "chain = model | JsonOutputParser() | _extract_country_names_streaming\n",
        "\n",
        "async for text in chain.astream(\n",
        "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
        "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
        "    \"Each country should have the key `name` and `population`\",\n",
        "):\n",
        "    print(text, end=\"|\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d59823f5-9b9a-43c5-a213-34644e2f1d3d",
      "metadata": {},
      "source": [
        ":::note\n由于上面的代码依赖于 JSON 自动补全，您可能会看到国家/地区的名称不完整（例如 `Sp` 和 `Spain`），这并不是您在提取结果中想要的结果！\n\n我们的重点是流式处理概念，而不是链式处理的结果。\n:::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6adf65b7-aa47-4321-98c7-a0abe43b833a",
      "metadata": {},
      "source": [
        "### 非流式组件\n\n诸如 `Retrievers` 之类的某些内置组件不提供任何 `streaming`。如果我们尝试 `stream` 它们会发生什么？🤨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b9b1c00d-8b44-40d0-9e2b-8a70d238f82b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[Document(page_content='harrison worked at kensho'),\n",
              "  Document(page_content='harrison likes spicy food')]]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\"harrison worked at kensho\", \"harrison likes spicy food\"],\n",
        "    embedding=OpenAIEmbeddings(),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\n",
        "chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd3e71b-439e-418f-8a8a-5232fba3d9fd",
      "metadata": {},
      "source": [
        "Stream 刚刚返回了来自该组件的最终结果。\n\n这样是没问题的 🥹！并非所有组件都必须实现流式输出——在某些情况下，流式输出可能不必要、困难或者根本就不适用。\n\n:::tip\n即使是使用非流式组件构建的 LCEL 链，在很多情况下仍然能够实现流式输出，流式输出的部分结果会在链中的最后一个非流式组件之后开始。\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "957447e6-1e60-41ef-8c10-2654bd9e738d",
      "metadata": {},
      "outputs": [],
      "source": [
        "retrieval_chain = (\n",
        "    {\n",
        "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
        "        \"question\": RunnablePassthrough(),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "94e50b5d-bf51-4eee-9da0-ee40dd9ce42b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base|d on| the| given| context|,| Harrison| worke|d at| K|ens|ho|.|\n",
            "\n",
            "Here| are| |3| |made| up| sentences| about| this| place|:|\n",
            "\n",
            "1|.| K|ens|ho| was| a| cutting|-|edge| technology| company| known| for| its| innovative| solutions| in| artificial| intelligence| an|d data| analytics|.|\n",
            "\n",
            "2|.| The| modern| office| space| at| K|ens|ho| feature|d open| floor| plans|,| collaborative| work|sp|aces|,| an|d a| vib|rant| atmosphere| that| fos|tere|d creativity| an|d team|work|.|\n",
            "\n",
            "3|.| With| its| prime| location| in| the| heart| of| the| city|,| K|ens|ho| attracte|d top| talent| from| aroun|d the| worl|d,| creating| a| diverse| an|d dynamic| work| environment|.|"
          ]
        }
      ],
      "source": [
        "for chunk in retrieval_chain.stream(\n",
        "    \"Where did harrison work? \" \"Write 3 made up sentences about this place.\"\n",
        "):\n",
        "    print(chunk, end=\"|\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8657aa4e-3469-4b5b-a09c-60b53a23b1e7",
      "metadata": {},
      "source": [
        "现在我们已经了解了 `stream` 和 `astream` 的工作原理，让我们一起探索流式事件的世界吧。🏞️"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baceb5c0-d4a4-4b98-8733-80ae4407b62d",
      "metadata": {},
      "source": [
        "## 使用 Stream Events\n\n事件流（Event Streaming）是一个 **beta** API。此 API 可能会根据反馈进行一些更改。\n\n:::note\n\n本指南演示了 `V2` API，并且需要 langchain-core >= 0.2。有关与旧版本 LangChain 兼容的 `V1` API，请参阅 [此处](https://python.langchain.com/v0.1/docs/expression_language/streaming/#using-stream-events)。\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61348df9-ec58-401e-be89-68a70042f88e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import langchain_core\n",
        "\n",
        "langchain_core.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52e9e983-bbde-4906-9eca-4ccc06eabd91",
      "metadata": {},
      "source": [
        "为了使 `astream_events` API 正常工作：\n\n* 尽可能在整个代码中使用 `async`（例如，async 工具等）\n* 如果定义了自定义函数/runnables，请传播回调\n* 无论何时在没有 LCEL 的情况下使用 runnables，请确保对 LLM 调用 `.astream()` 而不是 `.ainvoke`，以强制 LLM 流式传输 token。\n* 如果有任何不符合预期的情况，请告知我们！ :)\n\n### 事件参考\n\n下表展示了可能由各种 Runnable 对象发出的某些事件。\n\n:::note\n当流式传输正确实现时，runnable 的输入在输入流完全消耗后才能知道。这意味着 `inputs` 通常只包含在 `end` 事件中，而不是 `start` 事件中。\n:::\n\n| event                | name             | chunk                           | input                                         | output                                          |\n|----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|\n| on_chat_model_start  | [model name]     |                                 | \\{\"messages\": [[SystemMessage, HumanMessage]]\\} |                                                 |\n| on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n| on_chat_model_end    | [model name]     |                                 | \\{\"messages\": [[SystemMessage, HumanMessage]]\\} | AIMessageChunk(content=\"hello world\")           |\n| on_llm_start         | [model name]     |                                 | \\{'input': 'hello'\\}                            |                                                 |\n| on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n| on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n| on_chain_start       | format_docs      |                                 |                                               |                                                 |\n| on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n| on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n| on_tool_start        | some_tool        |                                 | \\{\"x\": 1, \"y\": \"2\"\\}                            |                                                 |\n| on_tool_end          | some_tool        |                                 |                                               | \\{\"x\": 1, \"y\": \"2\"\\}                              |\n| on_retriever_start   | [retriever name] |                                 | \\{\"query\": \"hello\"\\}                            |                                                 |\n| on_retriever_end     | [retriever name] |                                 | \\{\"query\": \"hello\"\\}                            | [Document(...), ..]                             |\n| on_prompt_start      | [template_name]  |                                 | \\{\"question\": \"hello\"\\}                         |                                                 |\n| on_prompt_end        | [template_name]  |                                 | \\{\"question\": \"hello\"\\}                         | ChatPromptValue(messages: [SystemMessage, ...]) |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f6ec135-3348-4041-8f55-bf3e59b3b2d0",
      "metadata": {},
      "source": [
        "### 聊天模型\n\n让我们先来看看聊天模型产生的事件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bab5f910-fee0-4a94-9f05-b469006333b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "events = []\n",
        "async for event in model.astream_events(\"hello\"):\n",
        "    events.append(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32972939-2995-4b2e-84db-045adb044fad",
      "metadata": {},
      "source": [
        ":::note\n\n对于 `langchain-core<0.3.37`，请显式设置 `version` 关键字参数（例如 `model.astream_events(\"hello\", version=\"v2\")`）。\n\n:::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad2b8f47-da78-4569-a49a-53a8efaa26bc",
      "metadata": {},
      "source": [
        "让我们来看几个开始事件和几个结束事件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c4a2f5dc-2c75-4be4-a8ca-b5b84a3cdbef",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'event': 'on_chat_model_start',\n",
              "  'data': {'input': 'hello'},\n",
              "  'name': 'ChatAnthropic',\n",
              "  'tags': [],\n",
              "  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',\n",
              "  'metadata': {'ls_provider': 'anthropic',\n",
              "   'ls_model_name': 'claude-3-sonnet-20240229',\n",
              "   'ls_model_type': 'chat',\n",
              "   'ls_temperature': 0.0,\n",
              "   'ls_max_tokens': 1024},\n",
              "  'parent_ids': []},\n",
              " {'event': 'on_chat_model_stream',\n",
              "  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',\n",
              "  'name': 'ChatAnthropic',\n",
              "  'tags': [],\n",
              "  'metadata': {'ls_provider': 'anthropic',\n",
              "   'ls_model_name': 'claude-3-sonnet-20240229',\n",
              "   'ls_model_type': 'chat',\n",
              "   'ls_temperature': 0.0,\n",
              "   'ls_max_tokens': 1024},\n",
              "  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 8, 'output_tokens': 4, 'total_tokens': 12, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},\n",
              "  'parent_ids': []},\n",
              " {'event': 'on_chat_model_stream',\n",
              "  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',\n",
              "  'name': 'ChatAnthropic',\n",
              "  'tags': [],\n",
              "  'metadata': {'ls_provider': 'anthropic',\n",
              "   'ls_model_name': 'claude-3-sonnet-20240229',\n",
              "   'ls_model_type': 'chat',\n",
              "   'ls_temperature': 0.0,\n",
              "   'ls_max_tokens': 1024},\n",
              "  'data': {'chunk': AIMessageChunk(content='Hello! How can', additional_kwargs={}, response_metadata={}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66')},\n",
              "  'parent_ids': []}]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "events[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "76cfe826-ee63-4310-ad48-55a95eb3b9d6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'event': 'on_chat_model_stream',\n",
              "  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',\n",
              "  'name': 'ChatAnthropic',\n",
              "  'tags': [],\n",
              "  'metadata': {'ls_provider': 'anthropic',\n",
              "   'ls_model_name': 'claude-3-sonnet-20240229',\n",
              "   'ls_model_type': 'chat',\n",
              "   'ls_temperature': 0.0,\n",
              "   'ls_max_tokens': 1024},\n",
              "  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 0, 'output_tokens': 12, 'total_tokens': 12, 'input_token_details': {}})},\n",
              "  'parent_ids': []},\n",
              " {'event': 'on_chat_model_end',\n",
              "  'data': {'output': AIMessageChunk(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 8, 'output_tokens': 16, 'total_tokens': 24, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},\n",
              "  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',\n",
              "  'name': 'ChatAnthropic',\n",
              "  'tags': [],\n",
              "  'metadata': {'ls_provider': 'anthropic',\n",
              "   'ls_model_name': 'claude-3-sonnet-20240229',\n",
              "   'ls_model_type': 'chat',\n",
              "   'ls_temperature': 0.0,\n",
              "   'ls_max_tokens': 1024},\n",
              "  'parent_ids': []}]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "events[-2:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c8f173-e9c7-4c27-81a5-b7c85c12714d",
      "metadata": {},
      "source": [
        "### Chain\n\n让我们重新审视解析流式 JSON 的示例链，以探索流式事件 API。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4328c56c-a303-427b-b1f2-f354e9af555c",
      "metadata": {},
      "outputs": [],
      "source": [
        "chain = (\n",
        "    model | JsonOutputParser()\n",
        ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
        "\n",
        "events = [\n",
        "    event\n",
        "    async for event in chain.astream_events(\n",
        "        \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
        "        'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
        "        \"Each country should have the key `name` and `population`\",\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cc00b99-a961-4221-a3c7-9d807114bbfb",
      "metadata": {},
      "source": [
        "如果你查看最初的几个事件，你会注意到有 **3** 个不同的开始事件，而不是 **2** 个开始事件。\n\n这三个开始事件分别对应：\n\n1. 链（模型 + 解析器）\n2. 模型\n3. 解析器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8e66ea3d-a450-436a-aaac-d9478abc6c28",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'event': 'on_chain_start',\n",
              "  'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'},\n",
              "  'name': 'RunnableSequence',\n",
              "  'tags': [],\n",
              "  'run_id': '4765006b-16e2-4b1d-a523-edd9fd64cb92',\n",
              "  'metadata': {}},\n",
              " {'event': 'on_chat_model_start',\n",
              "  'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`')]]}},\n",
              "  'name': 'ChatAnthropic',\n",
              "  'tags': ['seq:step:1'],\n",
              "  'run_id': '0320c234-7b52-4a14-ae4e-5f100949e589',\n",
              "  'metadata': {}},\n",
              " {'event': 'on_chat_model_stream',\n",
              "  'data': {'chunk': AIMessageChunk(content='{', id='run-0320c234-7b52-4a14-ae4e-5f100949e589')},\n",
              "  'run_id': '0320c234-7b52-4a14-ae4e-5f100949e589',\n",
              "  'name': 'ChatAnthropic',\n",
              "  'tags': ['seq:step:1'],\n",
              "  'metadata': {}}]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "events[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8512238-d035-4acd-9248-a8570da064c9",
      "metadata": {},
      "source": [
        "如果查看最后 3 个事件，你会看到什么？中间的呢？"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c742cfa4-9b03-4a5b-96d9-5fe56e95e3b4",
      "metadata": {},
      "source": [
        "让我们使用这个 API 来输出模型的流事件和解析器的流事件。我们忽略开始事件、结束事件以及来自链的事件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "630c71d6-8d94-4ce0-a78a-f20e90f628df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat model chunk: ''\n",
            "Chat model chunk: '{'\n",
            "Parser chunk: {}\n",
            "Chat model chunk: '\\n  \"countries'\n",
            "Chat model chunk: '\": [\\n    '\n",
            "Parser chunk: {'countries': []}\n",
            "Chat model chunk: '{\\n      \"'\n",
            "Parser chunk: {'countries': [{}]}\n",
            "Chat model chunk: 'name\": \"France'\n",
            "Parser chunk: {'countries': [{'name': 'France'}]}\n",
            "Chat model chunk: '\",\\n      \"'\n",
            "Chat model chunk: 'population\": 67'\n",
            "Parser chunk: {'countries': [{'name': 'France', 'population': 67}]}\n",
            "Chat model chunk: '413'\n",
            "Parser chunk: {'countries': [{'name': 'France', 'population': 67413}]}\n",
            "Chat model chunk: '000\\n    },'\n",
            "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}]}\n",
            "Chat model chunk: '\\n    {'\n",
            "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {}]}\n",
            "Chat model chunk: '\\n      \"name\":'\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "num_events = 0\n",
        "\n",
        "async for event in chain.astream_events(\n",
        "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
        "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
        "    \"Each country should have the key `name` and `population`\",\n",
        "):\n",
        "    kind = event[\"event\"]\n",
        "    if kind == \"on_chat_model_stream\":\n",
        "        print(\n",
        "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n",
        "            flush=True,\n",
        "        )\n",
        "    if kind == \"on_parser_stream\":\n",
        "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n",
        "    num_events += 1\n",
        "    if num_events > 30:\n",
        "        # Truncate the output\n",
        "        print(\"...\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798ea891-997c-454c-bf60-43124f40ee1b",
      "metadata": {},
      "source": [
        "因为模型和解析器都支持流式传输，所以我们可以实时地从这两个组件中看到流式传输的事件！是不是很酷？🦜"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5084148b-bcdc-4373-9caa-6568f03e7b23",
      "metadata": {},
      "source": [
        "### 过滤事件\n\n由于此 API 会产生大量事件，因此能够对事件进行过滤非常有用。\n\n您可以按组件 `name`、组件 `tags` 或组件 `type` 进行过滤。\n\n#### 按名称"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "42145735-25e8-4e67-b081-b0c15ea45dd1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'event': 'on_parser_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'my_parser', 'tags': ['seq:step:2'], 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'metadata': {}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n",
            "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n",
            "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n",
            "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n",
            "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n",
            "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n",
            "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n",
            "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n",
            "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n",
            "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n",
            "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n",
        "    {\"run_name\": \"my_parser\"}\n",
        ")\n",
        "\n",
        "max_events = 0\n",
        "async for event in chain.astream_events(\n",
        "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
        "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
        "    \"Each country should have the key `name` and `population`\",\n",
        "    include_names=[\"my_parser\"],\n",
        "):\n",
        "    print(event)\n",
        "    max_events += 1\n",
        "    if max_events > 10:\n",
        "        # Truncate output\n",
        "        print(\"...\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c59d5626-7dba-4eb3-ad81-76c1092c5146",
      "metadata": {},
      "source": [
        "#### 按类型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2a7d8fe0-47ca-4ab4-9c10-b34e3f6106ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'event': 'on_chat_model_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'model', 'tags': ['seq:step:1'], 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c', usage_metadata={'input_tokens': 56, 'output_tokens': 1, 'total_tokens': 57, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n  \"countries', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\": [\\n    ', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{\\n      \"', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='name\": \"France', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\",\\n      \"', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='population\": 67', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='413', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='000\\n    },', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n",
        "    {\"run_name\": \"my_parser\"}\n",
        ")\n",
        "\n",
        "max_events = 0\n",
        "async for event in chain.astream_events(\n",
        "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
        "    include_types=[\"chat_model\"],\n",
        "):\n",
        "    print(event)\n",
        "    max_events += 1\n",
        "    if max_events > 10:\n",
        "        # Truncate output\n",
        "        print(\"...\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1ec8dd4-9b5b-4000-b63f-5845bfc5a065",
      "metadata": {},
      "source": [
        "#### 按标签\n\n:::caution\n\n标签会由可运行组件的子组件继承。\n\n如果你使用标签进行过滤，请确保这是你想要的结果。\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c237c218-5fd6-4146-ac68-020a038cf582",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'event': 'on_chain_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'RunnableSequence', 'tags': ['my_chain'], 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'metadata': {}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}}, 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b', usage_metadata={'input_tokens': 56, 'output_tokens': 1, 'total_tokens': 57, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n",
            "{'event': 'on_parser_start', 'data': {}, 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'metadata': {}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n",
            "{'event': 'on_parser_stream', 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n",
            "{'event': 'on_chain_stream', 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': []}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n  \"countries', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n",
            "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\": [\\n    ', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n",
            "{'event': 'on_parser_stream', 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n",
            "{'event': 'on_chain_stream', 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': []}\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "chain = (model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})\n",
        "\n",
        "max_events = 0\n",
        "async for event in chain.astream_events(\n",
        "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
        "    include_tags=[\"my_chain\"],\n",
        "):\n",
        "    print(event)\n",
        "    max_events += 1\n",
        "    if max_events > 10:\n",
        "        # Truncate output\n",
        "        print(\"...\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e05e54c4-61a2-4f6c-aa68-d2b09b5e1d4f",
      "metadata": {},
      "source": [
        "### 非流式组件\n\n还记得有些组件为什么不能很好地流式传输，因为它们不操作 **输入流** 吗？\n\n虽然这类组件在使用 `astream` 时会破坏最终输出的流式传输，但 `astream_events` 仍将从支持流式传输的中间步骤中产生流式事件！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "0e6451d3-3b11-4a71-ae19-998f4c10180f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function that does not support streaming.\n",
        "# It operates on the finalizes inputs rather than\n",
        "# operating on the input stream.\n",
        "def _extract_country_names(inputs):\n",
        "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
        "    if not isinstance(inputs, dict):\n",
        "        return \"\"\n",
        "\n",
        "    if \"countries\" not in inputs:\n",
        "        return \"\"\n",
        "\n",
        "    countries = inputs[\"countries\"]\n",
        "\n",
        "    if not isinstance(countries, list):\n",
        "        return \"\"\n",
        "\n",
        "    country_names = [\n",
        "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
        "    ]\n",
        "    return country_names\n",
        "\n",
        "\n",
        "chain = (\n",
        "    model | JsonOutputParser() | _extract_country_names\n",
        ")  # This parser only works with OpenAI right now"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a972e1a6-80cd-4d59-90a0-73563f1503d4",
      "metadata": {},
      "source": [
        "正如预期，`astream` API 工作不正常，因为 `_extract_country_names` 不在流上操作。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f9a8fe35-faab-4970-b8c0-5c780845d98a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['France', 'Spain', 'Japan']\n"
          ]
        }
      ],
      "source": [
        "async for chunk in chain.astream(\n",
        "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
        "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
        "    \"Each country should have the key `name` and `population`\",\n",
        "):\n",
        "    print(chunk, flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b279ea33-54f1-400a-acb1-b8445ccbf1fa",
      "metadata": {},
      "source": [
        "现在，让我们通过 `stream_events` 来确认我们是否仍然看到来自模型和解析器的流式输出。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2c83701e-b801-429f-b2ac-47ed44d2d11a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat model chunk: ''\n",
            "Chat model chunk: '{'\n",
            "Parser chunk: {}\n",
            "Chat model chunk: '\\n  \"countries'\n",
            "Chat model chunk: '\": [\\n    '\n",
            "Parser chunk: {'countries': []}\n",
            "Chat model chunk: '{\\n      \"'\n",
            "Parser chunk: {'countries': [{}]}\n",
            "Chat model chunk: 'name\": \"France'\n",
            "Parser chunk: {'countries': [{'name': 'France'}]}\n",
            "Chat model chunk: '\",\\n      \"'\n",
            "Chat model chunk: 'population\": 67'\n",
            "Parser chunk: {'countries': [{'name': 'France', 'population': 67}]}\n",
            "Chat model chunk: '413'\n",
            "Parser chunk: {'countries': [{'name': 'France', 'population': 67413}]}\n",
            "Chat model chunk: '000\\n    },'\n",
            "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}]}\n",
            "Chat model chunk: '\\n    {'\n",
            "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {}]}\n",
            "Chat model chunk: '\\n      \"name\":'\n",
            "Chat model chunk: ' \"Spain\",'\n",
            "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}\n",
            "Chat model chunk: '\\n      \"population\":'\n",
            "Chat model chunk: ' 47'\n",
            "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}\n",
            "Chat model chunk: '351'\n",
            "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351}]}\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "num_events = 0\n",
        "\n",
        "async for event in chain.astream_events(\n",
        "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
        "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
        "    \"Each country should have the key `name` and `population`\",\n",
        "):\n",
        "    kind = event[\"event\"]\n",
        "    if kind == \"on_chat_model_stream\":\n",
        "        print(\n",
        "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n",
        "            flush=True,\n",
        "        )\n",
        "    if kind == \"on_parser_stream\":\n",
        "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n",
        "    num_events += 1\n",
        "    if num_events > 30:\n",
        "        # Truncate the output\n",
        "        print(\"...\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e91bdd3-f4a3-4b3c-b21a-26365c6c1566",
      "metadata": {},
      "source": [
        "---\n### 传播回调\n\n:::caution\n如果您在工具中使用调用可运行对象（runnables），则需要将回调传播到该可运行对象；否则，将不会生成任何流事件。\n:::\n\n:::note\n当使用 `RunnableLambdas` 或 `@chain` 装饰器时，回调会在后台自动传播。\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1854206d-b3a5-4f91-9e00-bccbaebac61f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'bad_tool', 'tags': [], 'run_id': 'ea900472-a8f7-425d-b627-facdef936ee8', 'metadata': {}}\n",
            "{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '77b01284-0515-48f4-8d7c-eb27c1882f86', 'metadata': {}}\n",
            "{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '77b01284-0515-48f4-8d7c-eb27c1882f86', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n",
            "{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'ea900472-a8f7-425d-b627-facdef936ee8', 'name': 'bad_tool', 'tags': [], 'metadata': {}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "def reverse_word(word: str):\n",
        "    return word[::-1]\n",
        "\n",
        "\n",
        "reverse_word = RunnableLambda(reverse_word)\n",
        "\n",
        "\n",
        "@tool\n",
        "def bad_tool(word: str):\n",
        "    \"\"\"Custom tool that doesn't propagate callbacks.\"\"\"\n",
        "    return reverse_word.invoke(word)\n",
        "\n",
        "\n",
        "async for event in bad_tool.astream_events(\"hello\"):\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23e68a99-7886-465b-8575-116022857469",
      "metadata": {},
      "source": [
        "这是一个重新实现的版本，它能正确地传播回调。你会注意到，现在我们也能从 `reverse_word` runnable 中获得事件了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a20a6cb3-bb43-465c-8cfc-0a7349d70968",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'correct_tool', 'tags': [], 'run_id': 'd5ea83b9-9278-49cc-9f1d-aa302d671040', 'metadata': {}}\n",
            "{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '44dafbf4-2f87-412b-ae0e-9f71713810df', 'metadata': {}}\n",
            "{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '44dafbf4-2f87-412b-ae0e-9f71713810df', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n",
            "{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'd5ea83b9-9278-49cc-9f1d-aa302d671040', 'name': 'correct_tool', 'tags': [], 'metadata': {}}\n"
          ]
        }
      ],
      "source": [
        "@tool\n",
        "def correct_tool(word: str, callbacks):\n",
        "    \"\"\"A tool that correctly propagates callbacks.\"\"\"\n",
        "    return reverse_word.invoke(word, {\"callbacks\": callbacks})\n",
        "\n",
        "\n",
        "async for event in correct_tool.astream_events(\"hello\"):\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "640daa94-e4fe-4997-ab6e-45120f18b9ee",
      "metadata": {},
      "source": [
        "如果您在 Runnable Lambdas 或 `@chains` 中调用 runnables，则会自动为您传递 callbacks。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "0ac0a3c1-f3a4-4157-b053-4fec8d2e698c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'metadata': {}}\n",
            "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': '5cf26fc8-840b-4642-98ed-623dda28707a', 'metadata': {}}\n",
            "{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': '5cf26fc8-840b-4642-98ed-623dda28707a', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n",
            "{'event': 'on_chain_stream', 'data': {'chunk': '43214321'}, 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n",
            "{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "async def reverse_and_double(word: str):\n",
        "    return await reverse_word.ainvoke(word) * 2\n",
        "\n",
        "\n",
        "reverse_and_double = RunnableLambda(reverse_and_double)\n",
        "\n",
        "await reverse_and_double.ainvoke(\"1234\")\n",
        "\n",
        "async for event in reverse_and_double.astream_events(\"1234\"):\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35a34268-9b3d-4857-b4ed-65d95f4a1293",
      "metadata": {},
      "source": [
        "再配合 `@chain` 装饰器："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c896bb94-9d10-41ff-8fe2-d6b05b1ed74b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'metadata': {}}\n",
            "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': '64fc99f0-5d7d-442b-b4f5-4537129f67d1', 'metadata': {}}\n",
            "{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': '64fc99f0-5d7d-442b-b4f5-4537129f67d1', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n",
            "{'event': 'on_chain_stream', 'data': {'chunk': '43214321'}, 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n",
            "{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import chain\n",
        "\n",
        "\n",
        "@chain\n",
        "async def reverse_and_double(word: str):\n",
        "    return await reverse_word.ainvoke(word) * 2\n",
        "\n",
        "\n",
        "await reverse_and_double.ainvoke(\"1234\")\n",
        "\n",
        "async for event in reverse_and_double.astream_events(\"1234\"):\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3efcd9",
      "metadata": {},
      "source": [
        "## 后续步骤\n\n现在您已经了解了使用 LangChain 串流最终输出和内部步骤的几种方法。\n\n要了解更多信息，请查看本节中的其他操作指南，或关于 Langchain Expression Language 的[概念指南](/docs/concepts/lcel/)。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}