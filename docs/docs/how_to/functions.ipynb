{
  "cells": [
    {
      "cell_type": "raw",
      "id": "ce0e08fd",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_position: 3\n",
        "keywords: [RunnableLambda, LCEL]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc4bf6e",
      "metadata": {},
      "source": [
        "# 如何运行自定义函数\n\n:::info 先决条件\n\n本指南假定您熟悉以下概念：\n- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)\n- [链接 runnables](/docs/how_to/sequence/)\n\n:::\n\n您可以将任意函数用作 [Runnables](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)。这对于格式化或当您需要 LangChain 组件未提供的功能时非常有用，而用作 Runnables 的自定义函数被称为 [`RunnableLambdas`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)。\n\n请注意，传递给这些函数的所有输入都必须是单个参数。如果您有一个接受多个参数的函数，则应编写一个包装器，该包装器接受单个字典输入并将其解包为多个参数。\n\n本指南将涵盖：\n\n- 如何使用 `RunnableLambda` 构造函数和便捷的 `@chain` 装饰器显式地从自定义函数创建 runnable\n- 在链中使用时将自定义函数强制转换为 runnables\n- 如何在自定义函数中接受和使用运行元数据\n- 如何通过让自定义函数返回生成器来实现流式处理\n\n## 使用构造函数\n\n下面，我们使用 `RunnableLambda` 构造函数显式地包装我们的自定义逻辑："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c34d2af",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU langchain langchain_openai\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6bb221b3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='3 + 9 equals 12.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-73728de3-e483-49e3-ad54-51bd9570e71a-0')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "def length_function(text):\n",
        "    return len(text)\n",
        "\n",
        "\n",
        "def _multiple_length_function(text1, text2):\n",
        "    return len(text1) * len(text2)\n",
        "\n",
        "\n",
        "def multiple_length_function(_dict):\n",
        "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
        "\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
        "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
        "        | RunnableLambda(multiple_length_function),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        ")\n",
        "\n",
        "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7926002",
      "metadata": {},
      "source": [
        "## `@chain` 装饰器的便捷用法\n\n你也可以通过添加 `@chain` 装饰器将任意函数转换为可链式调用。这在功能上等同于以上所示的用 `RunnableLambda` 构造函数包装函数。下面是一个例子："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3142a516",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The subject of the joke is the bear and his girlfriend.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "prompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")\n",
        "\n",
        "\n",
        "@chain\n",
        "def custom_chain(text):\n",
        "    prompt_val1 = prompt1.invoke({\"topic\": text})\n",
        "    output1 = ChatOpenAI().invoke(prompt_val1)\n",
        "    parsed_output1 = StrOutputParser().invoke(output1)\n",
        "    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()\n",
        "    return chain2.invoke({\"joke\": parsed_output1})\n",
        "\n",
        "\n",
        "custom_chain.invoke(\"bears\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4728ddd9-914d-42ce-ae9b-72c9ce8ec940",
      "metadata": {},
      "source": [
        "上面，我们使用 `@chain` 装饰器将 `custom_chain` 转换为一个可运行对象，并通过 `.invoke()` 方法调用它。\n\n如果你正在使用 [LangSmith](https://docs.smith.langchain.com/) 进行追踪，你应该能在其中看到一个 `custom_chain` 追踪记录，其中嵌套着对 OpenAI 的调用。\n\n## 链中的自动类型强制转换\n\n在链中使用自定义函数并结合管道操作符 (`|`) 时，你可以省略 `RunnableLambda` 或 `@chain` 构造函数，并依赖类型强制转换。下面是一个简单示例，展示了一个函数如何接收模型的输出并返回其前五个字母："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5ab39a87",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Once '"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])\n",
        "\n",
        "chain_with_coerced_function.invoke({\"topic\": \"bears\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9a481d1",
      "metadata": {},
      "source": [
        "请注意，我们无需将自定义函数 `(lambda x: x.content[:5])` 包装在 `RunnableLambda` 构造函数中，因为管道运算符左侧的 `model` 本身就是 `Runnable`。自定义函数会被**强制转换为**一个 runnable。有关更多信息，请参阅 [此部分](/docs/how_to/sequence/#coercion)。\n\n## 传递运行元数据\n\nRunnable lambdas 可以选择接受一个 [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig) 参数，它们可以使用该参数将回调、标签和其他配置信息传递给嵌套的运行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ff0daf0c-49dd-4d21-9772-e5fa133c5f36",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'foo': 'bar'}\n",
            "Tokens Used: 62\n",
            "\tPrompt Tokens: 56\n",
            "\tCompletion Tokens: 6\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $9.6e-05\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "\n",
        "def parse_or_fix(text: str, config: RunnableConfig):\n",
        "    fixing_chain = (\n",
        "        ChatPromptTemplate.from_template(\n",
        "            \"Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\"\n",
        "            \" Don't narrate, just respond with the fixed data.\"\n",
        "        )\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    for _ in range(3):\n",
        "        try:\n",
        "            return json.loads(text)\n",
        "        except Exception as e:\n",
        "            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n",
        "    return \"Failed to parse\"\n",
        "\n",
        "\n",
        "from langchain_community.callbacks import get_openai_callback\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    output = RunnableLambda(parse_or_fix).invoke(\n",
        "        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n",
        "    )\n",
        "    print(output)\n",
        "    print(cb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1a5e709e-9d75-48c7-bb9c-503251990505",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'foo': 'bar'}\n",
            "Tokens Used: 62\n",
            "\tPrompt Tokens: 56\n",
            "\tCompletion Tokens: 6\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $9.6e-05\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.callbacks import get_openai_callback\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    output = RunnableLambda(parse_or_fix).invoke(\n",
        "        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n",
        "    )\n",
        "    print(output)\n",
        "    print(cb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922b48bd",
      "metadata": {},
      "source": [
        "## 流式处理\n\n:::note\n[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) 最适合不需要支持流式处理的代码。如果您需要支持流式处理（即能够处理输入块并产生输出块），请改用 [RunnableGenerator](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableGenerator.html)，如下例所示。\n:::\n\n您可以在链中使用生成器函数（即使用 `yield` 关键字的函数，其行为类似迭代器）。\n\n这些生成器的签名应该是 `Iterator[Input] -> Iterator[Output]`。或者对于异步生成器：`AsyncIterator[Input] -> AsyncIterator[Output]`。\n\n这些功能很有用：\n- 实现自定义输出解析器\n- 修改上一步的输出，同时保留流式处理能力\n\n以下是一个逗号分隔列表的自定义输出解析器示例。首先，我们创建一个生成此类列表作为文本的链："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "29f55c38",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lion, tiger, wolf, gorilla, panda"
          ]
        }
      ],
      "source": [
        "from typing import Iterator, List\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\"\n",
        ")\n",
        "\n",
        "str_chain = prompt | model | StrOutputParser()\n",
        "\n",
        "for chunk in str_chain.stream({\"animal\": \"bear\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46345323",
      "metadata": {},
      "source": [
        "接下来，我们定义一个自定义函数，它将聚合当前流式输出，并在模型生成列表中的下一个逗号时将其产生："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f08b8a5b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['lion']\n",
            "['tiger']\n",
            "['wolf']\n",
            "['gorilla']\n",
            "['raccoon']\n"
          ]
        }
      ],
      "source": [
        "# This is a custom parser that splits an iterator of llm tokens\n",
        "# into a list of strings separated by commas\n",
        "def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:\n",
        "    # hold partial input until we get a comma\n",
        "    buffer = \"\"\n",
        "    for chunk in input:\n",
        "        # add current chunk to buffer\n",
        "        buffer += chunk\n",
        "        # while there are commas in the buffer\n",
        "        while \",\" in buffer:\n",
        "            # split buffer on comma\n",
        "            comma_index = buffer.index(\",\")\n",
        "            # yield everything before the comma\n",
        "            yield [buffer[:comma_index].strip()]\n",
        "            # save the rest for the next iteration\n",
        "            buffer = buffer[comma_index + 1 :]\n",
        "    # yield the last chunk\n",
        "    yield [buffer.strip()]\n",
        "\n",
        "\n",
        "list_chain = str_chain | split_into_list\n",
        "\n",
        "for chunk in list_chain.stream({\"animal\": \"bear\"}):\n",
        "    print(chunk, flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a5adb69",
      "metadata": {},
      "source": [
        "调用它会得到一个完整的数值数组："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9ea4ddc6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['lion', 'tiger', 'wolf', 'gorilla', 'raccoon']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_chain.invoke({\"animal\": \"bear\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e320ed",
      "metadata": {},
      "source": [
        "## 异步版本\n\n如果你在 `async` 环境中工作，这里有一个上面示例的 `async` 版本："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "569dbbef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['lion']\n",
            "['tiger']\n",
            "['wolf']\n",
            "['gorilla']\n",
            "['panda']\n"
          ]
        }
      ],
      "source": [
        "from typing import AsyncIterator\n",
        "\n",
        "\n",
        "async def asplit_into_list(\n",
        "    input: AsyncIterator[str],\n",
        ") -> AsyncIterator[List[str]]:  # async def\n",
        "    buffer = \"\"\n",
        "    async for (\n",
        "        chunk\n",
        "    ) in input:  # `input` is a `async_generator` object, so use `async for`\n",
        "        buffer += chunk\n",
        "        while \",\" in buffer:\n",
        "            comma_index = buffer.index(\",\")\n",
        "            yield [buffer[:comma_index].strip()]\n",
        "            buffer = buffer[comma_index + 1 :]\n",
        "    yield [buffer.strip()]\n",
        "\n",
        "\n",
        "list_chain = str_chain | asplit_into_list\n",
        "\n",
        "async for chunk in list_chain.astream({\"animal\": \"bear\"}):\n",
        "    print(chunk, flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3a650482",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['lion', 'tiger', 'wolf', 'gorilla', 'panda']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await list_chain.ainvoke({\"animal\": \"bear\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3306ac3b",
      "metadata": {},
      "source": [
        "## 后续步骤\n\n您已经了解了在链中使用自定义逻辑和实现流送的几种不同方法。\n\n欲了解更多信息，请参阅本节中有关可运行对象的其他指南。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}