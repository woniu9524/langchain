{
  "cells": [
    {
      "cell_type": "raw",
      "id": "beba2e0e",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_position: 2\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb0735c0",
      "metadata": {},
      "source": [
        "# 如何在聊天模型中使用少样本示例\n\n:::info 先决条件\n\n本指南假定您已熟悉以下概念：\n- [Prompt 模板](/docs/concepts/prompt_templates)\n- [示例选择器](/docs/concepts/example_selectors)\n- [聊天模型](/docs/concepts/chat_models)\n- [向量存储](/docs/concepts/vectorstores)\n\n:::\n\n本指南介绍如何使用示例输入和输出来提示聊天模型。向模型提供几个这样的示例称为[少样本提示](/docs/concepts/few_shot_prompting/)，这是一种简单而强大的指导生成的方法，在某些情况下可以显著提高模型性能。\n\n对于如何进行少样本提示，似乎还没有达成广泛共识，并且最优的提示编译很可能因模型而异。因此，我们提供类似 [FewShotChatMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate.html?highlight=fewshot#langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate) 的少样本提示模板，将其作为一个灵活的起点，您可以根据需要对其进行修改或替换。\n\n少样本提示模板的目标是根据输入动态选择示例，然后将这些示例格式化到最终的提示中提供给模型。\n\n**注意：** 以下代码示例仅适用于聊天模型，因为 `FewShotChatMessagePromptTemplates` 被设计为输出格式化的[聊天消息](/docs/concepts/messages)，而不是纯文本字符串。对于与兼容完成模型 (LLM) 的纯文本模板类似的少样本提示示例，请参阅[少样本提示模板](/docs/how_to/few_shot_examples/)指南。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d716f2de-cc29-4823-9360-a808c7bfdb86",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 固定示例\n\n最基本（也是最常用）的少样本提示技术是使用固定提示示例。这样您就可以选择一个链，对其进行评估，而无需担心生产环境中的其他活动部件。\n\n模板的基本组成部分是：\n- `examples`: 要包含在最终提示中的字典示例列表。\n- `example_prompt`: 通过其 [`format_messages`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html?highlight=format_messages#langchain_core.prompts.chat.ChatPromptTemplate.format_messages) 方法将每个示例转换为一个或多个消息。常见的示例是将每个示例转换为一条人类消息和一条 AI 回复消息，或者是一条人类消息后跟一条函数调用消息。\n\n下面是一个简单的演示。首先，定义您希望包含的示例。让我们给 LLM 一个不熟悉的数学运算符，用“🦜”符号表示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5b79e400",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU langchain langchain-openai langchain-chroma\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30856d92",
      "metadata": {},
      "source": [
        "如果我们问模型这个表达式的结果是什么，它会失败："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "174dec5b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='The expression \"2 🦜 9\" is not a standard mathematical operation or equation. It appears to be a combination of the number 2 and the parrot emoji 🦜 followed by the number 9. It does not have a specific mathematical meaning.', response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 17, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-aad12dda-5c47-4a1e-9949-6fe94e03242a-0', usage_metadata={'input_tokens': 17, 'output_tokens': 54, 'total_tokens': 71})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "\n",
        "model.invoke(\"What is 2 🦜 9?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6d58385",
      "metadata": {},
      "source": [
        "现在我们来看看，如果我们给 LLM 一些示例来让它工作，会发生什么。我们将在下面定义一些示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0fc5a02a-6249-4e92-95c3-30fff9671e8b",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"2 🦜 2\", \"output\": \"4\"},\n",
        "    {\"input\": \"2 🦜 3\", \"output\": \"5\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8710ecc-2aa0-4172-a74c-250f6bc3d9e2",
      "metadata": {},
      "source": [
        "接下来，将它们组合成 few-shot prompt 模板。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "65e72ad1-9060-47d0-91a1-bc130c8b98ac",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HumanMessage(content='2 🦜 2'), AIMessage(content='4'), HumanMessage(content='2 🦜 3'), AIMessage(content='5')]\n"
          ]
        }
      ],
      "source": [
        "# This is a prompt template used to format each individual example.\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "print(few_shot_prompt.invoke({}).to_messages())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5490bd59-b28f-46a4-bbdf-0191802dd3c5",
      "metadata": {},
      "source": [
        "最后，我们组装最终的提示，如下所示，将 `few_shot_prompt` 直接传递给 `from_messages` 工厂方法，并将其与模型一起使用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9f86d6d9-50de-41b6-b6c7-0f9980cc0187",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd8029c5",
      "metadata": {},
      "source": [
        "现在，让我们向模型提出初始问题，看看它的表现如何："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "97d443b1-6fae-4b36-bede-3ff7306288a3",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='11', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 60, 'total_tokens': 61}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5ec4e051-262f-408e-ad00-3f2ebeb561c3-0', usage_metadata={'input_tokens': 60, 'output_tokens': 1, 'total_tokens': 61})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chain = final_prompt | model\n",
        "\n",
        "chain.invoke({\"input\": \"What is 2 🦜 9?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ab7114-f07f-46be-8874-3705a25aba5f",
      "metadata": {},
      "source": [
        "我们看到模型已经根据给定的少样本示例推断出鹦鹉表情符号代表加法！\n\n## 动态少样本提示\n\n有时，您可能希望根据输入仅从整体集合中选择少量示例进行展示。为此，您可以将传递给 `FewShotChatMessagePromptTemplate` 的 `examples` 替换为 `example_selector`。其他组件与上述相同！我们的动态少样本提示模板将如下所示：\n\n- `example_selector`：负责为给定输入选择少样本示例（以及返回它们的顺序）。这些实现了 [BaseExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.base.BaseExampleSelector.html?highlight=baseexampleselector#langchain_core.example_selectors.base.BaseExampleSelector) 接口。一个常见的例子是基于向量存储的 [SemanticSimilarityExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector.html?highlight=semanticsimilarityexampleselector#langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector)\n- `example_prompt`：通过其 [`format_messages`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html?highlight=chatprompttemplate#langchain_core.prompts.chat.ChatPromptTemplate.format_messages) 方法将每个示例转换为一个或多个消息。一个常见的例子是将每个示例转换为一条人类消息和一条 AI 回复消息，或者一条人类消息后跟一条函数调用消息。\n\n这些同样可以与其他消息和聊天模板组合，以组装您的最终提示。\n\n让我们通过一个使用 `SemanticSimilarityExampleSelector` 的示例进行探讨。由于此实现使用向量存储根据语义相似性选择示例，因此我们首先需要填充该存储。这里的基本思想是，我们希望搜索并返回与文本输入最相似的示例，因此我们嵌入提示示例的 `values` 而不是考虑键："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ad66f06a-66fd-4fcc-8166-5d0e3c801e57",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"2 🦜 2\", \"output\": \"4\"},\n",
        "    {\"input\": \"2 🦜 3\", \"output\": \"5\"},\n",
        "    {\"input\": \"2 🦜 4\", \"output\": \"6\"},\n",
        "    {\"input\": \"What did the cow say to the moon?\", \"output\": \"nothing at all\"},\n",
        "    {\n",
        "        \"input\": \"Write me a poem about the moon\",\n",
        "        \"output\": \"One for the moon, and one for me, who are we to talk about the moon?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f7e384a-2031-432b-951c-7ea8cf9262f1",
      "metadata": {},
      "source": [
        "### 创建 `example_selector`\n\n创建了 vectorstore 后，我们就可以创建 `example_selector`。在这里，我们将单独调用它，并设置其 `k` 值以仅检索最接近输入的两个示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7790303a-f722-452e-8921-b14bdf20bdff",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'input': 'What did the cow say to the moon?', 'output': 'nothing at all'},\n",
              " {'input': '2 🦜 4', 'output': '6'}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_selector = SemanticSimilarityExampleSelector(\n",
        "    vectorstore=vectorstore,\n",
        "    k=2,\n",
        ")\n",
        "\n",
        "# The prompt template will load examples by passing the input do the `select_examples` method\n",
        "example_selector.select_examples({\"input\": \"horse\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc77c40f-3f58-40a2-b757-a2a2ea43f24a",
      "metadata": {},
      "source": [
        "### 创建 Prompt 模板\n\n现在，我们使用上面创建的 `example_selector` 来组装 Prompt 模板。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "253c255e-41d7-45f6-9d88-c7a0ced4b1bd",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HumanMessage(content='2 🦜 3'), AIMessage(content='5'), HumanMessage(content='2 🦜 4'), AIMessage(content='6')]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "# Define the few-shot prompt.\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    # The input variables select the values to pass to the example_selector\n",
        "    input_variables=[\"input\"],\n",
        "    example_selector=example_selector,\n",
        "    # Define how each example will be formatted.\n",
        "    # In this case, each example will become 2 messages:\n",
        "    # 1 human, and 1 AI\n",
        "    example_prompt=ChatPromptTemplate.from_messages(\n",
        "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(few_shot_prompt.invoke(input=\"What's 3 🦜 3?\").to_messages())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "339cae7d-0eb0-44a6-852f-0267c5ff72b3",
      "metadata": {},
      "source": [
        "然后我们可以将这个few-shot聊天消息提示模板传递到另一个聊天提示模板中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e731cb45-f0ea-422c-be37-42af2a6cb2c4",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "messages=[HumanMessage(content='2 🦜 3'), AIMessage(content='5'), HumanMessage(content='2 🦜 4'), AIMessage(content='6')]\n"
          ]
        }
      ],
      "source": [
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(few_shot_prompt.invoke(input=\"What's 3 🦜 3?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2408ea69-1880-4ef5-a0fa-ffa8d2026aa9",
      "metadata": {},
      "source": [
        "### 与聊天模型配合使用\n\n最后，您可以将模型连接到少样本提示。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0568cbc6-5354-47f1-ab4d-dfcc616cf583",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='6', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 60, 'total_tokens': 61}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d1863e5e-17cd-4e9d-bf7a-b9f118747a65-0', usage_metadata={'input_tokens': 60, 'output_tokens': 1, 'total_tokens': 61})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = final_prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "\n",
        "chain.invoke({\"input\": \"What's 3 🦜 3?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c87fad3c",
      "metadata": {},
      "source": [
        "## 后续步骤\n\n您现在已经学会了如何在聊天提示中添加少样本示例。\n\n接下来，请查看本节中关于提示模板的其他操作指南，相关的关于使用文本补全模型进行少样本操作的操作指南 [few shotting with text completion models](/docs/how_to/few_shot_examples)，或者其他的 [示例选择器操作指南](/docs/how_to/example_selectors/)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e26b53",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}