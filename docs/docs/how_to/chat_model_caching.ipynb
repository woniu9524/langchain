{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dcf87b32",
      "metadata": {},
      "source": [
        "# 如何缓存聊天模型响应\n\n:::info 先决条件\n\n本指南假定您熟悉以下概念：\n- [模型聊天](/docs/concepts/chat_models)\n- [LLM](/docs/concepts/text_llms)\n\n:::\n\nLangChain 提供了一个可选的[模型聊天](/docs/concepts/chat_models)缓存层。这在两个主要方面很有用：\n\n- 如果您经常多次请求相同的完成，通过减少对 LLM 提供商的 API 调用次数来节省您的费用。这在应用程序开发过程中尤其有用。\n- 通过减少对 LLM 提供商的 API 调用次数来加快您的应用程序速度。\n\n本指南将引导您了解如何在应用程序中启用此功能。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289b31de",
      "metadata": {},
      "source": [
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n\n<ChatModelTabs customVarName=\"llm\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c6641f37",
      "metadata": {},
      "outputs": [],
      "source": [
        "# | output: false\n",
        "# | echo: false\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
        "\n",
        "llm = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5472a032",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <!-- ruff: noqa: F821 -->\n",
        "from langchain_core.globals import set_llm_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "357b89a8",
      "metadata": {},
      "source": [
        "## 内存缓存\n\n这是一个临时的缓存，它会将模型调用存储在内存中。当您的环境重启时，它将被清除，并且不会在进程之间共享。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "113e719a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 645 ms, sys: 214 ms, total: 859 ms\n",
            "Wall time: 829 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-b6836bdd-8c30-436b-828f-0ac5fc9ab50e-0')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "from langchain_core.caches import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a2121434",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 822 µs, sys: 288 µs, total: 1.11 ms\n",
            "Wall time: 1.06 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-b6836bdd-8c30-436b-828f-0ac5fc9ab50e-0')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# The second time it is, so it goes faster\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88ff8af",
      "metadata": {},
      "source": [
        "## SQLite 缓存\n\n此缓存实现使用 `SQLite` 数据库存储响应，并且能够跨进程重启保持数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "99290ab4",
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm .langchain.db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fe826c5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# We can do the same thing with a SQLite cache\n",
        "from langchain_community.cache import SQLiteCache\n",
        "\n",
        "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eb558734",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 9.91 ms, sys: 7.68 ms, total: 17.6 ms\n",
            "Wall time: 657 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-39d9e1e8-7766-4970-b1d8-f50213fd94c5-0')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "497c7000",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 52.2 ms, sys: 60.5 ms, total: 113 ms\n",
            "Wall time: 127 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', id='run-39d9e1e8-7766-4970-b1d8-f50213fd94c5-0')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# The second time it is, so it goes faster\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2950a913",
      "metadata": {},
      "source": [
        "## 下一步\n\n您现在已经学会了如何缓存模型响应以节省时间和金钱。\n\n接下来，请查看本节中聊天模型的其他操作指南，例如[如何让模型返回结构化输出](/docs/how_to/structured_output)或[如何创建自己的自定义聊天模型](/docs/how_to/custom_chat_model)。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}