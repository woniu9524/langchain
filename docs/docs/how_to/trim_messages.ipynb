{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eaad9a82-0592-4315-9931-0621054bdd0e",
      "metadata": {},
      "source": [
        "# 如何修剪消息\n\n:::info 先决条件\n\n本指南假设您熟悉以下概念：\n\n- [消息](/docs/concepts/messages)\n- [聊天模型](/docs/concepts/chat_models)\n- [链](/docs/how_to/sequence/)\n- [聊天历史记录](/docs/concepts/chat_history)\n\n本指南中的方法还需要 `langchain-core>=0.2.9`。\n\n:::\n\n所有模型都有有限的上下文窗口，这意味着它们可以接受的输入 [token](/docs/concepts/tokens/) 数量是有限的。如果您有非常长的消息，或者有一个积累了长消息历史的链/代理，那么您需要管理传递给模型的消息的长度。\n\n[trim_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) 可用于将聊天历史记录的大小减小到指定的 token 数量或指定的数量。\n\n\n如果直接将修剪后的聊天历史记录传递回聊天模型，修剪后的聊天历史记录应满足以下属性：\n\n1. 生成的聊天历史记录应 **有效**。这通常意味着应满足以下属性：\n   - 聊天历史记录 **以** `HumanMessage` **开头**，或者 **以** [SystemMessage](/docs/concepts/messages/#systemmessage) **开头后跟一个** `HumanMessage` **开头**。\n   - 聊天历史记录 **以** `HumanMessage` **或** `ToolMessage` **结尾**。\n   - `ToolMessage` 只能出现在涉及工具调用的 `AIMessage` 之后。\n\n   这可以通过设置 `start_on=\"human\"` 和 `ends_on=(\"human\", \"tool\")` 来实现。\n3. 它包括最近的消息并丢弃聊天历史记录中的旧消息。\n   这可以通过设置 `strategy=\"last\"` 来实现。\n4. 通常，新的聊天历史记录应包含 `SystemMessage`（如果\n   最初的聊天历史记录中存在的话），因为 `SystemMessage` 包含对聊天模型的特殊指令。如果存在，`SystemMessage` 几乎总是历史记录中的第一条消息。这可以通过设置 `include_system=True` 来实现。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4bffc37-78c0-46c3-ad0c-b44de0ed3e90",
      "metadata": {},
      "source": [
        "## 基于 Token 数量进行截断\n\n在此，我们将根据 Token 数量截断聊天记录。截断后的聊天记录将生成一个**有效**的聊天记录，其中包含 `SystemMessage`。\n\n为保留最近的消息，我们设置 `strategy=\"last\"`。我们还将设置 `include_system=True` 以包含 `SystemMessage`，并设置 `start_on=\"human\"` 以确保生成的聊天记录有效。\n\n这是使用 `trim_messages` 基于 Token 数量进行截断时的良好默认配置。请记住根据您的用例调整 `token_counter` 和 `max_tokens`。\n\n请注意，对于我们的 `token_counter`，可以传递一个函数（稍后将详细介绍）或一个语言模型（因为语言模型具有消息 Token 计数方法）。当您需要将消息截断以适应特定模型的上下文窗口时，传递相应的模型是有意义的："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9bed5ea-8aee-4d43-a717-77a431a02d2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "40ea972c-d424-4bc4-9f2e-82f01c3d7598",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    ToolMessage,\n",
        "    trim_messages,\n",
        ")\n",
        "from langchain_core.messages.utils import count_tokens_approximately\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
        "    HumanMessage(\"i wonder why it's called langchain\"),\n",
        "    AIMessage(\n",
        "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
        "    ),\n",
        "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
        "    AIMessage(\n",
        "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
        "    ),\n",
        "    HumanMessage(\"what do you call a speechless parrot\"),\n",
        "]\n",
        "\n",
        "\n",
        "trim_messages(\n",
        "    messages,\n",
        "    # Keep the last <= n_count tokens of the messages.\n",
        "    strategy=\"last\",\n",
        "    # highlight-start\n",
        "    # Remember to adjust based on your model\n",
        "    # or else pass a custom token_counter\n",
        "    token_counter=count_tokens_approximately,\n",
        "    # highlight-end\n",
        "    # Most chat models expect that chat history starts with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a SystemMessage followed by a HumanMessage\n",
        "    # highlight-start\n",
        "    # Remember to adjust based on the desired conversation\n",
        "    # length\n",
        "    max_tokens=45,\n",
        "    # highlight-end\n",
        "    # Most chat models expect that chat history starts with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a SystemMessage followed by a HumanMessage\n",
        "    start_on=\"human\",\n",
        "    # Most chat models expect that chat history ends with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a ToolMessage\n",
        "    end_on=(\"human\", \"tool\"),\n",
        "    # Usually, we want to keep the SystemMessage\n",
        "    # if it's present in the original history.\n",
        "    # The SystemMessage has special instructions for the model.\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28fcfc94-0d4a-415c-9506-8ae7634253a2",
      "metadata": {},
      "source": [
        "## 基于消息数量的截断\n\n另外，我们还可以通过设置 `token_counter=len` 来基于**消息数量**截断聊天记录。在这种情况下，每条消息将计为一个 token，而 `max_tokens` 将控制最大消息数量。\n\n这是使用 `trim_messages` 基于消息数量进行截断的一个良好默认配置。请记住根据你的具体用例调整 `max_tokens`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c8fdedae-0e6b-4901-a222-81fc95e265c2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='and who is harrison chasing anyways', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trim_messages(\n",
        "    messages,\n",
        "    # Keep the last <= n_count tokens of the messages.\n",
        "    strategy=\"last\",\n",
        "    # highlight-next-line\n",
        "    token_counter=len,\n",
        "    # When token_counter=len, each message\n",
        "    # will be counted as a single token.\n",
        "    # highlight-start\n",
        "    # Remember to adjust for your use case\n",
        "    max_tokens=5,\n",
        "    # highlight-end\n",
        "    # Most chat models expect that chat history starts with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a SystemMessage followed by a HumanMessage\n",
        "    start_on=\"human\",\n",
        "    # Most chat models expect that chat history ends with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a ToolMessage\n",
        "    end_on=(\"human\", \"tool\"),\n",
        "    # Usually, we want to keep the SystemMessage\n",
        "    # if it's present in the original history.\n",
        "    # The SystemMessage has special instructions for the model.\n",
        "    include_system=True,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9367857f-7f9a-4d17-9f9c-6ffc5aae909c",
      "metadata": {},
      "source": [
        "## 高级用法\n\n你可以将 `trim_messages` 用作构建块，以创建更复杂的处理逻辑。\n\n如果我们想允许拆分消息的内容，可以指定 `allow_partial=True`："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0265eba7-c8f3-4495-bcbb-17cd7ede3ece",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trim_messages(\n",
        "    messages,\n",
        "    max_tokens=56,\n",
        "    strategy=\"last\",\n",
        "    token_counter=count_tokens_approximately,\n",
        "    include_system=True,\n",
        "    allow_partial=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "245bee9b-e515-4e89-8f2a-84bda9a25de8",
      "metadata": {},
      "source": [
        "默认情况下，`SystemMessage` 不会被包含，因此你可以通过设置 `include_system=False` 或删除 `include_system` 参数来将其排除。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "94351736-28a1-44a3-aac7-82356c81d171",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trim_messages(\n",
        "    messages,\n",
        "    max_tokens=45,\n",
        "    strategy=\"last\",\n",
        "    token_counter=count_tokens_approximately,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f5d391d-235b-4091-b2de-c22866b478f3",
      "metadata": {},
      "source": [
        "我们可以通过指定 `strategy=\"first\"` 来执行获取 **`max_tokens`** 的翻转操作："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5f56ae54-1a39-4019-9351-3b494c003d5b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"i wonder why it's called langchain\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trim_messages(\n",
        "    messages,\n",
        "    max_tokens=45,\n",
        "    strategy=\"first\",\n",
        "    token_counter=count_tokens_approximately,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0625c094-380f-4485-b2d2-e5dfa83fe299",
      "metadata": {},
      "source": [
        "## 将 `ChatModel` 用作 token 计数器\n\n您可以将 ChatModel 作为 token 计数器传递。这将使用 `ChatModel.get_num_tokens_from_messages`。让我们演示一下如何与 OpenAI 一起使用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9ef35359-1b7a-4918-ab41-30bec69fb3dc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"i wonder why it's called langchain\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "trim_messages(\n",
        "    messages,\n",
        "    max_tokens=45,\n",
        "    strategy=\"first\",\n",
        "    token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab70bf70-1e5a-4d51-b9b8-a823bf2cf532",
      "metadata": {},
      "source": [
        "## 编写自定义 Token 计数器\n\n我们可以编写一个自定义 Token 计数器函数，它接收一个消息列表并返回一个整数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d930c089-e8e6-4980-9d39-11d41e794772",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -qU tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1c1c3b1e-2ece-49e7-a3b6-e69877c1633b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from typing import List\n",
        "\n",
        "import tiktoken\n",
        "from langchain_core.messages import BaseMessage, ToolMessage\n",
        "\n",
        "\n",
        "def str_token_counter(text: str) -> int:\n",
        "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "\n",
        "def tiktoken_counter(messages: List[BaseMessage]) -> int:\n",
        "    \"\"\"Approximately reproduce https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "\n",
        "    For simplicity only supports str Message.contents.\n",
        "    \"\"\"\n",
        "    num_tokens = 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    tokens_per_message = 3\n",
        "    tokens_per_name = 1\n",
        "    for msg in messages:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            role = \"user\"\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            role = \"assistant\"\n",
        "        elif isinstance(msg, ToolMessage):\n",
        "            role = \"tool\"\n",
        "        elif isinstance(msg, SystemMessage):\n",
        "            role = \"system\"\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported messages type {msg.__class__}\")\n",
        "        num_tokens += (\n",
        "            tokens_per_message\n",
        "            + str_token_counter(role)\n",
        "            + str_token_counter(msg.content)\n",
        "        )\n",
        "        if msg.name:\n",
        "            num_tokens += tokens_per_name + str_token_counter(msg.name)\n",
        "    return num_tokens\n",
        "\n",
        "\n",
        "trim_messages(\n",
        "    messages,\n",
        "    # highlight-next-line\n",
        "    token_counter=tiktoken_counter,\n",
        "    # Keep the last <= n_count tokens of the messages.\n",
        "    strategy=\"last\",\n",
        "    # When token_counter=len, each message\n",
        "    # will be counted as a single token.\n",
        "    # highlight-start\n",
        "    # Remember to adjust for your use case\n",
        "    max_tokens=45,\n",
        "    # highlight-end\n",
        "    # Most chat models expect that chat history starts with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a SystemMessage followed by a HumanMessage\n",
        "    start_on=\"human\",\n",
        "    # Most chat models expect that chat history ends with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a ToolMessage\n",
        "    end_on=(\"human\", \"tool\"),\n",
        "    # Usually, we want to keep the SystemMessage\n",
        "    # if it's present in the original history.\n",
        "    # The SystemMessage has special instructions for the model.\n",
        "    include_system=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b2a672b-c007-47c5-9105-617944dc0a6a",
      "metadata": {},
      "source": [
        "## 链式调用\n\n`trim_messages` 可用于命令式（如上例所示）或声明式，使其能轻松地与链式中的其他组件进行组合。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "96aa29b2-01e0-437c-a1ab-02fb0141cb57",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='A \"polly-no-wanna-cracker\"!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 32, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90d33c15d4', 'finish_reason': 'stop', 'logprobs': None}, id='run-b1f8b63b-6bc2-4df4-b3b9-dfc4e3e675fe-0', usage_metadata={'input_tokens': 32, 'output_tokens': 11, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# Notice we don't pass in messages. This creates\n",
        "# a RunnableLambda that takes messages as input\n",
        "trimmer = trim_messages(\n",
        "    token_counter=llm,\n",
        "    # Keep the last <= n_count tokens of the messages.\n",
        "    strategy=\"last\",\n",
        "    # When token_counter=len, each message\n",
        "    # will be counted as a single token.\n",
        "    # Remember to adjust for your use case\n",
        "    max_tokens=45,\n",
        "    # Most chat models expect that chat history starts with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a SystemMessage followed by a HumanMessage\n",
        "    start_on=\"human\",\n",
        "    # Most chat models expect that chat history ends with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a ToolMessage\n",
        "    end_on=(\"human\", \"tool\"),\n",
        "    # Usually, we want to keep the SystemMessage\n",
        "    # if it's present in the original history.\n",
        "    # The SystemMessage has special instructions for the model.\n",
        "    include_system=True,\n",
        ")\n",
        "\n",
        "chain = trimmer | llm\n",
        "chain.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d91d390-e7f7-467b-ad87-d100411d7a21",
      "metadata": {},
      "source": [
        "从 LangSmith 的跟踪中，我们可以看到在消息传递给模型之前，它们首先被修剪了：https://smith.langchain.com/public/65af12c4-c24d-4824-90f0-6547566e59bb/r\n\n仅查看修剪器，我们可以看到它是一个 Runnable 对象，可以像所有 Runnable 一样被调用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1ff02d0a-353d-4fac-a77c-7c2c5262abd9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trimmer.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc4720c8-4062-4ebc-9385-58411202ce6e",
      "metadata": {},
      "source": [
        "## 与 ChatMessageHistory 一起使用\n\n当[处理聊天记录](/docs/how_to/message_history/)时，截断消息尤其有用，因为聊天记录可能会变得任意长："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a9517858-fc2f-4dc3-898d-bf98a0e905a0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='A \"polygon\"!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 32, 'total_tokens': 36, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c17d3befe7', 'finish_reason': 'stop', 'logprobs': None}, id='run-71d9fce6-bb0c-4bb3-acc8-d5eaee6ae7bc-0', usage_metadata={'input_tokens': 32, 'output_tokens': 4, 'total_tokens': 36})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "chat_history = InMemoryChatMessageHistory(messages=messages[:-1])\n",
        "\n",
        "\n",
        "def dummy_get_session_history(session_id):\n",
        "    if session_id != \"1\":\n",
        "        return InMemoryChatMessageHistory()\n",
        "    return chat_history\n",
        "\n",
        "\n",
        "trimmer = trim_messages(\n",
        "    max_tokens=45,\n",
        "    strategy=\"last\",\n",
        "    token_counter=llm,\n",
        "    # Usually, we want to keep the SystemMessage\n",
        "    # if it's present in the original history.\n",
        "    # The SystemMessage has special instructions for the model.\n",
        "    include_system=True,\n",
        "    # Most chat models expect that chat history starts with either:\n",
        "    # (1) a HumanMessage or\n",
        "    # (2) a SystemMessage followed by a HumanMessage\n",
        "    # start_on=\"human\" makes sure we produce a valid chat history\n",
        "    start_on=\"human\",\n",
        ")\n",
        "\n",
        "chain = trimmer | llm\n",
        "chain_with_history = RunnableWithMessageHistory(chain, dummy_get_session_history)\n",
        "chain_with_history.invoke(\n",
        "    [HumanMessage(\"what do you call a speechless parrot\")],\n",
        "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556b7b4c-43cb-41de-94fc-1a41f4ec4d2e",
      "metadata": {},
      "source": [
        "在 LangSmith 跟踪中，我们可以看到我们检索到了所有的消息，但在将消息传递给模型之前，它们被修剪为仅包含系统消息和最后一条用户消息：https://smith.langchain.com/public/17dd700b-9994-44ca-930c-116e00997315/r"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75dc7b84-b92f-44e7-8beb-ba22398e4efb",
      "metadata": {},
      "source": [
        "## API 参考\n\n有关所有参数的完整说明，请参阅 API 参考：https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}