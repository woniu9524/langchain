{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "78b45321-7740-4399-b2ad-459811131de3",
      "metadata": {},
      "source": [
        "# 如何获取对数概率\n\n:::info 先决条件\n\n本指南假定您熟悉以下概念：\n- [对话模型](/docs/concepts/chat_models)\n- [Token](/docs/concepts/tokens)\n\n:::\n\n某些 [对话模型](/docs/concepts/chat_models/) 可以配置为返回代表给定 Token 可能性的 Token 级对数概率。本指南将介绍如何在 LangChain 中获取这些信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f5016bf-2a7b-4140-9b80-8c35c7e5c0d5",
      "metadata": {},
      "source": [
        "## OpenAI\n\n安装 LangChain x OpenAI 包并设置您的 API 密钥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5143fe-84d3-4a91-bae8-629807bbe2cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fd1a2bff-7ac8-46cb-ab95-72c616b45f2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f88ffa0d-f4a7-482c-88de-cbec501a79b1",
      "metadata": {},
      "source": [
        "为了让 OpenAI API 返回对数概率，我们需要配置 `logprobs=True` 参数。然后，对数概率将作为 `response_metadata` 的一部分包含在每个输出的 [`AIMessage`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) 中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d1bf0a9a-e402-4931-ab53-32899f8e0326",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'token': 'I', 'bytes': [73], 'logprob': -0.26341408, 'top_logprobs': []},\n",
              " {'token': \"'m\",\n",
              "  'bytes': [39, 109],\n",
              "  'logprob': -0.48584133,\n",
              "  'top_logprobs': []},\n",
              " {'token': ' just',\n",
              "  'bytes': [32, 106, 117, 115, 116],\n",
              "  'logprob': -0.23484154,\n",
              "  'top_logprobs': []},\n",
              " {'token': ' a',\n",
              "  'bytes': [32, 97],\n",
              "  'logprob': -0.0018291725,\n",
              "  'top_logprobs': []},\n",
              " {'token': ' computer',\n",
              "  'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],\n",
              "  'logprob': -0.052299336,\n",
              "  'top_logprobs': []}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\").bind(logprobs=True)\n",
        "\n",
        "msg = llm.invoke((\"human\", \"how are you today\"))\n",
        "\n",
        "msg.response_metadata[\"logprobs\"][\"content\"][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1ee1c29-d27e-4353-8c3c-2ed7e7f95ff5",
      "metadata": {},
      "source": [
        "并且也是流式 Message 块的一部分："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4bfaf309-3b23-43b7-b333-01fc4848992d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}]\n",
            "[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}]\n",
            "[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}]\n",
            "[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.0022134194, 'top_logprobs': []}]\n"
          ]
        }
      ],
      "source": [
        "ct = 0\n",
        "full = None\n",
        "for chunk in llm.stream((\"human\", \"how are you today\")):\n",
        "    if ct < 5:\n",
        "        full = chunk if full is None else full + chunk\n",
        "        if \"logprobs\" in full.response_metadata:\n",
        "            print(full.response_metadata[\"logprobs\"][\"content\"])\n",
        "    else:\n",
        "        break\n",
        "    ct += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19766435",
      "metadata": {},
      "source": [
        "## 下一步\n\n您现在已经学会了如何在 LangChain 中从 OpenAI 模型获取 logprobs。\n\n接下来，请查看本节中关于聊天模型的其他操作指南，例如[如何让模型返回结构化输出](/docs/how_to/structured_output)或[如何跟踪 token 使用情况](/docs/how_to/chat_token_usage_tracking)。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}