{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e5715368",
      "metadata": {},
      "source": [
        "# 如何追踪 ChatModels 的 Token 使用量\n\n:::info 先决条件\n\n本指南假设您已熟悉以下概念：\n- [Chat models](/docs/concepts/chat_models)\n\n:::\n\n追踪 [token](/docs/concepts/tokens/) 使用量以计算成本是部署应用程序的重要组成部分。本指南将介绍如何从您的 LangChain 模型调用中获取这些信息。\n\n本指南需要 `langchain-anthropic` 和 `langchain-openai >= 0.3.11`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7d1338-dd1b-4d06-b33d-d5cffc49fd6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU langchain-anthropic langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c71b676a",
      "metadata": {},
      "source": [
        ":::note 关于 OpenAI 流式传输的注意事项\n\nOpenAI 的 Chat Completions API 默认不流式传输 token 使用情况统计信息（请参阅此处的 API 参考文档\n[here](https://platform.openai.com/docs/api-reference/completions/create#completions-create-stream_options)）。\n要在使用 `ChatOpenAI` 或 `AzureChatOpenAI` 进行流式传输时恢复 token 计数，请将 `stream_usage` 设置为 `True`，\n如本指南所示。\n\n:::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "598ae1e2-a52d-4459-81fd-cdc68b06742a",
      "metadata": {},
      "source": [
        "## 使用 LangSmith\n\n您可以使用 [LangSmith](https://www.langchain.com/langsmith) 来帮助跟踪 LLM 应用中的 token 使用量。请参阅 [LangSmith 快速入门指南](https://docs.smith.langchain.com/)。\n\n## 使用 AIMessage.usage_metadata\n\n许多模型提供商会在聊天生成响应中包含 token 使用信息。如果可用，这些信息将包含在相应模型生成的 `AIMessage` 对象上。\n\nLangChain 的 `AIMessage` 对象包含一个 [usage_metadata](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html#langchain_core.messages.ai.AIMessage.usage_metadata) 属性。当填充时，此属性将是一个 [UsageMetadata](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.UsageMetadata.html) 字典，包含标准键（例如 `\"input_tokens\"` 和 `\"output_tokens\"`）。它们还将包含关于缓存的 token 使用量和来自多模态数据的 token 的信息。\n\n示例：\n\n**OpenAI**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b39bf807-4125-4db4-bbf7-28a46afff6b4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(model=\"gpt-4o-mini\")\n",
        "openai_response = llm.invoke(\"hello\")\n",
        "openai_response.usage_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2299c44a-2fe6-4d52-a6a2-99ff6d231c73",
      "metadata": {},
      "source": [
        "**Anthropic**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9c82ff80-ec4e-4049-b019-5f0bbd7df82a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_tokens': 8, 'output_tokens': 12, 'total_tokens': 20}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
        "anthropic_response = llm.invoke(\"hello\")\n",
        "anthropic_response.usage_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4ef2c43-0ff6-49eb-9782-e4070c9da8d7",
      "metadata": {},
      "source": [
        "### 流式传输\n\n一些提供商在流式传输的上下文中支持令牌计数元数据。\n\n#### OpenAI\n\n例如，OpenAI 将在流的末尾返回一个消息 [块](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html)，其中包含令牌使用信息。此行为受 `langchain-openai >= 0.1.9` 支持，可以通过设置 `stream_usage=True` 来启用。设置 `ChatOpenAI` 实例时也可以设置此属性。\n\n:::note\n默认情况下，流中的最后一个消息块将在消息的 `response_metadata` 属性中包含一个 `\"finish_reason\"`。如果我们启用流式传输模式下的令牌使用计数，则会在流的末尾添加一个包含元数据的附加块，以便 `\"finish_reason\"` 出现在倒数第二个消息块上。\n:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "07f0c872-6b6c-4fed-a129-9b5a858505be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\n",
            "content='Hello' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\n",
            "content='!' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\n",
            "content=' How' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\n",
            "content=' can' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\n",
            "content=' I' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\n",
            "content=' assist' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\n",
            "content=' you' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\n",
            "content=' today' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\n",
            "content='?' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\n",
            "content='' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini'} id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\n",
            "content='' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}\n"
          ]
        }
      ],
      "source": [
        "llm = init_chat_model(model=\"gpt-4o-mini\")\n",
        "\n",
        "aggregate = None\n",
        "for chunk in llm.stream(\"hello\", stream_usage=True):\n",
        "    print(chunk)\n",
        "    aggregate = chunk if aggregate is None else aggregate + chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd809ded-8b13-4d5f-be5e-277b79d51802",
      "metadata": {},
      "source": [
        "请注意，使用情况元数据将包含在各个消息块的总和中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3db7bc03-a7d4-4704-92ab-f8ba92ef59ae",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I assist you today?\n",
            "{'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}\n"
          ]
        }
      ],
      "source": [
        "print(aggregate.content)\n",
        "print(aggregate.usage_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dba63e8-0ed7-4533-8f0f-78e19c38a25c",
      "metadata": {},
      "source": [
        "要禁用 OpenAI 的流式令牌计数，请将 `stream_usage` 设置为 False，或者将其从参数中省略："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "67117f2b-ce68-4c1e-9556-2d3849f90e1b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='' id='run-8e758550-94b0-4cca-a298-57482793c25d'\n",
            "content='Hello' id='run-8e758550-94b0-4cca-a298-57482793c25d'\n",
            "content='!' id='run-8e758550-94b0-4cca-a298-57482793c25d'\n",
            "content=' How' id='run-8e758550-94b0-4cca-a298-57482793c25d'\n",
            "content=' can' id='run-8e758550-94b0-4cca-a298-57482793c25d'\n",
            "content=' I' id='run-8e758550-94b0-4cca-a298-57482793c25d'\n",
            "content=' assist' id='run-8e758550-94b0-4cca-a298-57482793c25d'\n",
            "content=' you' id='run-8e758550-94b0-4cca-a298-57482793c25d'\n",
            "content=' today' id='run-8e758550-94b0-4cca-a298-57482793c25d'\n",
            "content='?' id='run-8e758550-94b0-4cca-a298-57482793c25d'\n",
            "content='' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini'} id='run-8e758550-94b0-4cca-a298-57482793c25d'\n"
          ]
        }
      ],
      "source": [
        "aggregate = None\n",
        "for chunk in llm.stream(\"hello\"):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5d9617-be3a-419a-9276-de9c29fa50ae",
      "metadata": {},
      "source": [
        "您还可以通过在实例化聊天模型时设置 `stream_usage` 来启用流式 token 使用。当将聊天模型集成到 LangChain [链](/docs/concepts/lcel) 中时，这会非常有用：可以在 [流式传输中间步骤](/docs/how_to/streaming#using-stream-events) 或使用 LangSmith 等跟踪软件 [LangSmith](https://docs.smith.langchain.com/) 时监视模型使用情况的元数据。\n\n请看下面的示例，在此示例中，我们以结构化形式返回输出到期望的模式，但仍然可以观察到从中间步骤流式传输的 token 使用情况。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0b1523d8-127e-4314-82fa-bd97aca37f9a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token usage: {'input_tokens': 79, 'output_tokens': 23, 'total_tokens': 102}\n",
            "\n",
            "setup='Why was the math book sad?' punchline='Because it had too many problems.'\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Joke(BaseModel):\n",
        "    \"\"\"Joke to tell user.\"\"\"\n",
        "\n",
        "    setup: str = Field(description=\"question to set up a joke\")\n",
        "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
        "\n",
        "\n",
        "llm = init_chat_model(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    stream_usage=True,\n",
        ")\n",
        "# Under the hood, .with_structured_output binds tools to the\n",
        "# chat model and appends a parser.\n",
        "structured_llm = llm.with_structured_output(Joke)\n",
        "\n",
        "async for event in structured_llm.astream_events(\"Tell me a joke\"):\n",
        "    if event[\"event\"] == \"on_chat_model_end\":\n",
        "        print(f'Token usage: {event[\"data\"][\"output\"].usage_metadata}\\n')\n",
        "    elif event[\"event\"] == \"on_chain_end\" and event[\"name\"] == \"RunnableSequence\":\n",
        "        print(event[\"data\"][\"output\"])\n",
        "    else:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc8d313-4bef-463e-89a5-236d8bb6ab2f",
      "metadata": {},
      "source": [
        "Token 用量在聊天模型负载的相应 [LangSmith 追踪](https://smith.langchain.com/public/fe6513d5-7212-4045-82e0-fefa28bc7656/r) 中也可查到。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6845407-af25-4eed-bc3e-50925c6661e0",
      "metadata": {},
      "source": [
        "## 使用回调\n\n:::info 需要 ``langchain-core>=0.3.49``\n\n:::\n\nLangChain 实现了一个回调处理器和上下文管理器，它将跟踪返回 `usage_metadata` 的任何聊天模型的调用中的 token 使用情况。\n\n还有一些特定于 API 的回调上下文管理器，它们维护不同模型的定价，从而允许实时成本估算。它们目前仅为 OpenAI API 和 Bedrock Anthropic API 实现，并在 `langchain-community` 中提供：\n\n- [get_openai_callback](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.manager.get_openai_callback.html)\n- [get_bedrock_anthropic_callback](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.manager.get_bedrock_anthropic_callback.html)\n\n下面，我们将演示通用的 usage metadata 回调管理器。我们可以通过配置或上下文管理器来跟踪 token 使用情况。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f043cb9",
      "metadata": {},
      "source": [
        "### 通过配置跟踪 token 使用情况\n\n要通过配置跟踪 token 使用情况，请实例化一个 `UsageMetadataCallbackHandler` 并将其传递到配置中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b04a4486-72fd-48ce-8f9e-5d281b441195",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'gpt-4o-mini-2024-07-18': {'input_tokens': 8,\n",
              "  'output_tokens': 10,\n",
              "  'total_tokens': 18,\n",
              "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
              "  'output_token_details': {'audio': 0, 'reasoning': 0}},\n",
              " 'claude-3-5-haiku-20241022': {'input_tokens': 8,\n",
              "  'output_tokens': 21,\n",
              "  'total_tokens': 29,\n",
              "  'input_token_details': {'cache_read': 0, 'cache_creation': 0}}}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
        "\n",
        "llm_1 = init_chat_model(model=\"openai:gpt-4o-mini\")\n",
        "llm_2 = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
        "\n",
        "callback = UsageMetadataCallbackHandler()\n",
        "result_1 = llm_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
        "result_2 = llm_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
        "callback.usage_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a290085-e541-4233-afe4-637ec5032bfd",
      "metadata": {},
      "source": [
        "### 使用上下文管理器追踪 token 用量\n\n您还可以使用 `get_usage_metadata_callback` 来创建一个上下文管理器，并在其中聚合用量元数据："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4728f55a-24e1-48cd-a195-09d037821b1e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'gpt-4o-mini-2024-07-18': {'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'claude-3-5-haiku-20241022': {'input_tokens': 8, 'output_tokens': 21, 'total_tokens': 29, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}}\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.callbacks import get_usage_metadata_callback\n",
        "\n",
        "llm_1 = init_chat_model(model=\"openai:gpt-4o-mini\")\n",
        "llm_2 = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
        "\n",
        "with get_usage_metadata_callback() as cb:\n",
        "    llm_1.invoke(\"Hello\")\n",
        "    llm_2.invoke(\"Hello\")\n",
        "    print(cb.usage_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ab6d27",
      "metadata": {},
      "source": [
        "这两种方法都可以汇总多次调用每个模型的 token 用量。例如，您可以在 [agent](https://python.langchain.com/docs/concepts/agents/) 中使用它来跟踪对单个模型的重复调用中的 token 用量："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8acbead9",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fe945078-ee2d-43ba-8cdf-afb2f2f4ecef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What's the weather in Boston?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_weather (call_izMdhUYpp9Vhx7DTNAiybzGa)\n",
            " Call ID: call_izMdhUYpp9Vhx7DTNAiybzGa\n",
            "  Args:\n",
            "    location: Boston\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_weather\n",
            "\n",
            "It's sunny.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The weather in Boston is sunny.\n",
            "\n",
            "Total usage: {'gpt-4o-mini-2024-07-18': {'input_token_details': {'audio': 0, 'cache_read': 0}, 'input_tokens': 125, 'total_tokens': 149, 'output_tokens': 24, 'output_token_details': {'audio': 0, 'reasoning': 0}}}\n"
          ]
        }
      ],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "\n",
        "# Create a tool\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Get the weather at a location.\"\"\"\n",
        "    return \"It's sunny.\"\n",
        "\n",
        "\n",
        "callback = UsageMetadataCallbackHandler()\n",
        "\n",
        "tools = [get_weather]\n",
        "agent = create_react_agent(\"openai:gpt-4o-mini\", tools)\n",
        "for step in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]},\n",
        "    stream_mode=\"values\",\n",
        "    config={\"callbacks\": [callback]},\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()\n",
        "\n",
        "\n",
        "print(f\"\\nTotal usage: {callback.usage_metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33172f31",
      "metadata": {},
      "source": [
        "## 后续步骤\n\n您现在已经看到了跟踪受支持的提供商的 token 使用量的几个示例。\n\n接下来，请查看本节中有关聊天模型的其他操作指南，例如[如何让模型返回结构化输出](/docs/how_to/structured_output)或[如何为聊天模型添加缓存](/docs/how_to/chat_model_caching)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb40375d",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}