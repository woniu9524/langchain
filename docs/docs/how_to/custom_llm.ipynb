{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9e9b7651",
      "metadata": {},
      "source": [
        "# 如何创建自定义 LLM 类\n\n本 notebook 将介绍如何创建自定义 LLM 包装器，以防您想使用自己的 LLM 或 LangChain 不支持的其他包装器。\n\n通过标准的 `LLM` 接口包装您的 LLM，可以让您在现有的 LangChain 程序中使用您的 LLM，而只需进行最少的代码修改。\n\n此外，您的 LLM 将自动成为 LangChain 的 `Runnable`，并可直接受益于一些开箱即用的优化、异步支持、`astream_events` API 等。\n\n:::caution\n您当前所在页面记录的是 [文本补全模型](/docs/concepts/text_llms) 的用法。许多最新和最受欢迎的模型是 [聊天补全模型](/docs/concepts/chat_models)。\n\n除非您特别使用了更高级的提示技术，否则您可能是在查找[此页面](/docs/how_to/custom_chat_model/)。\n:::\n\n## 实现\n\n自定义 LLM 只需要实现两个必需的方法：\n\n| 方法        | 描述                                                               |\n|---------------|---------------------------------------------------------------------------|\n| `_call`       | 接收一个字符串和一些可选的停止词，并返回一个字符串。供 `invoke` 使用。 |\n| `_llm_type`   | 一个返回字符串的属性，仅用于日志记录目的。        |\n\n\n\n可选的实现：\n\n| 方法    | 描述                                                                                               |\n|---------------|-----------------------------------------------------------------------------------------------------------|\n| `_identifying_params` | 用于帮助识别模型和打印 LLM；应返回一个字典。这是一个 **@property**。                 |\n| `_acall`              | 提供 `_call` 的异步原生实现，供 `ainvoke` 使用。                                    |\n| `_stream`             | 用于逐个 token 流式输出的方法。                                                               |\n| `_astream`            | 提供 `_stream` 的异步原生实现；在较新版本的 LangChain 中，默认为 `_stream`。 |\n\n\n\n让我们来实现一个简单的自定义 LLM，它只返回输入的前 n 个字符。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2e9bb32f-6fd1-46ac-b32f-d175663710c0",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
        "\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain_core.outputs import GenerationChunk\n",
        "\n",
        "\n",
        "class CustomLLM(LLM):\n",
        "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
        "\n",
        "    When contributing an implementation to LangChain, carefully document\n",
        "    the model including the initialization parameters, include\n",
        "    an example of how to initialize the model and include any relevant\n",
        "    links to the underlying models documentation or API.\n",
        "\n",
        "    Example:\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            model = CustomChatModel(n=2)\n",
        "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
        "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
        "                                 [HumanMessage(content=\"world\")]])\n",
        "    \"\"\"\n",
        "\n",
        "    n: int\n",
        "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Run the LLM on the given input.\n",
        "\n",
        "        Override this method to implement the LLM logic.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt to generate from.\n",
        "            stop: Stop words to use when generating. Model output is cut off at the\n",
        "                first occurrence of any of the stop substrings.\n",
        "                If stop tokens are not supported consider raising NotImplementedError.\n",
        "            run_manager: Callback manager for the run.\n",
        "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
        "                to the model provider API call.\n",
        "\n",
        "        Returns:\n",
        "            The model output as a string. Actual completions SHOULD NOT include the prompt.\n",
        "        \"\"\"\n",
        "        if stop is not None:\n",
        "            raise ValueError(\"stop kwargs are not permitted.\")\n",
        "        return prompt[: self.n]\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Iterator[GenerationChunk]:\n",
        "        \"\"\"Stream the LLM on the given prompt.\n",
        "\n",
        "        This method should be overridden by subclasses that support streaming.\n",
        "\n",
        "        If not implemented, the default behavior of calls to stream will be to\n",
        "        fallback to the non-streaming version of the model and return\n",
        "        the output as a single chunk.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt to generate from.\n",
        "            stop: Stop words to use when generating. Model output is cut off at the\n",
        "                first occurrence of any of these substrings.\n",
        "            run_manager: Callback manager for the run.\n",
        "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
        "                to the model provider API call.\n",
        "\n",
        "        Returns:\n",
        "            An iterator of GenerationChunks.\n",
        "        \"\"\"\n",
        "        for char in prompt[: self.n]:\n",
        "            chunk = GenerationChunk(text=char)\n",
        "            if run_manager:\n",
        "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
        "\n",
        "            yield chunk\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
        "        return {\n",
        "            # The model name allows users to specify custom token counting\n",
        "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
        "            # can provide per token pricing for their model and monitor\n",
        "            # costs for the given LLM.)\n",
        "            \"model_name\": \"CustomChatModel\",\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
        "        return \"custom\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f614fb7b-e476-4d81-821b-57a2ebebe21c",
      "metadata": {
        "tags": []
      },
      "source": [
        "### 让我们来测试一下 🧪"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3feae15-4afc-49f4-8542-93867d4ea769",
      "metadata": {
        "tags": []
      },
      "source": [
        "这个 LLM 将会实现 LangChain 的标准 `Runnable` 接口，LangChain 的许多抽象都支持该接口！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dfff4a95-99b2-4dba-b80d-9c3855046ef1",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mCustomLLM\u001b[0m\n",
            "Params: {'model_name': 'CustomChatModel'}\n"
          ]
        }
      ],
      "source": [
        "llm = CustomLLM(n=5)\n",
        "print(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8cd49199",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'This '"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"This is a foobar thing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "511b3cb1-9c6f-49b6-9002-a2ec490632b0",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'world'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await llm.ainvoke(\"world\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d9d5bec2-d60a-4ebd-a97d-ac32c98ab02f",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['woof ', 'meow ']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.batch([\"woof woof woof\", \"meow meow meow\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fe246b29-7a93-4bef-8861-389445598c25",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['woof ', 'meow ']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await llm.abatch([\"woof woof woof\", \"meow meow meow\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3a67c38f-b83b-4eb9-a231-441c55ee8c82",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h|e|l|l|o|"
          ]
        }
      ],
      "source": [
        "async for token in llm.astream(\"hello\"):\n",
        "    print(token, end=\"|\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b62c282b-3a35-4529-aac4-2c2f0916790e",
      "metadata": {},
      "source": [
        "让我们确认它能与其他 `LangChain` API 很好地集成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d5578e74-7fa8-4673-afee-7a59d442aaff",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "672ff664-8673-4832-9f4f-335253880141",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"you are a bot\"), (\"human\", \"{input}\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c400538a-9146-4c93-9fac-293d8f9ca6bf",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "llm = CustomLLM(n=7)\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "080964af-3e2d-4573-85cb-0d7cc58a6f42",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'event': 'on_chain_start', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}\n",
            "{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}\n",
            "{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}, 'output': ChatPromptValue(messages=[SystemMessage(content='you are a bot'), HumanMessage(content='hello there!')])}}\n",
            "{'event': 'on_llm_start', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'input': {'prompts': ['System: you are a bot\\nHuman: hello there!']}}}\n",
            "{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'S'}}\n",
            "{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'S'}}\n",
            "{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'y'}}\n",
            "{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'y'}}\n"
          ]
        }
      ],
      "source": [
        "idx = 0\n",
        "async for event in chain.astream_events({\"input\": \"hello there!\"}, version=\"v1\"):\n",
        "    print(event)\n",
        "    idx += 1\n",
        "    if idx > 7:\n",
        "        # Truncate\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85e848a-5316-4318-b770-3f8fd34f4231",
      "metadata": {},
      "source": [
        "## 贡献\n\n我们感谢所有关于聊天模型集成的贡献。\n\n以下清单有助于确保您的贡献被添加到 LangChain 中：\n\n文档：\n\n* 为所有初始化参数添加了文档字符串 (doc-strings)，这些文档字符串将显示在 [API 参考](https://python.langchain.com/api_reference/langchain/index.html) 中。\n* 如果模型是由某个服务支持的，其类的文档字符串应包含指向该模型 API 的链接。\n\n测试：\n\n* [ ] 为重写的方法添加单元测试或集成测试。如果您重写了相应的代码，请验证 `invoke`、`ainvoke`、`batch`、`stream` 是否正常工作。\n\n流式输出（如果您正在实现）：\n\n* [ ] 确保调用了 `on_llm_new_token` 回调\n* [ ] `on_llm_new_token` 应在 yielding 块 **之前** 调用\n\n停止标记行为：\n\n* [ ] 应尊重停止标记\n* [ ] 停止标记应包含在响应中\n\n秘密 API 密钥：\n\n* [ ] 如果您的模型连接到 API，它可能会接受初始化参数中的 API 密钥。请使用 Pydantic 的 `SecretStr` 类型来处理密钥，这样人们在打印模型时就不会意外地打印出它们。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}