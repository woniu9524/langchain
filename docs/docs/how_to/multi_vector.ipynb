{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9172545",
      "metadata": {},
      "source": [
        "# 如何检索每篇文档的多个向量\n\n为每篇文档存储多个 [向量](/docs/concepts/vectorstores/) 通常很有用。有多种用例可以证明这一点。例如，我们可以为文档的多个 [块](/docs/concepts/embedding_models/) 进行 [嵌入](/docs/concepts/embedding_models/)，并将这些嵌入与父文档关联起来，这样 [检索器](/docs/concepts/retrievers/) 在命中这些块时就可以返回整个文档。\n\nLangChain 实现了一个基础的 [MultiVectorRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html)，它简化了这一过程。大部分的复杂性在于如何创建每篇文档的多个向量。本笔记介绍了一些创建这些向量的常用方法以及如何使用 `MultiVectorRetriever`。\n\n为每篇文档创建多个向量的方法包括：\n\n- 更小的块：将文档分割成更小的块，并对它们进行嵌入（这是 [ParentDocumentRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html)）。\n- 摘要：为每篇文档创建一个摘要，并将其与文档一起（或代替文档）进行嵌入。\n- 假设性问题：创建一些假设性问题，每篇文档都适用于回答这些问题，并将它们与文档一起（或代替文档）进行嵌入。\n\n请注意，这也使得添加嵌入的另一种方法成为可能——手动添加。这很有用，因为您可以显式地添加应该能够检索到文档的问题或查询，从而获得更多控制权。\n\n下面我们通过一个例子来 walkthrough。首先，我们实例化一些文档。我们将使用 [OpenAI](https://python.langchain.com/docs/integrations/text_embedding/openai/) 嵌入在（内存中的）[Chroma](/docs/integrations/providers/chroma/) 向量存储中索引它们，但任何 LangChain 向量存储或嵌入模型都可以满足需求。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09cecd95-3499-465a-895a-944627ffb77f",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain-chroma langchain langchain-openai > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "18c1421a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.storage import InMemoryByteStore\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "loaders = [\n",
        "    TextLoader(\"paul_graham_essay.txt\"),\n",
        "    TextLoader(\"state_of_the_union.txt\"),\n",
        "]\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
        "docs = text_splitter.split_documents(docs)\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa17beda",
      "metadata": {},
      "source": [
        "## 更小的块\n\n很多时候检索大型信息块很有用，但会嵌入更小的块。这使得嵌入能够尽可能地捕捉语义含义，同时将尽可能多的上下文传递下游。请注意，这正是 [ParentDocumentRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html) 所做的。这里我们展示其内部工作原理。\n\n我们将区分向量存储（索引（子）文档的嵌入）和文档存储（存储“父”文档并将其与标识符关联）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0e7b6b45",
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryByteStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    byte_store=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4feded4-856a-4282-91c3-53aabc62e6ff",
      "metadata": {},
      "source": [
        "接下来，我们通过拆分原始文档来生成“子”文档。请注意，我们将文档标识符存储在相应 [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) 对象的 `metadata` 中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5d23247d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# The splitter to use to create smaller chunks\n",
        "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
        "\n",
        "sub_docs = []\n",
        "for i, doc in enumerate(docs):\n",
        "    _id = doc_ids[i]\n",
        "    _sub_docs = child_text_splitter.split_documents([doc])\n",
        "    for _doc in _sub_docs:\n",
        "        _doc.metadata[id_key] = _id\n",
        "    sub_docs.extend(_sub_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0634f8-90d5-4250-981a-5257c8a6d455",
      "metadata": {},
      "source": [
        "最后，我们在向量存储和文档存储中对文档进行索引："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "92ed5861",
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever.vectorstore.add_documents(sub_docs)\n",
        "retriever.docstore.mset(list(zip(doc_ids, docs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c48c6d-850c-4317-9b6e-1ade92f2f710",
      "metadata": {},
      "source": [
        "Vector store 仅会检索小片段："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8afed60c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.', metadata={'doc_id': '064eca46-a4c4-4789-8e3b-583f9597e54f', 'source': 'state_of_the_union.txt'})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.vectorstore.similarity_search(\"justice breyer\")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "717097c7-61d9-4306-8625-ef8f1940c127",
      "metadata": {},
      "source": [
        "而检索器会返回更大的父文档："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3c9017f1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9875"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(retriever.invoke(\"justice breyer\")[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdef8339-f9fa-4b3b-955f-ad9dbdf2734f",
      "metadata": {},
      "source": [
        "检索器在向量数据库上执行的默认搜索类型是相似性搜索。LangChain 向量存储还支持通过 [Max Marginal Relevance](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html#langchain_core.vectorstores.base.VectorStore.max_marginal_relevance_search) 进行搜索。这可以通过检索器的 `search_type` 参数进行控制："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "36739460-a737-4a8e-b70f-50bf8c8eaae7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9875"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.retrievers.multi_vector import SearchType\n",
        "\n",
        "retriever.search_type = SearchType.mmr\n",
        "\n",
        "len(retriever.invoke(\"justice breyer\")[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a7ae0d",
      "metadata": {},
      "source": [
        "## 为文档关联摘要以进行检索\n\n摘要能够更准确地提炼出分块（chunk）的内容，从而带来更好的检索效果。这里我们展示如何创建摘要，然后对它们进行嵌入。\n\n我们构建了一个简单的[链（chain）](/docs/how_to/sequence)，它将接收一个输入的[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) 对象，并使用 LLM 生成摘要。\n\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\n\n<ChatModelTabs customVarName=\"llm\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6589291f-55bb-4e9a-b4ff-08f2506ed641",
      "metadata": {},
      "outputs": [],
      "source": [
        "# | output: false\n",
        "# | echo: false\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1433dff4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chain = (\n",
        "    {\"doc\": lambda x: x.page_content}\n",
        "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3faa9fde-1b09-4849-a815-8b2e89c30a02",
      "metadata": {},
      "source": [
        "请注意，我们可以跨文档[批量](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)处理链："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "41a2a738",
      "metadata": {},
      "outputs": [],
      "source": [
        "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ef599e-140b-4905-8b62-6c52cdde1852",
      "metadata": {},
      "source": [
        "然后，我们可以像之前一样初始化 `MultiVectorRetriever`，在向量数据库中索引摘要，并在文档数据库中保留原始文档："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7ac5e4b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryByteStore()\n",
        "id_key = \"doc_id\"\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    byte_store=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
        "\n",
        "summary_docs = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(summaries)\n",
        "]\n",
        "\n",
        "retriever.vectorstore.add_documents(summary_docs)\n",
        "retriever.docstore.mset(list(zip(doc_ids, docs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "862ae920",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # We can also add the original chunks to the vectorstore if we so want\n",
        "# for i, doc in enumerate(docs):\n",
        "#     doc.metadata[id_key] = doc_ids[i]\n",
        "# retriever.vectorstore.add_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0274892-29c1-4616-9040-d23f9d537526",
      "metadata": {},
      "source": [
        "查询向量存储将返回摘要："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "299232d6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content=\"President Biden recently nominated Judge Ketanji Brown Jackson to serve on the United States Supreme Court, emphasizing her qualifications and broad support. The President also outlined a plan to secure the border, fix the immigration system, protect women's rights, support LGBTQ+ Americans, and advance mental health services. He highlighted the importance of bipartisan unity in passing legislation, such as the Violence Against Women Act. The President also addressed supporting veterans, particularly those impacted by exposure to burn pits, and announced plans to expand benefits for veterans with respiratory cancers. Additionally, he proposed a plan to end cancer as we know it through the Cancer Moonshot initiative. President Biden expressed optimism about the future of America and emphasized the strength of the American people in overcoming challenges.\", metadata={'doc_id': '84015b1b-980e-400a-94d8-cf95d7e079bd'})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub_docs = retriever.vectorstore.similarity_search(\"justice breyer\")\n",
        "\n",
        "sub_docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f77ac5-2926-4f60-aad5-b2067900dff9",
      "metadata": {},
      "source": [
        "而 retriever 会返回更大的源文档："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e4cce5c2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9194"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieved_docs = retriever.invoke(\"justice breyer\")\n",
        "\n",
        "len(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "097a5396",
      "metadata": {},
      "source": [
        "## 假设性问题\n\nLLM 还可以用于生成一组假设性问题，这些问题可以针对特定文档提出，这些问题可能与 [RAG](/docs/tutorials/rag) 应用中的相关查询在语义上高度相似。然后，这些问题可以被嵌入并与文档关联起来，以改进检索。\n\n下面，我们使用 `with_structured_output` 方法将 LLM 的输出结构化为字符串列表。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "03d85234-c33a-4a43-861d-47328e1ec2ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class HypotheticalQuestions(BaseModel):\n",
        "    \"\"\"Generate hypothetical questions.\"\"\"\n",
        "\n",
        "    questions: List[str] = Field(..., description=\"List of questions\")\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\"doc\": lambda x: x.page_content}\n",
        "    # Only asking for 3 hypothetical questions, but this could be adjusted\n",
        "    | ChatPromptTemplate.from_template(\n",
        "        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\"\n",
        "    )\n",
        "    | ChatOpenAI(max_retries=0, model=\"gpt-4o\").with_structured_output(\n",
        "        HypotheticalQuestions\n",
        "    )\n",
        "    | (lambda x: x.questions)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dddc40f-62af-413c-b944-f94a5e1f2f4e",
      "metadata": {},
      "source": [
        "调用链在一个文档上演示了它会输出一个问题列表："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "11d30554",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"What impact did the IBM 1401 have on the author's early programming experiences?\",\n",
              " \"How did the transition from using the IBM 1401 to microcomputers influence the author's programming journey?\",\n",
              " \"What role did Lisp play in shaping the author's understanding and approach to AI?\"]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcffc572-7b20-4b77-857a-90ec360a8f7e",
      "metadata": {},
      "source": [
        "我们可以批处理链并像以前一样组装矢量存储和文档存储："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b2cd6e75",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch chain over documents to generate hypothetical questions\n",
        "hypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5})\n",
        "\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"hypo-questions\", embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryByteStore()\n",
        "id_key = \"doc_id\"\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    byte_store=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
        "\n",
        "\n",
        "# Generate Document objects from hypothetical questions\n",
        "question_docs = []\n",
        "for i, question_list in enumerate(hypothetical_questions):\n",
        "    question_docs.extend(\n",
        "        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]\n",
        "    )\n",
        "\n",
        "\n",
        "retriever.vectorstore.add_documents(question_docs)\n",
        "retriever.docstore.mset(list(zip(doc_ids, docs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75cba8ab-a06f-4545-85fc-cf49d0204b5e",
      "metadata": {},
      "source": [
        "请注意，查询底层向量存储将检索与输入查询在语义上相似的假定问题："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7b442b90",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='What might be the potential benefits of nominating Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court?', metadata={'doc_id': '43292b74-d1b8-4200-8a8b-ea0cb57fbcdb'}),\n",
              " Document(page_content='How might the Bipartisan Infrastructure Law impact the economic competition between the U.S. and China?', metadata={'doc_id': '66174780-d00c-4166-9791-f0069846e734'}),\n",
              " Document(page_content='What factors led to the creation of Y Combinator?', metadata={'doc_id': '72003c4e-4cc9-4f09-a787-0b541a65b38c'}),\n",
              " Document(page_content='How did the ability to publish essays online change the landscape for writers and thinkers?', metadata={'doc_id': 'e8d2c648-f245-4bcc-b8d3-14e64a164b64'})]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub_docs = retriever.vectorstore.similarity_search(\"justice breyer\")\n",
        "\n",
        "sub_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c32e43-5f4a-463b-a0c2-2101986f70e6",
      "metadata": {},
      "source": [
        "调用 retriever 会返回相应的文档："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7594b24e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9194"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieved_docs = retriever.invoke(\"justice breyer\")\n",
        "len(retrieved_docs[0].page_content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}