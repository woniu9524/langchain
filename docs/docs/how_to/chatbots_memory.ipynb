{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_position: 1\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 如何为聊天机器人添加记忆\n\n聊天机器人的一个关键特性是它们能够利用先前对话轮次的内容作为上下文。这种状态管理可以有多种形式，包括：\n\n- 简单地将之前的消息塞入聊天模型的提示中。\n- 上述方法，但会修剪旧消息，以减少模型需要处理的干扰性信息量。\n- 更复杂的修改，例如为长时间的对话合成摘要。\n\n我们将在下面更详细地介绍几种技术！\n\n:::note\n\n此指南先前使用 [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) 构建了一个聊天机器人。您可以在 [v0.2 文档](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/) 中访问此版本的指南。\n\n截至 LangChain 的 v0.3 版本，我们建议 LangChain 用户利用 [LangGraph 持久化](https://langchain-ai.github.io/langgraph/concepts/persistence/) 将 `memory` 集成到新的 LangChain 应用程序中。\n\n如果您的代码已依赖于 `RunnableWithMessageHistory` 或 `BaseChatMessageHistory`，则 **无需** 进行任何更改。我们不打算在不久的将来弃用此功能，因为它适用于简单的聊天应用程序，并且任何使用 `RunnableWithMessageHistory` 的代码将继续按预期工作。\n\n请参阅 [如何迁移到 LangGraph Memory](/docs/versions/migrating_memory/) 以获取更多详细信息。\n:::\n\n## 设置\n\n您需要安装一些软件包，并将您的 OpenAI API 密钥设置为名为 `OPENAI_API_KEY` 的环境变量："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "OpenAI API Key: ········\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet langchain langchain-openai langgraph\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们还为下面的示例设置一个聊天模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 消息传递\n\n最简单的内存形式就是将聊天历史消息传递给一个链条。下面是一个例子："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I said, \"I love programming\" in French: \"J'adore la programmation.\"\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "ai_msg = chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"Translate from English to French: I love programming.\"\n",
        "            ),\n",
        "            AIMessage(content=\"J'adore la programmation.\"),\n",
        "            HumanMessage(content=\"What did you just say?\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "print(ai_msg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们可以看到，通过将之前的对话传递到链中，它可以将其用作回答问题的上下文。这是支撑聊天机器人记忆的基本概念——本指南的其余部分将演示传递或重新格式化消息的便捷技术。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 自动历史记录管理\n\n之前的示例会显式地将消息传递给链（和模型）。这是一种完全可以接受的方法，但它需要外部管理新消息。LangChain 还提供了一种使用 LangGraph 的[持久化](https://langchain-ai.github.io/langgraph/concepts/persistence/)来构建具有记忆力的应用程序的方法。您可以通过在编译图时提供 `checkpointer` 来在 LangGraph 应用程序中[启用持久化](https://langchain-ai.github.io/langgraph/how-tos/persistence/)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"Answer all questions to the best of your ability.\"\n",
        "    )\n",
        "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the node and edge\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Add simple in-memory checkpointer\n",
        "# highlight-start\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "# highlight-end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们将最新的输入传递给对话，并让 LangGraph 使用 checkpointer 来跟踪对话历史："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\n",
              "  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n",
        "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\n",
              "  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39}),\n",
              "  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='c667529b-7c41-4cc0-9326-0af47328b816'),\n",
              "  AIMessage(content='You asked me to translate \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-134a7ea0-d3a4-4923-bd58-25e5a43f6a1f-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67})]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n",
        "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 修改聊天记录\n\n修改存储的聊天消息可以帮助您的聊天机器人处理各种情况。以下是一些示例：\n\n### 修剪消息\n\nLLM 和聊天模型具有有限的上下文窗口，即使您没有直接达到限制，也可能希望限制模型需要处理的分心内容。一种解决方案是在将历史消息传递给模型之前对其进行修剪。让我们使用上面声明的 `app` 和一个示例历史记录："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\n",
              "  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\n",
              "  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\n",
              "  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\n",
              "  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='c933eca3-5fd8-4651-af16-20fe2d49c216'),\n",
              "  AIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a0b21acc-9dbb-4fb6-a953-392020f37d88-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68})]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_ephemeral_chat_history = [\n",
        "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
        "    AIMessage(content=\"Hello!\"),\n",
        "    HumanMessage(content=\"How are you today?\"),\n",
        "    AIMessage(content=\"Fine thanks!\"),\n",
        "]\n",
        "\n",
        "app.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history\n",
        "        + [HumanMessage(content=\"What's my name?\")]\n",
        "    },\n",
        "    config={\"configurable\": {\"thread_id\": \"2\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们可以看到应用程序记住了预加载的名称。\n\n但假设我们有一个非常小的上下文窗口，并且希望将传递给模型的消息数量缩减到仅最近的 2 条。我们可以使用内置的 [trim_messages](/docs/how_to/trim_messages/) 工具，在消息到达 prompt 之前根据它们的 token 数量进行修剪。在这种情况下，我们将每条消息计为 1 个“token”，并且只保留最后两条消息："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import trim_messages\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "# Define trimmer\n",
        "# highlight-start\n",
        "# count each message as 1 \"token\" (token_counter=len) and keep only the last two messages\n",
        "trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n",
        "# highlight-end\n",
        "\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    # highlight-start\n",
        "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"Answer all questions to the best of your ability.\"\n",
        "    )\n",
        "    messages = [SystemMessage(content=system_prompt)] + trimmed_messages\n",
        "    # highlight-end\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the node and edge\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Add simple in-memory checkpointer\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "让我们称这个新应用并检查响应。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\n",
              "  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\n",
              "  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\n",
              "  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\n",
              "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a22ab7c5-8617-4821-b3e9-a9e7dca1ff78'),\n",
              "  AIMessage(content=\"I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 39, 'total_tokens': 66, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7b32d72-9f57-4705-be7e-43bf1c3d293b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 27, 'total_tokens': 66})]}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history\n",
        "        + [HumanMessage(content=\"What is my name?\")]\n",
        "    },\n",
        "    config={\"configurable\": {\"thread_id\": \"3\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们可以看到 `trim_messages` 被调用了，只有最近的两条消息会传递给模型。在这种情况下，这意味着模型忘记了我们给它的名字。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "请查看我们关于“如何修剪消息”的[操作指南](/docs/how_to/trim_messages/)以获取更多信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 摘要记忆\n\n我们也可以用其他方式使用相同的模式。例如，在调用我们的应用程序之前，我们可以使用额外的 LLM 调用来生成对话摘要。让我们重新创建我们的聊天记录："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_ephemeral_chat_history = [\n",
        "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
        "    AIMessage(content=\"Hello!\"),\n",
        "    HumanMessage(content=\"How are you today?\"),\n",
        "    AIMessage(content=\"Fine thanks!\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在，让我们更新模型调用函数，将之前的交互内容提炼成一个摘要："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, RemoveMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"Answer all questions to the best of your ability. \"\n",
        "        \"The provided chat history includes a summary of the earlier conversation.\"\n",
        "    )\n",
        "    system_message = SystemMessage(content=system_prompt)\n",
        "    message_history = state[\"messages\"][:-1]  # exclude the most recent user input\n",
        "    # Summarize the messages if the chat history reaches a certain size\n",
        "    if len(message_history) >= 4:\n",
        "        last_human_message = state[\"messages\"][-1]\n",
        "        # Invoke the model to generate conversation summary\n",
        "        summary_prompt = (\n",
        "            \"Distill the above chat messages into a single summary message. \"\n",
        "            \"Include as many specific details as you can.\"\n",
        "        )\n",
        "        summary_message = model.invoke(\n",
        "            message_history + [HumanMessage(content=summary_prompt)]\n",
        "        )\n",
        "\n",
        "        # Delete messages that we no longer want to show up\n",
        "        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]\n",
        "        # Re-add user message\n",
        "        human_message = HumanMessage(content=last_human_message.content)\n",
        "        # Call the model with summary & response\n",
        "        response = model.invoke([system_message, summary_message, human_message])\n",
        "        message_updates = [summary_message, human_message, response] + delete_messages\n",
        "    else:\n",
        "        message_updates = model.invoke([system_message] + state[\"messages\"])\n",
        "\n",
        "    return {\"messages\": message_updates}\n",
        "\n",
        "\n",
        "# Define the node and edge\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Add simple in-memory checkpointer\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "让我们看看它是否还记得我们给它的名字："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [AIMessage(content=\"Nemo greeted me, and I responded positively, indicating that I'm doing well.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 60, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee42f98d-907d-4bad-8f16-af2db789701d-0', usage_metadata={'input_tokens': 60, 'output_tokens': 16, 'total_tokens': 76}),\n",
              "  HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='788555ea-5b1f-4c29-a2f2-a92f15d147be'),\n",
              "  AIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 67, 'total_tokens': 75, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-099a43bd-a284-4969-bb6f-0be486614cd8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 8, 'total_tokens': 75})]}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history\n",
        "        + [HumanMessage(\"What did I say my name was?\")]\n",
        "    },\n",
        "    config={\"configurable\": {\"thread_id\": \"4\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "请注意，再次调用应用程序会继续累积历史记录，直到达到指定的邮件数量（在本例中为四封）。届时，我们将根据初始摘要和新邮件生成另一份摘要，以此类推。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}